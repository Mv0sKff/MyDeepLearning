{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mv0sKff/MyDeepLearning/blob/main/week_3/Assignment_Basic_MLP_in_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8Cwq-uPYvpa"
      },
      "source": [
        "# Build basic 2-Layer MLP to solve the xor-Problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "K4FDsqgaYvps"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs #for data generatio\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2x1wijYZYvpu"
      },
      "outputs": [],
      "source": [
        "X, y = make_blobs(n_samples=200, n_features=2, cluster_std=.1\n",
        "                  ,centers= [(1,1), (1,0), (0,0),(0,1)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "aoAh4a4KYvpv"
      },
      "outputs": [],
      "source": [
        "#make blobs into binary problem\n",
        "y[y==2]=0\n",
        "y[y==3]=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "RMPaCrKBYvpw",
        "outputId": "668c6dbf-3f24-427c-d576-4032b5e61dd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7d1e56f8ef90>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi8AAAGdCAYAAADaPpOnAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAlz5JREFUeJzs3Xd8FNUWwPHfnS3pldBBERXFBkoTLCCCKIhiQbAB9oINrKiAWMCuT0URLNjBBhaQDmJBQYqKgoCA1AAhvW2yO/f9MSEQsi1lkyyc7+eTJ5m9M3M2L9k9e8u5SmutEUIIIYQIE0ZtByCEEEIIURGSvAghhBAirEjyIoQQQoiwIsmLEEIIIcKKJC9CCCGECCuSvAghhBAirEjyIoQQQoiwIsmLEEIIIcKKvbYDqG6mabJjxw7i4uJQStV2OEIIIYQIgtaanJwcmjRpgmH471s55JKXHTt20Lx589oOQwghhBCVsHXrVpo1a+a3zSGXvMTFxQHWk4+Pj6/laIQQQggRjOzsbJo3b176Pu7PIZe87Bsqio+Pl+RFCCGECDPBTPmQCbtCCCGECCuSvAghhBAirEjyIoQQQoiwIsmLEEIIIcKKJC9CCCGECCuSvAghhBAirEjyIoQQQoiwIsmLEEIIIcLKIVekTtQ8rTUUrwTPZlDxEHEGSkXVdlhCCCEOUZK8iCrRRb+hsx4Bz6b9B1UMxNwOMTfK5phCCCGqnSQvotJ08Z/o9MGA56AH8tC5z6EohNg7ayU2IYQQhy6Z8yIqTee8gJW4mN4fz30DbabXaExCCCEOfZK8iErRnj1Q9DO+EheLBwpm1lRIQgghDhOSvIjKMdOCaGRDm3tCHooQQojDiyQvonKMBkCgybgelK1hTUQjhBDiMCLJi6gUZasHzrMAm59WdojsXVMhCSGEOExI8iIqTcXdCzjw9WukYu9BGYk1GZIQQojDgCQvotKUozWq3sdgb33QA0mo+NGo2JtqJzAhhBCHNKnzIqpEOU5CpUxDF68Fz3+g4sDZAaUctR2aEEKIQ5QkL6JaKMfx4Di+tsMQQghxGJBhIyGEEEKEFUlehBBCCBFWJHkRQgghRFiR5EUIIYQQYUWSFyGEEEKEFUlehBBCCBFWJHkRQgghRFgJafKyePFi+vbtS5MmTVBKMX36dL/tv/zyS3r27En9+vWJj4+nc+fOzJ49O5QhCiGEECLMhDR5ycvLo02bNowfPz6o9osXL6Znz57MnDmT5cuXc84559C3b19WrlwZyjCFEEIIEUaU1lrXyI2UYtq0afTr169C55144okMGDCAUaNGBdU+OzubhIQEsrKyiI+Pr0SkQgghhKhpFXn/rtPbA5imSU5ODsnJyT7buFwuXC5X6ffZ2dk1EZoQQojDyLrl//L7or9Ba046qzXHdzwGpVRth3XYqtPJy/PPP09ubi5XXHGFzzbjxo1jzJgxNRiVEEKIw0Xa9r08ccWL/L1kHYZhgALTY3Jsu5aM+uxeGrVoUNshHpbq7Gqjjz/+mDFjxvDpp5/SoIHvX44RI0aQlZVV+rV169YajFIIIcShqiCvkHu7jeafZRsAazTA9JgAbPx9M8O7jiI3M682Qzxs1cnkZcqUKdx44418+umn9OjRw2/biIgI4uPjy3wJIYQQVTXvg8Xs2LgLj9ss95jHbZK2PZ3v3l5QC5GJOpe8fPLJJ1x33XV88skn9OnTp7bDEUIIcZia/+Fi/M1q0aZm7vuLaioccYCQJi+5ubmsWrWKVatWAbBp0yZWrVrFli1bAGvIZ9CgQaXtP/74YwYNGsQLL7xAp06dSE1NJTU1laysrFCGKeowrYvQrsXogq/QRcvQuvwnICGECIWstBwCrcfdsmY7v85YXjMBiVIhTV5+++03Tj31VE499VQAhg8fzqmnnlq67Hnnzp2liQzAxIkTcbvdDB06lMaNG5d+3X333aEMU9RROv8z9O6z0Bk3orPuR6dfjU47F+1aXNuhCSEOA02ObYRh8/826XF7ePSip/n+059rKCoBNVjnpaZInZdDg86fgs72VttHAQqV9DYq4oyaDksIcRj5+etljO73bOCGCuISY5iyYxLOCEfoAztEVeT9u87NeRFC60J0jq8XDA1odM44DrG8WwhRx5x+YTu6XNwhcD0XDTkZefzyzW81E5iQ5EXUQa6FoHP9NNDgXgfu9TUSjtbF6MLvMDNuwUy7DDPjLrTrB0mehDjEGYbByE+H0+PaswO3tRmkbtpdA1EJkORF1EWePeB3jn8Jc0/IQ9FmNnrvlejMu8H1Pbj/BNdcdMYN6MyhaF0U8hiEELXH7rBzxf0XB2xnmibx9eJqICIBkryIusjWEGt4KAAj9JUtddYIcP9V8t2+lU4e6z+u+ejcl0MegxCidh15QjOOPKGZ3+Eju8NOl34dajCqw5skL6LuiegGyt8nGAPsrVGOY0MahnZvBdc8SpOV8i0g/yO0KRU2hTiUKaW48elr0GifncIDH+xHfLL0vNQUSV5EnaNUBCpuhK9HAYWKfyT0gRQtIWAPkC6A4j9DH4sQoladfmE7Hv7oHmITYgBrjgsK7E4714y8nGtH96/lCA8vdXpjxsOB1iWTT809YNQHe6vDeqdSXbQUnfc+FK8ElQDaBRTub2A7EhX/GMrZsQaiCbYgnq+eGSFETduydjvfTpjDml/X44xwcHrf9vS6rlu19IqcM/AMzrikI0u+/o1dm3cTXy+OLv06SI9LLZDkpRZp18/onLFW8rKPvRXEPYKK6Fx7gdUSnfsaOvcVwMb+hMBmfUUPRkVeAI5Tai65c7QJopEdHK1DHooQdZVpmqxfvpHczDyaHNOIxkc1rLVYvn59Nq/e+RaGzcAs2Y/ozx/X8NFTn/PM7JEc1+GYKl1/+4adzH53Ibv+20N8chwnndWauKTY6ghdVJAUqasl2vUjOuNG9tUt2e/wLMJm/Tyu9/GoAuyo+vNRtkY1GRbm3gFQ/Afee1cMiLwYI/GZGo1JiLpi/kc/8M6jn7D7v/0r/9qecxJ3vHo9R57QvEZj+X3RX9zX/TGvjxk2RUxCDB9uep3ouKgKX1trzbuPfsIn46aVVtxVSuFxezj9wnY8OnUYEVERVYhegBSpq/O01ujs0ZRPXCg9prNHW+20Rnv2oD2paH3oDk/ovMlYvSxeHwU86PxPay6gEirxxZJVTQf+qVgJJvbjambujRB10DcT5vD0ta+USVwA/lj8N3d1eYQta7fXaDyfv/iNz1L+pkeTk5HLvA8qt7XIV+Nn8cm4aSXXMjE9Jh639Xq8dOYKXr5lYuWCFpUmyUttKF4Onq34ngyqwbMFnfsKOu1C9J4z0HvORu85E537+qFZW6R4Of7njphQXPPVK5WtKSrlK1Ts3WA7wloFZT8WFfcoqt4UlFF3e/eECJW87Hwm3Pue18dMj0lhnou3HvqwWu5VkFvA9Fe/47b2DzCw2S3c1eURZr2zgCJXcZl2y+f+junxPU9NoVgx7/cK37+osIhPxn7p83HT1Mz/6Ad2bwl93Smxn8x5qQ2encG1yxtPmXV55l507v+gaBkkTUSpQ2kPjWDmsdROrq2MRIi9DRV7W63cX4i65vtPl1BU6PtDlOkx+eWb5WTuySKxfkKl75OemsG93UazfX2qtUxZW8fW/LKOGZPm8sycUaXDQKbpfwaE1hp3cfC910u/W8lnz3/NqoWrg2it+eXbFVx0e6+gry+qRnpeapjWLnTxPxU5o/z3RT9BwRfVGVbtc56B72EjAIVydqmpaIQQfuzesgeb3d/fq5UspG1Pr9J9nhn8Gjs27rJWZZa8FOqSJGXdbxt5Y9i7pW2P73iM3x2glaE4scvxQd33i5e+5ZE+Y/lj8d/BBaoU8z/+gUvqDeGCyCu54aRhfDV+VrneIVF9JHmpQdq1CL37DMiv6vioQud/VC0x1RUqZgi+h40UqEiIvjykMWhdaC3Vdv2E9qSF9F5ChLOElHi/QzQHtqusrf9sZ8XcP0pXDR3M9JjM+2Ax2XtzALj07j5+Y7I7bJx//TkB7/vfmm1MuO+90nsEQ5uaNb+sIzcjD3eRmy1rtjH+rrcZcf6TfnuoROVJ8lJDdNHv6IzbQOdUx9XAvanqV3FvQxfORhcuRJvVEVflKWc7VPxjWMNHB36iM4AIVOKbKCM5JPfW2oPOfRW9uws6/Rp0xnXoPWdiZtyDNqv2yVGIQ1HXKzr7LVlgGIqTzjye+s3qVfoeq39cG7CNu9jDP7/9C8BZl53OxXec77Nt46MbERkTeEXQtxPmYPPTg+OLPnDYSoPW8OcPa/joyUOsl7yOkOSlhujc8fv+5b+higP7CQT8v0ZVfLlfaSyeVMz0m9Bp56Iz70Rn3oLe3QUze1ytTgZW0VehUmZA9ECwHw/2k1GxQ1H156IiTg/JPbXW6KxH0LmvHrSTtQmu2ei9A9FmdkjuLUS4Sm6UxOXD+3p9TCmFMhTXP3VVle4RbD2nfe2UUtbwko/Ttv2zg1dufyvg9dav2ITHR2/PwQzDf4za1HzzxmyKi2T4qLpJ8lIDtJkHRd/jfzWNgohzUQ1+RsVcj//qrjaI7FPJWDLQewdC0Y+UTaRckD8ZnXkf3kr/aDMLXbQSXfwXWrsrde9gKPsxGPGjMVK+xkj5AhV7J8oWwqJXxX9Aoa+VBB7wbIH86lk1IcSh5IZxV3H1I5fhiLDWfaiSN/Lkxok88c0ITj6rasUb23Q7MeA8fkeEg+M7WoXncjPz+O6t+T4/H5oekwWf/EjaDv+9qRFRjqDWD9iddo5o3QzD7v9tNCcjj13/HRrD0EWFRSz45Ec+ePwzvvzfDHZvrb3nJauNaoLOJ/AuyTYwUlAqAh15PuS+VrKc+uCExwAcJXNEKhFK3vtgpuI9OdLgmgXFK8DZzjpiZqCzn4HCb4CSTw9GCsTcCtHXhvVWBlqb6OwnA7Qy0flTULG310hMQoQLwzAY8sRALht+Ib98u5y8rHyaHN2Iduedgs3mfzJvMBq3bMjpfdqx9LuVXueeGIbighu6E5sYQ5GrmI+e/Jxil/8PVqbH5I9Ff9H9qrN8tul8UQdWzPe9X5lhU/S8pivD3rqVDx77jK3PTAv4XML3VXK/H778lRdufJ28zHxsdhumaTJh+Hv0vrkHd7xyPXZHzaYT0vNSE4xEUIFKSJso+5EAKOVEJb8H9n2lrO2U5pkqAZX8NsreonKxFHxGoF4dXbKSSZs56L1XQuFXlCYuAGYaOudJdE54V5bV2U+BO4i6D+bu0AcjRJiKS4ql57Vd6XfHBXS84NRqSVz2uf/dobQ40arUu69nZ9+KolO6nsjNz13Lnm17ufmUe/n8xW+DumagIaGeg7qSkBLvdeWSUgrDZqP//Rdhs9lof37boIaYXr3zbdzFoeuxDrUV8//kif4vkJeVD4DH7UGbVhHVmRPn8eodgYfjqpskLzVAKQdE9SfQUmAiL9n/na0xqt7XqKT3IHowRF+NSngB1eAHlLND5YMx9wZo4AHPLuuf+ZPBsxmfw13576DdGyofSy3Sxf9AwQfBNVaVr1MhhKi8+HpxvPrLWO5/dyitOx1Loxb1OaXrCYz8dDhPz34UZ6STkRc9TeqmXUFf8/hO/vc3iomP5rn5o0lsYP3dGzYDZSiUUjijHIyZ9kDp1gcndjmuZIm2/76VFXP/sIa06qh/f9/Ml/+bwZcvz2D9io3lHp88agpK4XUAQWvNzLfmk7q5Zj/kybBRDVGxt6FdC8CzjbLJgPUboeIeQdnKzsxXSkFE5+rdpNFIAtPfOKUBno2YmfeAazEBe2nyP0XFP1x98dUQq3fpwA0gfbFB9GU1EJEQwpt1yzcy571F/L3E2sB2z7a9JDaI59h2Ldn9Xxr/rtoc1HUMu0HbbifS/LimAdseddIRfPDva3z/2RKWz/0dj9ukdadj6Tmoa5mNGJVSjJn+ADedfG/pkm3vNNPHz6LvbXWriN3enRk8OeBFVv+4trRnS5ua4zsdy8hPh9OgeQp7tu1lzZJ1fq9jGAaLP1vCFfdfXBNhA5K8VIg2s615KCoKbEdVaL6HMhKh3lR0zstQMA1wWQ/Yj7UmpUbW0C911OWQNxHfSYkJnu0lVYADdYeaJclY1WitwbXIql3jXmP9fCPPR0VfjbI1rvL1vfLsIPDzA4hDRQ8KTQxCCL+WzV7FyL7jOHANgcdtsvizX1g+5w/OuqwTNrutdJ8hX5ShqN+sHve/OzToezsjnfS8tis9r+3qt11yoySO63g0y75b5bON1rCthvd6CqQgr5B7u41mZ0mv1YFLvdcv/5fhXUfx5srnyM3MC3gtZaig2lUnSV6CoD1p6JznofBroGTc0nYUxN6Jirow6OsoIxmV8Dg67kHrzVNFga1pjU56VdGD0QVflgwfBdhLKCDDms9TBdYmlSOh4FPK9ITkvY3O/xCS3kU5T63SPbwykrBGTQOsAKv3SY3vZC2EsOZVPHfdeEyPLrcC0vSY5GXls2LenwTz8tn/3ou4csQlxCbGhCTW6LgoDJvC9PhemGFz2ChyFeOMCM22LsVFxcycNJ+v35jNjg2pRMZEcM7AM7lsWB+aHlP+Q+C8DxazfcNOr0NBHrfJ7v/SmP3uIs4b0i1gguhxe2jcMoSrQr2QOS8BaDMdnd6/ZNLqAROuPJvRWcPRed43J/NHGTEox7Eoe7MaX62jbPVQ9aaCo101XM2DivRe6yFoBZ+XJC7W9Q68NroAnX4N5t5rMLMexXT73xNKu7dg5ryMmXkfZvZT6CLfk3FV1EUEHDJSSSh7y6CehhCiei39biUZqZleSzeAlcCkbt4dcL+ihPrxXP/UlSFLXADO6NfJb+ICUOxyM7DpzSybvara7+8qcPFQrycZf9fbbFmzDXeRm9yMPGZMmsutp97PXz+X35Jm3gffo/ysg9JaM/eD74lNjKHrFZ39LgmPiIqg6xU1u32LJC8B6NzXwZNK+Tc66xdV5zwddqXkla0pRr0PUSkzUQnPgapMFUwDnJ3AWfnicVprdN47+F5IqIFiKF5qJThpXTEzbi9XZ0ZrjZnzMjqtJ+S9CYUzIP9DdHp/zIxb0bqw/KUd7YG4AAGmQ9HSSjwzIQ5PHo+Hn79exiu3T+Klmycwc9I8CnILKnWtzX9tC/zhTkNsYozPYnHKUFw89PyA+zAdqCCvkNTNu8nLCn4Y5MxLO9KsVeOANV9yM/MY2ffp0qrA1eWTsdP484c11vDaATmU6TYpKijisUufK7faKSst22diWNpmj1Wg84axVxGfHIftoOe37/+fu8bfWLpBZk2R5MUPrYtKlhb7y+w1FE6v/D3MXKtEf8GX6OI/Av4yVSdlPwYVdTGoSixtjOiBSnyjaj1HOg88/xK4Bs4BXPPQGQeNWxd8DHmvl1zHc8AX1lyarFFe7p0JBNoSwY52LQw+NiEOYzs37uL61vcwut+zzHxrPrMnL+KlW99kQNNbWD43iJIEB3AVuJj1zvygXg9ve2kIzihnmaXNSilQcGr3kxj4UL+g7rl7yx6eu248l9a7jmtbDuWS5OsYedHTXlffHMzhdPDM3FEBJwNrU2OaJp+M9VUYs+KKi4r5+vVZZbcnOIBpajJ3Z/HT9GVljjdu2cjvRpaGzaDx0dZQUIMj6vPar+M445KOZc5pcVJzxkx7gPMGd6v6E6kgmfPij5kJOtCnBgPt3lLhIkRam1ZJ+ry3gQN6BuytIOEZlOPECl4x2Pu6wTUfXTANPHvA1hSMxiUrkALNc1GAAxJfxYgMvMFZYJVMfIoWot0bUPZjSvYlesNPYxMKv0J77kHZmuw/HPQ2CK4KhaZ1ARR8gy6cCWYW2I9BRQ9EOatjmE6IuslV4OL+c8eQtt0qxXDg/IjC3EJGXvQ0b6x4jiNbNwvqep+/+C07NwZe/tz46Ib0HNSVk89qzZf/m8HCKT9RmFtI01aNufj28zlvSLegiqft3LiLO09/mNzM3NK6LVprln63kuVzf2fcrEdp07X8a/KebXspyC2kfvN6NGiewsTfn+fx/i/w0/SlPj+TaVPz89fLcBW4iIgKvNdSILu3pJGT4b+XyOawsW7ZBrr2379ytc/NPfjNzxCW6THpc1OP0u8bHlmfkVPvJSstm13/7SEmIZomRzeqtUKlkrz4o2LYt5TZNw1GgOEHb2fljIX898s/4N6ATr8K6n2BsvuvR1Dhe5q56IzroXgVVqebCe6/CbxcuPQKgBtyX0JHdKvyL60yYtD21uBeS4V6XwCd/wUq/kFrdVIwReRciyD6gL1WjHpgJIPfjRfdKPsJwcfk2YFOv7akMnLJ7417LbrwK3TU1aj4UWFdkVgIXxZN/Zld/+3x+pjWGtNjMu3lGdzz5i0Br6W19tuTcKAhYwaglKJxy4YM/d/1DP3f9RWOHeC1u98hJyO3XCVf02OitebZwa/xwcbxGIbV67Dkm9/4YMynrF9hbZDrjHTQ89quDHlyIJHREQFfzrSpKcyrnuTF4QzibVxr7Ae163xRe04+uzV/Ll5Tvr2CU7uf7HUeS0JKfJV2C68uMmzkhzJiIKIb/ovLeVAV3GdIu7dCvq8CaSboInTOqxW6ZlD3zXrY2stn330A79sP+GNayYZ7dbXEpGJupKKJixVGScISsGcMQMFB816UskPUVfh+vspKXoP8/1Zrjc64tWQJNux/TiU/34KPIP+joK4lRLj5cdrS0joh3njcJt9/viSoa+Vl5ZO+MzNgu9adjvVb5t+X3VvT+PCJz3nu+vFMGD6ZX2YsZ+nMFV63IAAr0di9JY2VJVsGfPf2fEZd/AwbDqgvU1RYzKx3F3BX50eCXjL8+/d/l/k+c08Wu/7bU24Tx6LCInb9t4ecjFy8qd88hWatGvvtyPa4TTpcUHbV5rZ1O/l31WafH6guvKVnheYK1TTpeQlAxQ5Fu37Aew+MARHdUY7gP50DJUuu/S3T9Vg7Gpt5VgJVDbRnB7hm4z9RcJQMIW0JfEH3enCcXPXAIi8E9z8ltWeCKRpXwmZVuMR+dBDnmWA/rswRrTVE9oDCueA5eCa+DVCoxJdQRnRw8RQtLelB8k3nvQXRV6GUfGYQh5bCvMKAPSVFBcEN1Toi7CgF/qa72OwGx5x6lN/raK3LvTF/Mm4a7478xNr5WgEovnh5RsCYlKH47+9ttGp/dGkp/IOfr8dtsuu/PcQmBdoKxjLuqpept3AMuZl5fPTE56z5dT0A0fFR9L6xBxfe2pMvXvyWOe8twlXyszutx8lcO6o/J525f9NLpRQDH7qE569/3et9rJ9VS07sUvY1cMKwyRTmubzOK1LAK0PfosvFHWp8z6Jg1c2o6hDlOAWSJqIz7wWdgfUjMwENkRegEsZV+JraTCPwfA8TdBZQTcv7in4hcA9HMUT3h9wXAl9PRaHdW6xaLIWzrJ4Nx/Go6GsgomeFtrNXcfehI85F538MxX+CJ/AEORV1iTWh2rPdWvFU9AveExgDbI3BuX+sVxcutOr2eNYf0C4Ca36LAyLPQ8XcVKGkVBf9TMAkytxhFfWzHxH0dYWoq3b8m8o3b8zhtzmr2Lsjw+8IuzIUR54Q3HyXiKgITutxCisXrPbZG+Jxm3Tp17Hc8dzMPKa9MpMZk+aRviOd2KRYzhvcjcuHX8jyuX/wziMfA6ArOkxtaqLjopj/4Q+4i3z/jZsek82r/wvqmh6PyZjLnyMjNatMr1V+dgFf/m8G01+diWmaZZZgr1r4F6sW/sVjX95P577tS4+fN7gbOzak8vHYL7HZDTxuE8NQmKamWasmjJn+QJnX5N1b0/ht7u++5+Voa6XRrzNWcIaXn3NdIMlLEFTEmdDgB3AttHocVJT1Bl3JNyFlNEQHnBxrB5VYqet7pYMpOgfYW1qbSGrvXZQWJ5pISLsQa8PGkj/moqXool8g8lJIGFuhHgblPBXlPLVkAu4EyPuf78aRA9EF062hN5297wqUf/W0AU5UwkulseiCb9FZ93q5qNt63smfYjgqM9fIQ3ATkIP8/0GIOuyn6Ut5csCLmKb2mWAcSJuai4ZeEPT1r3z4UlbM876zs81ucNTJR3Jaj7I9vxm7s7jnzEdJ3bgLs6RXJCc9l2mvzGTOe4uIiHIGff/y97TR6cJ2fPj4Z9jsht/aMsUuNxHREbjy/U/216YmIzWr9N8HMj0mppdbmB4TpeDZIa8xdftEnJHWc1JKcd2TV9JtQBdmTprPlrXbiUmMpuvlnenSrwMOZ9nCeDs37gr4WdawGez4N/g9o2qaJC9BUsoJkb2AaijjH3Ux5L7kp4ENInsHP2QRDGfbIBrZUI7TIOZmdO6LPtooa+Jr1n1AEWXfjEv+XfglOE+D6CuCDk8X/Y7OexNcC0quE4+1CuvArmYHRN9k9ZjkTaXsX9++f+/r/bBDZB9U7C2lE5+1LkRnj8b7X61VFI+ccZD8dtBx76Mcp6IJsGusSrJWdwkRxlI37+bJAS/idnuCmq6mFHS+qAM9B50d9D3adD2RB9+/k+dveB2P21NSx0XhcXs46uQjGTvz4dLJs/uMv/NtUjfvLk1c9jE9JrmZeeSk+/tA5l/X/p1J35lBZExEUMu3j2jdlPXLA/cgV4bWkJuRx4/TltL9yjNLjxe5ijnyxOYMfSXwpOW4IIa2TI9JXFLoCvtVVUgH3xcvXkzfvn1p0qQJSimmT58e8JxFixZx2mmnERERwTHHHMPkyZNDGWKtULbGEHOzj0dtoKJRsXdW7z3tx4CjI4EmH+NaADG3QPT1WD0JtgO+sHbHth0FOgffvQgKnTc56Nh04Wx0+gCrZ6v0mtlAMdhaQ9xjkDgR1XA5ytkGXHPx/arpgfgxqIZ/YCQ+B7Yj0YXz0HnvoLPHlcTt5/kX/Yj2VOLTRkQ3MJrg++erUDHXWjuMCxHGvp0wx0oQgkhcGraozy3PD2bUZ/diswU3+bOosIiZk+Yx7ZUZRMdHkdQggWNOPYqeg7sybtajjF/2NEkNE8uck56awQ9f/orp9j3pNhBlqNIvu8NWOpRjd9pZ8MmP3Hrq/Xz75tzSpdT+/Pv75oBtqsLmsLFlzTZcBS6mPDOdq468lT5RV9E78kqeGPAC65b7L4J31MlH0PRY/5N87Q4bXS7uUM2RV5+Q9rzk5eXRpk0brr/+ei699NKA7Tdt2kSfPn249dZb+eijj5g/fz433ngjjRs3plevurUbZ1Wp2OFgJFo1SkqHPgBHG1TCkyj7kdV/z8Tn0XsHgOm7zL7OfhRlb4ER/xA6+iqrHoy5C4wUVFQ/lL0lZuaD+J/focGzIagJx9rMQWfeb51TLhnS4PkHpbNQkdYyZ/PgPZC8yXkeHO3Rnv/Q2Y+WLIc2vFzfe+w643a0mQ5GgrWNQFR/VIDl8ErZIOl1dPqgkiG3ffcqua/zbD8JqxDhY8X8PwMOFSU3SmTS6heJS4r1Ov+tyFXMb7NXkbkri3pNk2nX8xTsDjt52fk80GMM65ZvRKGsXg4FGbuyyN6by+AxAzAMg23rdvDNG3P46+e1GDYbTY9tHNTwlT/a1Dzw3h1k7s5m1+bdLJ/3B9vX7cRdtL9HNT87uGrBvpKo6qJNjcPp4P4ej7P21/WlyZnHbfLTtKX8NG0ZY6Y/QKfep3k9XynFDWOv4vH+PuY3Krh8eF/i61W8DEhNUbqGSroqpZg2bRr9+vXz2ebBBx9kxowZrF69fxnuwIEDyczMZNasWUHdJzs7m4SEBLKysoiPr/216IFo7YKi36xqs/aW1V7b5WBm3oeQ87ifFjaI6IqRNOGgODUU/4YuXGDVTPFsIlAyoBr+jlL+S0brvA/QOU/i92OckYKq/yNKGZh7eoNng99rWiLZX2Cuor/iB86dUWA0QiV/iLI3D3im9qRaE48LvrGSGPtRqOirIPJCa3m2EGHutnYPsGHlJr9tkhsnMnX7JK+PzXp3IRPvf7/MME5C/XiG/u96ls/9nbnvf+81EbHZDU4++wTOveosXrx5AspQpUlCoNVJ+yhDee2FMQxFYsNEPv7vDWx2Gz98+SuPX/58wOsZNqPKSVNlXTS0F9++MafcMBlY77eRsZFM3TGRqJhIn9eY9e5Cxt/1NoV5Lmx2G6ZpTfS97J4LueHpq8sNzYVaRd6/69Sr6ZIlS+jRo0eZY7169eKee+7xeY7L5cLl2j8xKjs722fbukipCIg4o9qu5215YBnFy/DfC+EB1+Iy19FmOjrjFij+nf2rrfz9wRrgODlg4gKg3WuxelL8zBcx06yVVyrJ2g3aE6hwIJSpWlxhB82lMXejM4dCva8CrqJStkaouOEQN7wK9xei7mp7zkls/OM/n2/aNrvBqd29l1GYPXkhL9xQfklv1p5sxl71st9kwOM2WbVgNb8v/Muqq3TAm3ZQH8EVpDRNZu+OjDL3sNkNDJvBwx/fXVrXZMabc/zGYtgMWp/eivrNklk09eeAtzZsBlprbDYDj8dKEAY80I/VP65l9U9rK5QAGYbirP6dWTTlZ6+JC1jvAwU5BXw/9WfOv767z2udf905dO1/Oj988Su7Nu8hrl4sZ19+OsmNkoKOp7bUqeQlNTWVhg3LbqvdsGFDsrOzKSgoICqq/JvhuHHjGDNmTE2FWCdp93/WBoeF34DOQ9uaWp/2o68un0DoYgK/8XtK2ihrG4P0G61KtoDfJKOUCdE3BBe8CrbCZMms+qiL0MXLArStbh6rhkvxcnC2D9xciENY39vOY9orvmujeNwm/e4sv7KouKiYiQ/4Ks5pCWrlUiUGCwzDoPtVZ3Lri4P5ZNw0vnt7PvnZBRg2gzMv7cSVIy7l6DYtSttvW7fTbyymxyR9ZwYXDz0/qORl2Ju3UJjvYu/2dJIaJXLOwDNIaphIbmYeYy5/nlULVmOzGyilcLs9REZHcO7VZzN78kI8bg+2kuTH4zY589JOXPfEAL4PcF+7wxawhwwgKjaqVvYmqqo6lbxUxogRIxg+fP+n3OzsbJo3D9y9f6jQRb+jMwaV7NVTMg/Esw2d8xwUzITkD8rMO1GOk9GuBfhOYAywt96/zLnop8pV0y34Ah3ZNXDvi7M75H/op4EBjvb7n0PURZD9JBXdc8i3YAvj2aBomSQv4rDX5OhGPPTBXYy75hWUonQCq81u9Src8coNHN/x2HLnrZy/muy0QJuhVo99tU72/bdt9xO5642biIqJ5NYXBnPTs9eQl5VPZEwkzojyk+jj68X63O5gn7jkWI5t1zJwMAranHMijY9qWO6h2MQYnps3mn+WbeDHaUtx5btocWJzzrnyDKJio7h+7JXM//AHdmxIJSYhmm4Dz6DFic3JSgs8wqA1VVoeXtfVqeSlUaNG7NpVdqXHrl27iI+P99rrAhAREUFERNX3hwhHWnvQmXeCduF1sqv7b3Tuy6j4R/YfjuoPua9Zj3tNYExUzOD9VymcTYUq3+5T9AM662FUYvkl4VqbUDANnf+eVV3X7/5RJir21gO+zaLaEpeoy1G2FmgMyH22eq4pxGGg24AzOOqUI/nqtVn8NnslpkfT9pwTuWjo+bRqd7TXczJ3Z9VIbC3bHMmJnY9j15Y0EhvE0+Oas2l7zkllhnxtNhvxyb4no/a4pisbVm722cujlKLHNWfT7NjGtOt5CqsWrva6CsmwG7Tv2cZr4nKg4zocw3Edys93jE+O45K7epc7npAST6v2R7NhxUafQ0cet4fT+x66H7bqVPLSuXNnZs6cWebY3Llz6dy5s48zDnOu78FM9dPAhIJP0XHDS3tAlC0FEp9HZw6j7BYFJfNgIi+FyIv3X0IXUKm9hzChcCbaPbzMRFetTatIXOEMvCct+4rNWf9W8Y9ZRQJLL5tZiVi8UNGo+JHWz0UXWTVmtL8XVw8462alSSFqw5Gtm3HX+BuDbp/SrF5wDYOZ0uaDYTPo1Ps0rn/qqsCN/eh1XTc+f+kb9u7MKLdyyGY3qNckmfOGdAPg3rdvZ9hZI9mzNa1MImEYivrN6jFs0q1UF4/bQ3GRm4goJ1c/chmjL/H+ocuwGbRq15KTzjy+2u5d14R0KnFubi6rVq1i1apVgLUUetWqVWzZYu2dM2LECAYNGlTa/tZbb2Xjxo088MADrF27ltdff51PP/2UYcOGhTLMsKWLVxMw/9QF4C5brlpFXoCq9zlE9i7ZOdtpTbBNeBGVMK7MJxRl9/4pKmiuhWW/L/iyJHEBn69QEd1RsXej6n+Pih5Q9jGb/08wB12I8nVXrOem4kbtT+iUE6KvBZ9FD2xgPwEc3pcdCiECa9PtBFKaJvv8M1NK0fTYxpx2rjXZ17BV/O1JKehzc8+qhAlATEIML37/OEeddERpLPviaXHSEbywaAwx8VYR0frN6vH6b89wzaj+1G9eD0eEnQZHpHDt6Ct4/bdnSGmSXOV4/l7yD6P6PUPvyCvpG3sNV7e4jS1rt3Pzc9diGArDZqAMhc1uxdjylCN5/OuHyryWF+QWMGPiXEZc8CTDzh7Jq3e8xabVQexjV0eFdKn0okWLOOecc8odHzx4MJMnT2bIkCFs3ryZRYsWlTln2LBh/P333zRr1oyRI0cyZMiQoO8Zbkulq0LnvoHOfYVAQzoq5btKJyHaswu9pyuVK2tvQ8UOR8XeVHrETOsL7nX4m3OjYu9Cxd7u86pm6vGB4zFaoJLGo3OegqIDJrapJIi5ASO2bM0VrYvRmXeDax5lh8kUGI1LlkoHtzeLEMK7n6Yv5bHLnrM6Vw54CVCGQinF2JkP07b7Sfz67Qq+e3s+m//aSuqm3QGvu+/8ER/eRbcB1bt686+f/+HPxdaChVO6nsAJnVsFvXdbdfjhi194cqA1/H7gJGJlKE4643iGv3UbCz76gS1rtxEVE8lZl3emfa82ZZY5b1u3g/vPHUPajvTS+jn75gNd9+SVXPVw4DpsNaEi7981VuelphxWyUvx3+i9/fw3Mpqg6i+o0k7GOv9jdPZjBF/obT+VNAkV0dW6jjbRu1oTsE84ogdGkvcdUgHMXZ1KNsn0d43uGEkT0J7t6PRbS3aO3veCo8FxKirxfyhbo9JTtDbBNR+dP9Xaw0oXgREPtqNQUX0gspfVSyOEqLRfvl3OG8Mns2PD/iHvI1o3Zej/rue0HqeUaVuY7+KKxjdSkOO/9MEZ/Toy6LEraHlK9Rf3rE25mXkMaHozRYVFXl82laEYPGYAVz9ymc9ruIvdDDnuLvZs3etzBdWoz+/jrEs7VVfYlRa2dV5ExSjHCWhnZyhaiq/eFxV7a5USF8Badm00Rue9XlLrJRgGGA3BecB8ldLtBgIst9Z5vh8qXg+2RuD2n7yoyD5W9d69V1sVgq2z9zco/gOdfo1Vu6VkJZNSBkT2BPdGdNFiK1bPXvD8hy5aALn/g+T3UbYm/uMXQvh0+oXt6NTnNNb99i/pqZmkNE3mmFOP8tqbERkdwcW3n8/U577yWlzOZjc4scvxPPbl/TUReo2b98FiiguLfe/+bGq+Gj+LK0dc4rOg3JJvlrNrs++VU4ah+PTZ6XUieamImi2fJ6qdSnzZmo8B7J/fUfLfmJsgaoCXsypxn8hzMOp9hmrwK6TMB7u/yasG4EQlvmyVzd93DaWgpBfGr+J/vc7y17mvoff2KRl28sUGtpbWJpoFn5VsheAtsfOAZwsUflX2HoWz0Lkv7G8DlPY2ebaj02+0emiEEJWmlOK4DsfQuW97jj2tpd9hmEFjrtjfI7N/Lj9gLdt+ZMo9IY21Nv37++aAc38yUjPJzfD9gW/5nN9Li+95Y5qatUs3UJAb3NYHdYX0vIQ5ZSRBvU+tqriFM8DMBvuRqKgrUI7ytRaq437KSELXewfyJqPz3gd94Ji0zSqFf8BuzmU4zwbXfP830bvB/Sc49nch64JvS+b3gN85PvYTUUnjUcqJWfCV73bWs0EXTLd6lvbdJ/dNfA+PeaytCYp+goizAlxbCHGg9NQM/vp5HWhN686tgp7Iumjqz/y+6C+UUuh9XRAaGhyRwtjvHgmLarCVFWydFkeE77dyjzu4MhfBbDhZl0jyEua09oBrIbrgM/BsB6M+ynEKhGBjxwMp5YTYm63eHZ2Hxo7CDSrS7x4+yogJbhWke2tp8qK1RudNIOAayriHUdGD93+KMzP8t0eXtCn5zswC918BArOjXd+jJHkRIih52fm8cvskFk39uXTOhTIUXft35u43biY20ffmrSsX/MmzQ17z+me8d0c6j/d/gfFLn67RCbQ1qfNF7flqvO99/QybwclntSYq1ncx0NadjmXWOwt830RB46MaEpMQXZVQa5wkL2FMaxc64zYo+pH9K2Q2oIt+grx3IHkyykjwfb57GxR+jTb3oIwGEHVxhedzKKVAxfrbWb0sI8hJ1Ebi/n+b6QGGigDs4NlZ9kXMdgSYu/E9ydgGtgOSPF0cXGzBthPiELVu+b98/fps1v32L85IB10u7kjvm84lsX7Z15viomIe7PkE61dsLDNZVJuaxZ//wvb1qbz84xM4I733MHwy9ksMw/seQx63yfrlG1m5YHXp8upDzannnswxpx7Fpj//89ozYpomV464xO81zrnyDN68/30Kcgu9zhsCuPTuPmGXAMqclzCmc547YBnwQfMz3GvRWQ97P0+bmNlPo9POtYZi8qeic/+H3nMOZs5zldo7JGjOLqB8J1SAtZzZ2eGAA8Hsp1S+nYoeiP/VUZ6ydWSMZDAaBLyHchyaL5RCBOPjsV8ytMNDzPvgezb9uYV/lv3L5FFTGNLqLv5ZVnbH90VTf+afZRu8Jh+mx2T9io0s+OQnr/cpyCtk5YLVfvcYstlt/Dx9adWeUB1mGAZPzRjBkSdYhT5tdhuGoUprugx781ba9Wzj9xpRsVE89uX92J320jow1rWtZKVr/870vf280D2JEJHkJUxpMwfyp+B/d+h5aPfW8g/lvQH572D1xZpYb/qm9X3eJMh7M0RRW8NNKvZu/23ihpVdkmykWF9+eUkqIs8H5xl4/zVXENEdIs49IDYDFT0I38XqFKg4iOoTIBYhDk2/fLucdx/9BCg7R0KbmoLcAkZc8BQFefuXNc96ZwHK8P2JXhmKWW97nwNXXBhMD6fGVVAUXPBhKrlREq8vf4Ynvx3BeYO70W3gGQx5fCAfb5lA7xvPDXwB4NTuJzNx1fP0vqknifXjiYqN5LiOxzLiw7t4+ON7sNl8T+itq2TYKFwV/wEE+qPV1jLqA8vzm/novIn+z8qbCDFDUCqy6nF6E301Chc652WsfYr2DXlFoOKGl/SY7KeUDR19jbVU2escFgNUPERecNB5dkh6E537P8j/eP8SbBUL0deiYu8ov4w85joo+hWKfqDsHBsbYEMlvhp4s0khDlGfPv8Vhs37MI7p0eSk57Lgox9Kq9zu2brX51AFWEnPnm17vT4WmxRDUsMEMnb53rbD9OjSKriHMpvNRqfep9Gpd+WrfDdr1YS7xt9YoS0d6jJJXsJWsDPDD2pXtKRkvyI/dC4U/QIR3SoTWEBKKYi5AaKugMLZYO6xhmsie6GM2PLhFP8D+Z/hPXGxAXZU0msoVX6DTqWcqLj70bF3QPE6q364vZXPxEwpByRNsHbFzv8Q3BtBRUJkb1TMEO8rqIQ4DJimyeof1uBvVNkwFKsW/VWavKQ0SyZ1826fCYwylLVlgNdrGVx0+/l8MOZTn5sP2p12egw6u2JPpJZs/Wc701/9jp+mL8Vd5ObYdkfT784L6HjBqWE336QukOQlXDlOxPq/L8B8EOepZb/X+cFd3wyyXRC0mW7tIl28xlqNFNkDnGehjDiIvtz/uZ49VjE5neO9gYqEpI9RztZ+r6NUFDj9jw3vb+uA6IHleoCEOJxprf0mLlCyV725/wPT+dd1Ly2t77W9qTn/+u4+H+9/X1+WzV7Fml/WlUmADJuBNjX3vXO7392h64ql361k9CXPYppm6UaPK+b9wW+zV3HJXb257aUhksBUkMx5CVPKSIbIvvj+v9AGjtPL9xQEu8dRVTdkLKELvkbvPgud86y1IWPBl+iMm9F7L0Z7dgU+P//jksTFR0+TzkO5g636K4SoLJvNxnEdjimd6OmVhpPO2P9BotvAM2jVrqXXQmuGzeDoti3oftWZ5R7bJyIqgmfnjmTwmAEkNUoErM7T03qcwvMLH6P7lb7PrSuy9+bweP/n8RR7yuxQvW/obdorM/n+0599nS58kOQljKn4R8G+74XiwNKTCmxNUInPlT/HcUJJRV5f/9cbYD8F5TiuyvHpomXorPuBYqzPZB5Ke4rc/6IzbrDq1PhT+DX+h8iUVZxPCBFylw+/0OcQjjIUkTER9DxgGMcZ4eCZuaM4+/LTy0zcVYbizEs68tz80URElR/uPVBEVARXP3IZU7dPZHrGZL7N+4hx3z3CKWef4Pe8umL25EUUFRb7XMVpGIovX5bXsIqSYaMwpow4qPcJFEy3NhP07ARbPVTUZRDV33rc23kJ49DpV4EupGy1WhuoKFTC2GqJT+dOwEqSfJTnd6+zJsb6m1tj+hgu2n8Xq6qwECLkul7RhTW/rufLl2eUmbhr2AzsDjtjpj9ATELZonOxiTE88skwbnl+EH/99A9aa04843jqN6tXoXsrpcpdOxz8veQfv7UyTVOzdtkGTNP0uT+RKE+SlzCnVGSF52coR2uo9wU691UonIWVXNgh8gJrBY79qCrHpXVRSfE8f4PkdnThfJS/5MXeEopX4bfQXDUNcQkh/FNKcesLg+lw/ql8Nf67kiJ1Ts7o15GLh55P45YNfZ6b0rQeXa/oUoPR1g2GzQhYHFwpJXNeKkiSl8OUsrdEJb6ENp8EnQUqEWVUY3loXYT/xAXrce1/q3sVfRU6a4WfFh6ZWCtEDVJK0f68NrQ/L7gJ8Ie7U7ufzOLPl/h83LAZtD3nREleKkiSl8OcMmKAEHTFqhgwGoGZ6qeRGXBujS7+x/99ovqDo4P/NkKIWpO2fS8zJs5j6Xcr8bg9nHxma/re3osjjm9a26HViHOvPpN3Hv2YvMw8TE/5D3Smx6T/fRfXQmThTQbYREgopVDR1+K7Wi2AHaIu9fmodv0I+ZP83yj6OvnEIkQdtWL+nwxpdRcfj/2Sdb/9y7+rNvPNhNnceOIwZk6aV6Fraa0pLvI98bWuioqNYtx3jxIdH11m0vK+Uv23PD9IerEqQelw+00IIDs7m4SEBLKysoiPD3ITwMOQ1hqKV6ELvgRPqjXRN/JicJ5ebcmA1kXo9BugeCllh5BsgIlKeBYV5fsTh5lxC7gW433Cb8l1oq/BiH+kWuIVQlSfjF2ZXNtyqO+VNgpe+XksrTsd6/c6WWnZfP7it8ycNI/svTklK5q6ccX9F9GoRaC9yOqO7L05zHpnAT9/vYyiwmKO73AMfW87D5vDzsxJ89iydjvRcZGcdenpdOnXAYfTUdsh17iKvH9L8nIY0tqNznoQCr9hf2n+kv86zy6pVls9WwNoXQR576HzPygZQlJWgbrYm1HOjn7PNXd1BJ3p/waOUzDqfV4tsQohqs9HT33Be6On+qyua7MbnHX56Tzy8TCf19i7M4O7z3iEPVv3ltmSwGY3iIqN4qUfnqDFic19nl/XffTUF0weOQXDbmC6TQybwvRomh/flGfnjiSlacVWZIW7irx/y7DRYUjnvgKF35Z85yn736If0dmPV9u9rI0Yb0LV/x7VYCWq4Z8YyW8FTFysk4OZknX4fToRIhysmPeH332NPG6T5XP+8HuNV+94i7Rte8vtpeRxm+TnFDD2qpfDbhhpn4VTfmLyyCkApcXr9s2J2b5hJ4/2fTpsn1tNkOTlMKPNPMh/D98rgUyrlL9nT7XeVymFMmLK7hYdSER3rB4hXwz/y6yFELXGX+Kyj2n6LkCZtiOdn79aVmb36jLnekw2/bmFNb+uL73WtnU72PTnf2V2tq6LtNZ8PPYLnztum26Tf1dt5vdFf9VwZOFDkpfDTfHKwBsz4oGi2i9XraIH7fuXl0cNUFEQ3b8mQxJCBOmkM4/3ui3APja7wcln+d6TbPPqrYETIAX/rtzEd2/PZ/Cxd3Ld8Xdzc5v76N/wRsbf/Q552dW3R1t1ytydFfD52ew2ls70Vybi8CbJy+FGF1dvuxBSjlaoxJexVvQf/KvqgJg7QJXfhVoIUfsuvKWnz54FsIZ+Lrmrj8/HHRFBDBtrWDprJS/eNIHUzbtLD7vyXXz9+mzu7Ta6TvbCFBcF2FAXQAXZ7jAlycvhxtEa/8uX97U7MeShBENF9kLVX4CKHVqyJ9O+icQuyH0GvftMdP6UGotHu7dh5ryCmfUQZvbT6OK/a+zeQoSTBkfUZ8SHd2PYjNJlwbB/ifCQJwZy2rkn+zy/dadjiU30X4PKMBS/fLPc+uagTgzTY7Lxj//46tXvKvcEQqhe4yQS6vufkOop9tCqvVQP90WSl8OMsjWCiHPxPZfEBo421hYCdYSyNQRnJ3D/A7jKPqgz0dmj0HkfhjQGrTVmzv/QaedC3utQ8BXkv4fe2w8z425rVZUQooyu/TszYeVznH9dd1KaJZPUKJEuF3fkhUVjuPqRy/ye64x00v++i3w+rgzFESc0K5MYHUybmq/fmF3p+EPFZrdx8dDzffZMKUMRlxxL1/6daziy8CEVdg9DKn4MOn0teHZQds8gGxiJqIQXais0n3T2OKyPVt7HiHXu8xB1afVucXCg/I8gb/y+u5V9zDULnR2LSngqNPcWIowdddIR3PPmLZU6d+BD/di9dQ8z3ixf0M7msBEVE+lzQu8+e7buxePxYLP5m/xf8wY82I8/vv+b3xetBmDfwiKb3cCw2xj9+X04IyuwwOEwIz0vhyFlq4+q96U1FGM0ABSoJIi5HlXvK5T9iNoOsQzt/hfcf+F7c0ZA54NrQWjur93ovNf9tYCCL9CeXSG5vxCHK8MwcOUVeR3pdhe5Wbt0g99JwQARUc5yuzVn7sli1rsLmf7qdyyf+7vfVU+h4oxwMPa7hxn6yg00P74phs0gKi6S8wafwxvLn6VNt7oxdF9XSc/LYUoZiRB7Jyr2ztoOJTBPWhCNDDB3B25WGcV/gRkoBhNciyB6QGhiEOIw9N+abcz7cLH3BzXWn73Hd+Jhsxt0v+rM0qrhHreHiQ98wFevzcLj9qAMhTY1DY5I4YH37qBN15pNGBxOBxcPPZ+Lh55fo/c9FEjPi6j7bPWDaGSW9CKFQICdry1GkO2EEMH6furPfntW9i019tbGMBR2p6PMvJlXhk5i2v9m4nF7ypy/Z9teHur1JOuW/1ud4YsQkuRF1HnK3hLsJ+P311XFQOS5oQnA3tL/vQEwwe5/jxYhRMXkpOdi+Fluvc8JnVsBVhJjc1hzWxIbJvLMnJE0P87avXr7hp3MnDTfa9VabWpMj8kHYz6rxuhFKMmwkQgLKn4EOn0QvibtqrgHUSoqNPe21UdH9ADXfLxvEmmArSk4Tw/J/YU4XDU6qgEeP8NCAI5IB+NmPcr29Tv5dcYKil3FHHtaSzr1OQ2bff8k3YWf/IRhM3wOM5kek19nrCA3My/gEu2aZJpmuTk7QpIXESaUsz0kvYvOHg2ejfsfMOqhYu9DRftfdlnl+8c/it77B5h7KJvA2AAnKuEFlJIXGCGq07nXnMWkBz/EY3rfWd6wG/S4+iwioyM4uk0Ljm7Twue1stKyMQyFj0sBVkmEnIzcWk9e9mzby2fPf82c9xaRl5VPUsMEet/Yg0uH9SE+Oa5WY6srQv5qO378eFq0aEFkZCSdOnVi6dKlftu//PLLHHfccURFRdG8eXOGDRtGYaHMJRCgIjqhUr5DJX+GSngJlTQZVf+HkCcuYNXHUfW+hOirQe1bjm2HyN6olC9QzrYhj0GIw01i/QRufvZar48ZdoPElHgGPXZFUNdqeGT9gL04dqedxAYJFY6zOv23Zhu3tL2Pr16fRV6Wtb1Bxq4sPh73JUM7PER6akatxldXhDR5mTp1KsOHD2f06NGsWLGCNm3a0KtXL3bv9r4q5OOPP+ahhx5i9OjRrFmzhrfffpupU6fy8MMPhzJMEUaUUihnG1RUH1REF1RQO09X071tKRjxj6Ia/IZq8Cuq4SqMxBdQ9mNqLAYhDjeX3tOHB9+/k8YtG5YeM2wGZ1zckVd/HUdK03pBXefcq8/yO3/GsBl0v/JMomIifbYJNa01467+H3lZ+aU7TZc+ZmpSN+3m3nMeIystu5YirDuUDuGe2506daJDhw689tprgDV217x5c+68804eeuihcu3vuOMO1qxZw/z580uP3Xvvvfz666/8+OOPQd0zOzubhIQEsrKyiI/3X35ZCCFEeNBas3n1FvJzCmlyTCOSKtFDMvXZr3jrofLVuA2bQVxSDOOXPUPDI4NZ3Rga/yzbwB2dRgRsV69JEq/+Mo76zYJL3MJFRd6/Q9bzUlRUxPLly+nRo8f+mxkGPXr0YMmSJV7P6dKlC8uXLy8dWtq4cSMzZ86kd+/ePu/jcrnIzs4u8yWEEOLQopTiqJOP5MQux1UqcQEY8MDFDJt4KylNkw+4MLTv1YZXloyt1cQFYP2KTUG1y0jN5MWbJoQ4mrotZH3uaWlpeDweGjZsWOZ4w4YNWbt2rddzrrrqKtLS0jjzzDPRWuN2u7n11lv9DhuNGzeOMWPGVGvsQgghDk29bzyXXtd1Y/3yjeTnFNKsVWMaNE+p7bCAIHfSBkxT89ucVezcuKvMcNrhpE4tj1i0aBFjx47l9ddfZ8WKFXz55ZfMmDGDJ554wuc5I0aMICsrq/Rr69atNRixEEKIcGOz2Ti+47Gcdu7JdSZxAWh3XpuA2x2U0rBhZXA9NYeikPW8pKSkYLPZ2LWr7H4vu3btolGjRl7PGTlyJNdeey033ngjACeffDJ5eXncfPPNPPLII17XukdERBAREVH9T0AIIYSopKLCIn5f9Bd5Wfk0O64Jx7Q9KuA5KU2S6XHN2cx9fxHBzEZ1RDiqIdLwFLLkxel00q5dO+bPn0+/fv0Aa8Lu/PnzueOOO7yek5+fXy5B2bcTaAjnFQshhBDVQmvNFy99y0dPfkFuZl7p8WNOPYrhk27l2NNa+j3/rtdvZM+2vayc/6ffds5IByef3bpaYg5HIR02Gj58OJMmTeK9995jzZo13HbbbeTl5XHdddcBMGjQIEaM2D+zum/fvrzxxhtMmTKFTZs2MXfuXEaOHEnfvn3r3HbmQgghxME+fPxz3rzv/TKJC8DGP/5j2Nmj2LR6i9/zI6IieGbOSLpc3MFnG6UUFw89n5j4aJ9tDnUhLZIxYMAA9uzZw6hRo0hNTaVt27bMmjWrdBLvli1byvS0PProoyilePTRR9m+fTv169enb9++PPXUU6EMUwghhKiyjF2ZfPTU514fMz0mxa5iJo+cwphpD/i9jlKKR6YM46krX+Ln6cuw2Q08brP0v12v6Mz1Y68KxVMIGyGt81IbpM6LEEKI2vDFS9/y5v3vl+5W7Y1Sis/3vB1UmX+tNX/9tJY5731PemoGKU3rcd6QbrTudCxKBd6wMtxU5P1b9jYSQgghqkH6zgxsNgO3nw2UtNZk7s4OKnlRSnHSma056czDd26LL3VqqbQQQggRrpIaJQbcP0kpRWJ9GRWoKklehBBCiGpwzpVn+h3OMWwGnS48jfh6sjN0VUnyIoQQQlSDeo2TuHLEJV4fM2wGDqed6564soajOjRJ8iKEEEJUk8FjBnDj09cQHR9V5vgRrZvywqIxtDzlyFqK7NAiq42EEEKIauYqcLFy/mrys/Np2qoJrdq1PCRXCFUnWW0khBBC1KKIqAhOv7BdbYdxyJJhIyGEEEKEFUlehBBCCBFWJHkRQgghRFiR5EUIIYQQYUWSFyGEEEKEFUlehBBCCBFWJHkRQgghRFiR5EUIIYQQYUWSFyGEEEKEFamwKw5Z2r0BnfchuBYCHnCchoq5FuXsUNuhCSGEqAJJXsQhSRfORmcOAzTgsQ665qJdsyB2GCr2ttoMTwghRBXIsJE45GhPakni4qE0cYHSf+vcl9Cun2sjNCGEENVAkhdxyNH5UwETq9fFGxs6//0ajEgIIUR1kuRFHHqKlmIlL754oGhZTUUjhBCimknyIg49KphfaxXyMIQQQoSGJC/ikKOcXfD/q20DZ5eaCkcIIUQ1k+RFHHqi+gNOfPeumKiY62owICGEENVJkhdxyFG2FFTSBCCCsr/iNkCh4h9DOU+tneCEEEJUmdR5EYckFdEF6s+xVh65FgFucLZHRV+Nsh9T2+EJIYSoAklexCFL2Rqh4u6GuLtrOxQhhBDVSIaNhBBCCBFWJHkRQgghRFiR5EUIIYQQYUWSFyGEEEKEFUlehBBCCBFWQp68jB8/nhYtWhAZGUmnTp1YunSp3/aZmZkMHTqUxo0bExERQatWrZg5c2aowxRCCCFEmAjpUumpU6cyfPhwJkyYQKdOnXj55Zfp1asX//zzDw0aNCjXvqioiJ49e9KgQQM+//xzmjZtyn///UdiYmIowxRCCCFEGFFaax2qi3fq1IkOHTrw2muvAWCaJs2bN+fOO+/koYceKtd+woQJPPfcc6xduxaHw1Gpe2ZnZ5OQkEBWVhbx8fFVil8IIYQQNaMi798hGzYqKipi+fLl9OjRY//NDIMePXqwZMkSr+d8/fXXdO7cmaFDh9KwYUNOOukkxo4di8fjCVWYQgghhAgzIRs2SktLw+Px0LBhwzLHGzZsyNq1a72es3HjRhYsWMDVV1/NzJkz2bBhA7fffjvFxcWMHj3a6zkulwuXy1X6fXZ2dvU9CSGEEELUOXVqtZFpmjRo0ICJEyfSrl07BgwYwCOPPMKECRN8njNu3DgSEhJKv5o3b16DEQshhBCipoUseUlJScFms7Fr164yx3ft2kWjRo28ntO4cWNatWqFzWYrPda6dWtSU1MpKiryes6IESPIysoq/dq6dWv1PQkhhBBC1DkhS16cTift2rVj/vz5pcdM02T+/Pl07tzZ6zlnnHEGGzZswDTN0mPr1q2jcePGOJ1Or+dEREQQHx9f5ksIIYQQh66QDhsNHz6cSZMm8d5777FmzRpuu+028vLyuO666wAYNGgQI0aMKG1/2223kZ6ezt133826deuYMWMGY8eOZejQoaEMUwghhBBhJKR1XgYMGMCePXsYNWoUqamptG3bllmzZpVO4t2yZQuGsT9/at68ObNnz2bYsGGccsopNG3alLvvvpsHH3wwlGEKIYQQIoyEtM5LbZA6L0IIIUT4qRN1XoQQQgghQkGSFyGEEEKEFUlehBBCCBFWJHkRQgghRFiR5EUIIYQQYUWSFyGEEEKEFUlehBBCCBFWJHkRQgghRFiR5EUIIYQQYUWSFyGEEEKEFUlehBBCCBFWJHkRQgghRFiR5EUIIYQQYUWSFyGEEEKEFUlehBBCCBFWJHkRQgghRFiR5EUIIYQQYUWSFyGEEEKEFUlehBBCCBFWJHkRQgghRFiR5EUIIYQQYUWSFyGEEEKEFUlehBBCCBFWJHkRQgghRFiR5EUIIYQQYUWSFyGEEEKEFUlehBBCCBFWJHkRQgghRFiR5EUIIYQQYUWSFyGEEEKEFUlehBBCCBFWJHkRQgghRFiR5EUIIYQQYaVGkpfx48fTokULIiMj6dSpE0uXLg3qvClTpqCUol+/fqENUAghhBBhI+TJy9SpUxk+fDijR49mxYoVtGnThl69erF7926/523evJn77ruPs846K9QhCiGEECKMhDx5efHFF7npppu47rrrOOGEE5gwYQLR0dG88847Ps/xeDxcffXVjBkzhpYtW4Y6RCGEEEKEkZAmL0VFRSxfvpwePXrsv6Fh0KNHD5YsWeLzvMcff5wGDRpwww03BLyHy+UiOzu7zJcQQgghDl0hTV7S0tLweDw0bNiwzPGGDRuSmprq9Zwff/yRt99+m0mTJgV1j3HjxpGQkFD61bx58yrHLYQQQoi6q06tNsrJyeHaa69l0qRJpKSkBHXOiBEjyMrKKv3aunVriKMUQgghRG2yh/LiKSkp2Gw2du3aVeb4rl27aNSoUbn2//77L5s3b6Zv376lx0zTtAK12/nnn384+uijy5wTERFBRERECKIXQgghRF0U0p4Xp9NJu3btmD9/fukx0zSZP38+nTt3Ltf++OOP588//2TVqlWlXxdddBHnnHMOq1atkiEhIYQQQoS25wVg+PDhDB48mPbt29OxY0defvll8vLyuO666wAYNGgQTZs2Zdy4cURGRnLSSSeVOT8xMRGg3HEhhBBCHJ5CnrwMGDCAPXv2MGrUKFJTU2nbti2zZs0qncS7ZcsWDKNOTb0RQgghRB2mtNa6toOoTtnZ2SQkJJCVlUV8fHxthyOEEEKIIFTk/Vu6PIQQQggRViR5EUIIIURYkeRFCCGEEGFFkhchhBBChBVJXoQQQggRViR5EUIIIURYkeRFCCGEEGFFkhchhBBChBVJXoQQQggRViR5EUIIIURYkeRFCCGEEGFFkhchhBBChBVJXoQQQggRViR5EUIIIURYkeRFCCGEEGFFkhchhBBChBVJXoQQQggRViR5EUIIIURYkeRFCCGEEGFFkhchhBBChBVJXoQQQggRViR5EUIIIURYkeRFCCGEEGFFkhchhBBChBVJXoQQQggRViR5EUIIIURYkeRFCCGEEGFFkhchhBBChBVJXoQQQggRViR5EUIIIURYkeRFCCGEEGFFkhchhBBChJUaSV7Gjx9PixYtiIyMpFOnTixdutRn20mTJnHWWWeRlJREUlISPXr08NteCCGEEIeXkCcvU6dOZfjw4YwePZoVK1bQpk0bevXqxe7du722X7RoEVdeeSULFy5kyZIlNG/enPPOO4/t27eHOlQhhBBChAGltdahvEGnTp3o0KEDr732GgCmadK8eXPuvPNOHnrooYDnezwekpKSeO211xg0aFDA9tnZ2SQkJJCVlUV8fHyV4xdCCCG80e5tYO4ElQj2Y1BK1XZIYa0i798h7XkpKipi+fLl9OjRY/8NDYMePXqwZMmSoK6Rn59PcXExycnJoQpTCCGECJouXoOZfi06rTs6/Wr03j7ovReiCxfWdmiHjZAmL2lpaXg8Hho2bFjmeMOGDUlNTQ3qGg8++CBNmjQpkwAdyOVykZ2dXeZLCCGECAVdvAa9dyAULSv7gHsDOvNWdOF3tRPYYaZOrzZ6+umnmTJlCtOmTSMyMtJrm3HjxpGQkFD61bx58xqOUgghxOFCZz8FuADz4EcAjc4ajdZFNR/YYSakyUtKSgo2m41du3aVOb5r1y4aNWrk99znn3+ep59+mjlz5nDKKaf4bDdixAiysrJKv7Zu3VotsQshhBD7aG1iulZA8VLKJy4HNswElwwfhVpIkxen00m7du2YP39+6THTNJk/fz6dO3f2ed6zzz7LE088waxZs2jfvr3fe0RERBAfH1/mSwghhKgOWpvovA/RaedCxsAgzjDAI6tjQ80e6hsMHz6cwYMH0759ezp27MjLL79MXl4e1113HQCDBg2iadOmjBs3DoBnnnmGUaNG8fHHH9OiRYvSuTGxsbHExsaGOlwhhBACAK01OnskFHwGBLuSyAQjMYRRCaiB5GXAgAHs2bOHUaNGkZqaStu2bZk1a1bpJN4tW7ZgGPs7gN544w2Kioq4/PLLy1xn9OjRPPbYY6EOVwghhLAU/VSSuIA1pyUYTojwvsBEVJ+Q13mpaVLnRQghRHUwM4aCawHgCfocFXs3KnZo6II6hFXk/TvkPS9CCCFEWHKvJ/jExYmKvRVibg9lRKKEJC9CCCGENyomcBsjBRV7H0T2QBnS219T6nSdFyGEEKK2qKg++J+oa6Cir0VFXyqJSw2T5EUIIYTwJupyMJIBm5cHbaDiIeqKmo5KIMmLEEII4ZUyElHJH4CtcckRO6WzLYyGqOQPULZ6tRXeYU3mvAghhBA+KPsxkDIXXIvRRb8AGuXsCBHdUOrQeAvVRcvQ+R9D8V+golCR50PUFXU6MTs0fvJCCCFEiChlg8hzUJHn1HYo1Uprjc55FvLfxhoas1ZW6dx/IO9tSH4P5TixVmP0RYaNhBBCiMNR4bcliQuUXRJugs5FZ9yI1q7aiCwg6XkRQghRIVprKPoJnT8VPJvBSEJFXgRRF6JUZG2HVyO0Loain8GzB2wNwNkl7IaRdN5bWKupvNWqNcHcC4XfQVS/mg0sCOH1kxZ1ytZ/trP6x7UopTj57NY0PaZx4JMOI1prVi5YzbwPvmfvjnRSmtXjvMHdOOXsE1Aq2H1ShKhbtHajs+6HwhnsH2pQ1nyQvEmQ/AHK1qCWowwtXfAVOmccmOn7Dxr1IO4RVNSFtRdYBWgzH9xrArSyoYuWoiR5EXVZUWERWmsioiL8tsvYlckzg19j+Zzfyxzv2PtUHph8BwkptVfvQGvNnq1pFOQW0uCIFKJio2oljqLCIsb0f4GlM1Zg2A1Mt4nNbjBn8iLOuKQjj3xyDw6no1ZiE6JK8t6Ewpkl3+wbaij55O7Zgs68C1VvSm1EViN0wddW8nYwcy86azgoGyrygpoPrMKC3Rmobu4gJHNeBIs/X8Kdp4+gT/TVXBhzDbe0vY857y3C27ZXBbkF3NttNKsW/Fnusd/m/M593R/DVVA7Y6Q/f7WM2067n6tb3M6NJw3nsgY38NLNE8jck1XjsYy/512WfbcSANNtAuAp+e/PXy3jrQc/qvGYhKgqrYvQeZPx/YbmgeIV6OLVNRhVzdHajc552n+b7HFoHfxeSLVFGTFgb4X/InwelKNDTYVUIZK8HOYmj5rCE1e8yLrf/i09tmn1Fp67bjyv3vFWuQRm7vuL2bpuR+kb8YFMt8nm1VtZ+MlPIY/7YDMnzWP0Jc+y8c8tpceKC4uZNXkhd3V+hKy07BqLJXNPFrPfWYg2vb/Aa1PzzZtzyM3Mq7GYhKgW7g2gA30YMKDolxoJp8YV/Qpmmv82ZioUL6+ZeKpIxVyP70TUAJUIUb1rMKLgSfJyGFvz63o+evILAMwD3mj3vel+88Ycls1aVeac2ZMX+s3TlVLMfm9RNUdaVlZaNr/OXMHS71aSnZ5D9t4cXr3TmjF/cMJguk12/beHDx//PKQxHej3RX/jcfv/5FVcWMyfPwQabxairin/oaU8BTqYdqGhi9dg5ryMmf0UOv9jtJlTfRcPlLjs4wmyXW2LvASirin55sAqwoZV7yVpUp2dgC1zXsLEzk27mDlpPht/30xEtJPOF3Wga//OOCOdlb7mNxNmY7MbXntRAAybwVfjv6PjBaeWHsvYlYmX0aRSWmsydmb4fOyvn/9hzuRF7N2RTnKjRHoO7sZJZx7Pn4vXMPOteWxbt5P4erF0v/Isul5R9vnlZefz+j3vMv/DH0qTA7vTTqt2LXEXu33GZHpMZr27gJueuxZnROjnmbiLfMdSmXZC1Bn2o63NCrW/XkMPOE/183hoaDMPnXUvuBZgvRErNB7IHgcJT1TPpFOjYXDtbI2qfq8Q0WYGFC0HPOBog4ofCZHd0fkfQfEaUJEQeT4qeiCqDj8PSV7CwFfjZzH+7ndQSmF6TJSh+OGLX3lv1FSenTeKJkdX7hds/fKNPhMXsN70N6zcVOZYgyPqk7Y93eeQiGEzaNiifrnj7mI3T1/zCt9/tqQ0YbLZDWa9u5D6zeuxZ+ve0uOGofht9u9MeWYaz80fTXKjJIpcxTx03hOsW74R07M/ZneRm79/WRfwuRbmucjcnUWD5ikB21ZVq/YtAzdScGy7INoJUYcoFYWOGgj57+K9F8ZmJTiO9jUdWknisqjkuwN7Pl3orAfBqIeKOKtqN3F2AKOxNTTkdbhFga0ZOGo+eQtE60J09lgo+BzY98FJQUQPVMITGElv1GZ4FSbDRnXcstmreO3Ot9GmLn3T3pc47Nm2lxHnP+m318GfyGj/q4qAcj07fW7q4TNxASvh6X1jj3LH3x7xMYs/t8bBPQdNYN2zdW+Z7/cNYW1bv5MnrngRgAUf/8japRvKJC6lNEFNiI+Oq5mVR82Pa0qbc07EsHv/87LZDTr2Po1GLQ7t5aTi0KTi7gFnx5LvDvwdN8BIRiW+VuOlAHTx2pIeF18fxhQ699Uq30cpm9VTUXLNg+8BoOJH1rlSCFp70Bm3QcGn7E9cADS4FqD3XoU2c2srvEqR5KUO0lqzdul63hg2meeG+H4hMD0mO/7dxZJvKjc57IxLOqEM339khs3g7Ms7lznWbWAXTuhyHIat/K+OYShO6XoCZ17aqczxvKw8vn59ltfVS/6YbpPVP65l/YqNfPfWPL+x+mPYDNr1PIXYxJhKnV8ZD7w7lORGSeV+ToahSGlWj2Fv3lJjsQhRnZSKQCW9jUp4Ghwng0oC21Go2LtRKd+g7C2q7V7azEZ79qIDzKHRhbPxvvPzPiYUr8Is/AEz6zHM9BswM+9Du74PeO2DqcgeqMQ3wNa87AO2I1BJE1ER3YK+lvbsRRevRx9YLyYUXIuh6Ce8J3ce8GyCgs9CG0M1k2GjOsZV4OKpK//Hkq+XYdgM7z0NB7DZbSyduYKzDkoY9snJyGXeB4vZsGoTDqeD0y9sR4cL2mKz2bjghu58+ux08rILyt3HMBSOCAcX3d6rzHGH08HTsx5hwr3vM+e9RaXzNhwRds6/vjs3PzcIm73si8gfi9dQVFhc0R+FFYfNYMW8P9m9Jc1vj48/WmuuGXm538dXLVzNut824nDaaX9+W444vmml7rVPgyPqM2HFs0x/9Tu+e3sBmXuySG6YSO+benDR0F7EJ8dV6fpC1CalHBB1KSrq0pBcXxfOQee+Ce6SkgxGA4geBDFDUMrLPD+dj/8lvyUyb2B/YT0buvBra4graSLKiA06PhXZHSLOgeI/wNxtxec4JegeF128Gp3zQklCAaDQEeegYu9FOY4NOg6tPeCai87/BNybwIhHRV4M0f1RRuL+dgVfcODeRV6vlf8pKua6oO9d25Su6MfhOi47O5uEhASysrKIj6+9YmmV9czgV5n/0Q9Bv1EbNoPuV53Jg+/dWe6xH6f9yrhrXqG4sBjDpgCFx+3hiNZNGTfrURo0T2HDyk2MuOApMndnYbMbaA2maRKTEM0TXz3EyWe19nnv7PQc/ln2L0opjutwNHFJ3v/4f/jiFx7v/0JQz8fb8xs8ZgA/TvuV9Ss2Vqpe0g1jr2LgQ5d4fWzTn/8xpv8LbF+3E8NmWBuVmZqOvU9lxId312hvjRACdN7b6JxnsAYGDvxQpcB5JippgpU8HXhO/lR09kgqx4CIHhhJr1Xy/IrRRb+h04dgDd8c+PxsoJyo5E9QjhMCX0cXozPvLBkuO/BnpcBIQSV/VNoLZqZdtj8R9EXFYTSs3SXeFXn/lmGjOmT3lj3M/zD4xAWs+S+t2h1d7vg/yzbwxBUvUlxYjNYaj9ssXaGzbf1OHuz5OO5iN8ecehQfbX6dBybfQferz+Lcq89i2IRb+GTrm34TF4D45Dg69GpL+/Pa+ExcAI457aigPhR5Y3pMTujcirbnnFSpxEUZCmeU9xVZu7fsYVjXUez8d1fpvfb97H+b/TsP934Kj6fuF5sS4lCh3ZutXY6B8kMcGop+QOe8UH4IOrIPEEXlXmhMq/fCvaXcI1q70PlTMNP6Ye5qh7nnXHTu+EoP82it0VkjKJ+4AHhAu9CZw9DFfwceZs97E1wL9z+H/XcBMx2dcdv+a9gaEvDt3gj9YobqJMlLHZG9N4c3hk2u0LwQpcAR6aDnoK7lHpv67FcohdfrmW6Tbet2suTr3wBrUm7PQV154N07eGDyHfS+qQdRMdW3tr/xUQ3p0KttheesGDaD5sc3ITImgumvflepe2utcUQ4ME2TPxb/zax3F/LT9KUU5rv44qUZFOQUeh2aMz0ma35Zz7LvVlXqvkKIitMFnxLwbSn/HfTey9Ce1NJDyohFJYwt+e7g84N8myv6oWwsZh46/Rp09ihrDyCdA56t6NxX0Wl9vSY7ARUvB89/+J5YbIJnE3pvP/TeC9Eu78X+rErH7+O30rHnX6uoHqCiLvFzTwCFiu4f3HOoI2TOS5B2btzF5r+2EhEdwUlnHFel+ioH+/7Tn3lm8GsUFwU/L8RmNwDFI5/cU25oQ2vNz18v87sM2rAZ/PTVUs667PTKhl0hwyfdyq2nPUDWnuAq3Ro2g7ikGEZ/cT/PDRkfsOibPzEJ0QxpdRc7N+4qPRYVF4np0X7nFBk2gwWf/MDpF7ar9L2FEBVQvA5/8zJKudeg0wdBytelRdRUVB8wktG5r0HxMkoOgvMscM0JcEGFdv2Mzp8CnlTAZiUr7HtNPjBJMK2ejcy7od6XFVtZ5N5cgbYb0BnXQdI7qIjOBz32H+jMABewWT+HiNMhojs4OpRU/j34Nc8GtqYQNSD42OoASV4C2PFvKi/fOpGV8/ePF8YmxjDgwX4MeODiKi+J+/uXdTx11ctWD0mQnS4R0U7OvLQTlw/vyzFtjyr3uOkx8RT7fwHQpklRQVFlQq6UlKb1uOqRS3njnskB2ybUj6fvredx0e29yE7P5Z9lGyp1T8NmcMrZrXluyGt4DkpSCnIKA55vekyy06qxOqcQwj8jhvJzXbzxgGeztUHkAZOGVURnVERntJmBNrOh+M+S3odA17SGjqxhp2BeiD3g/suasOtsE0T7EkZFJuprwERnPwEpM8q+1wT9vlOyfFvZIGkiOvtxKPyKMj8L5xmohHGoCsVW+yR58WP31jTu6vIIOell17/nZubx9oiPyNqTzS3PD6rSPT599isMQ+FxB/6DsdkNzrr8dB75eFiAdjaaHtuYHRtSfQ5DKcPgqJOOrFTMldWkZXDF9MZMe4ATuxwHUK5IXkWcfFZrXAVFmKau1Eolm92odAFAIUTFqYie6MJgh4gNdMEM7yuePHsg4yYwd2K9zYViB2UDildWLHlxnmn1BumC4OPxbLASJcdJ+w/bjgQjGfzOvfGAc/8qVGXEoBKfQXvug6JllFbYtR8RfPx1iMx58eOTsV+Sk5Hrc2jh85e+KTMUUVGmafLLt7/5Hd7Zx7AZpDStx20vDgnq2v3uuIBAf4jn39A9qGtVl3bnnUJcsp/liAoatqhP69P3LxWMr1f5TwP977uINb+sD7jc3BeP26zxn5EQh7XI88DWAv81W/YxQZcfhtZmBjr9WmsJM2BNjvWz+WCl6Qqfr4wYVMytFb+VZ2fZ6ygHKnoIvico28B+nNdKx8pWHxXVGxXVN2wTF5DkxSd3sZs573+P6W/eiGEwpwqbEJoeM6jExWa3cendfXht6TiSGyUFde0Lb+3JaT3blBvW2lc07a7xN1K/Wb2KB10FDqeDm5+91ncDDbc8PxjD2P9reWy7ljQ6quKVaA2bwa8zV1QmzFJNjm3kdSXXhlWbeO668VxW/3ouThzEg+c9zi/fLq9wET4hRFlKOVHJk8FWfji8PBvYyv99kv9ZyXwQf0PnytoxOeLsyoRZQkNEl4qfFnOr9VWy/1JQDC+v1TE3QsQFJd/sS/askhgYDVGJb9S5Sr/VSZIXH3Iz8wLOCVFKlZa2rwy7w07TYxv5/QVTCq5+9DJueX4QifUTKnTtJ75+kJuevZYGR+xfAnfK2Scwbtaj9Lm5Z6XjroyM3Vm8++gnTB45BZvdKLfyKKlhAo9OHV6u2J5hGNz0zDVUlFKKosKqzenZ/V9auYTk+09/ZmiHh5j/0WKy9+aQn13AqoV/MfKip3nzvvclgRGiArTW6KJVmFmPYKYPwsy4B4r/hnrTIXpIgLM9qOgryl+zcCYBh3/sx2A0XIqKuqxygWOzas7Yj6nwmUopjLjhqPqLIe4BIMA2LUYTcLT1ch07KvElVNJEiOhq9Vg5TkHFPYpK+RZlb1bh2MKJzHnxITo+GrvDhtvvxFdNUsPgEwpv+t3Rm9fvecfn44bN4IIbz63UtR1OB/3v7cvlwy8kPzsfu9NORFTg/Yyq245/Uxl21kgy92SXHcJRkNQwkdtfHsJZl55erjLvPmdf3pkH3ruD8Xe9Q15WflCVhz1uD7PfWei3TSCeYjda69LkMm37XsZd+wqmaZZ5bdwXyxcvfcvJZ7XmjH4dvV1OCHEArT3WMuSCzyhT9dY103qzTpwA7nVQ9AteJ9tGXY1ynublwv52vC5hlrRxtCNQ5Vmv7MeiEp8Purn2pELhDLSZjrI1hcg+1vBNzA1oFYfOftTnuSp+BEp572dQSkFEtwptSXCokJ4XH5wRDroNPKNkSbJ3HrdJj2ur0u1oDe+0O8/H8I6CeybcQkqT5CrdQylFTEJMjScupmnyy4wVDO3wIOmpmeUTDg1Ze7L5adpSn4nLPj2v7cqnOycx8tPh3PTMNbTr2abSex0FQylodlyTMkNYMyfNtyb++vhQZ9gMpr0yM2QxCXFIyZt4wH46nrL/Lf4TskdYvQoxt4E64EOi0RgVNxIVP8r7de2t8D9nxgYOa0GAsqVA5MUE/VZoNEclvIiq9znKCPy6rLUHM/tJ9J5u6JznIG8yOnsMevcZJXVaQEVfgYp/AtRBFWVVknWvyF5erixkewA/tq3bwe0dHsSVX1TujVcpxXlDunHf27dX6R5gza/56rVZTH/tO1I37UYpOK3HKQx4sB+ndj+5ytevDRm7Mrmr88Okbt4TsK1hM/hk64Sg5/MA5OcUcH/3x1i/clOlVhLF148nOy3bd++ygjtfvbHM3k4jzn+S3+b87ve6jkgHM/M/rnA8QtQ1Wrut1SwqqtqX0WpdhN59Bugsv+1UymyU/Si0LgLPVsAOtuY+eyIAtOsnqz6Kv+sesIGiNvPQGTdB8W94X1JdciyyDyrhOZQKfsDCzHkO8ib5jiPhWVRUPysO7bI2UDT3gtEQIs4stw3Coa4i798ybORHs1ZNePH7x3lm0KtsXr219LjdaefioedXai6GN3aHncuGXcil9/ShMN+F3WHD4QzfX1qtNXef8WhQiQtYwy7//v5fhZKXqNhIRn1xP3PeXcic9xey+780HJEOXPmB57mM+OhuuvbvzIyJ83j1jrfKDUMppWjfqw29byo7XGfYjYBlIGxedtsWIpxoMxed9ybkf1K6mkc7OqJib0dVZoKqN8VrAiYuoMD1I9iPsjZjtHuZnOuNs4tVcK1gKmX/YEv+HXkZOPdXJVdGDCS/D6756PwvSpZXOwEHKBvYm1tzYxztKzQBVpsZkDfZf5uclyHyIpQyUCoCImt2LmI4k+QlgGPaHsXE319g7dIN/FdSYbfdeaeEZFdgpVS1luWvDXlZeSyc8lOFl5DbHWW7ebXWrJz/Jz9/tQxXQREtTzmSHteeTWxiDDMmzuPzF79h+3pr+WCz45pwz5u3kJ9dwJv3vx+wJyapYQI2u42Lbu9Fo6MaMOWZafy5eA0A9ZvX45I7e3PJ3b2xO8r+ebQ/ry3LvluF9pG92OwG7Xu1rdDzFqIu0WYuOv1qcP9DmR6I4t+s3oyEZ1FRF1fDndxBtFFBtjvoLKUg/nFwnIjOews8JWX8bc1QMddD1JUopazJ9cXLoOh3UAY4z8RInljh+x1Me1LR+VOh8Bv2V+j1wdxRUsMlPHvYa1ONDBuNHz+e5557jtTUVNq0acOrr75Kx46+JzV+9tlnjBw5ks2bN3PsscfyzDPP0Lt376DuFe67Soeb7L05fPf2AhZ9+hOpG3eTl5VHRX+jImMj+XTnpNLELWNXJo/0Gcf6FRtL58KYHhO708aJXY5n1cLVZT5QWXs4wandT2blggA7pwJvrHi2XGXiwnwX7iI3MQnRPj9d5WbmcU3L2ynILrQm7R5Mwcs/PFlaYE+IcGPmvGjNRfFZjdaJavAjykis0n20mY3e3QUIsKIz+WOUs3ytkqDvo7U1DIO2dlou+dvW7g3ojDut/X9K57uY4OxsreAJYj6L1/sVLrR2eva68aJ3Kum98uX/D1N1alfpqVOnMnz4cEaPHs2KFSto06YNvXr1Yvfu3V7b//zzz1x55ZXccMMNrFy5kn79+tGvXz9Wr14d6lBFBW1YuYkhre7i7Yc/YsOKTeRmVjxxUUrR744LShMX0zR5uPdYNv6xGbBWDXncHrTWFLvcVuICZYZu9t1z5YI/UTY/3boKmhzTiKPbtCj3UGR0BLGJMX67hWMTY3h61qNExUeWaWfYDAxDMXzirZK4iLCltdsaKvL7plsMBV9V+V7KiIeofvieWGsD27Elq4GqcB+lULYUa2XPvsTFsxu992prewHAer4lz7loKTp9iDXHpoK0ews68w6s3pYKFMasQqE4rT3WHJ/8L6zEqRJxh6uQ97x06tSJDh068NprrwHWm1Pz5s258847eeihh8q1HzBgAHl5eXz77belx04//XTatm3LhAkTAt5Pel5qRlFhEVe3uJ3svTmVrmAL0PWKLoz48K7SHpZls1fx8AVPVepaylABh4xGfjqcsy+v2qec7PQc5kxexK8zVlBc5OaE04+lzy09aXpM4ypdV4japD1p6D2B5rTYIeoyjIQnqn4/M+eAIaoD/25toOJQ9T6uVB2VQMycF0om0fp53bIdjYq9FSIvsObbBHPd7GcgfzLBL7u2gbMTRvLkINuXpQsXoLNHg3nAEL1KQMXdh4oOr00W96kzE3aLiopYvnw5I0aMKD1mGAY9evRgyZIlXs9ZsmQJw4cPL3OsV69eTJ8+3Wt7l8uFy+Uq/T47O7hdi0V5xUXF/PD5L8x9/3vSUzNpcGQKF1x/Lp0uPA2brewnpO8/XULm7kAT7nyzOWw8M+dRTjn7xDK9GPuWTVdmF+lAiUujoxpUOXEBiE+O4/Lhfbl8eN8qX0uIOkMFOd9ORVXP7Yw4SJ4CBZ+g8z+xSuAb8RB1KSr6WpQtRPuKFRy0MaE3nn/RWfdD/sfWrs5GTODrFi2mQomLiva93DsA7foenXmblwey0NkjARMVfWWlrh0uQpq8pKWl4fF4aNiwYZnjDRs2ZO3atV7PSU1N9do+NTXVa/tx48YxZsyY6gn4MJabmceDPR9n3fKNpT0Ym//ayi/fLKf9eW0YM/0BnJH7P4GsXPhnUMXifHlg8lDadN2/0Vj23hx+nbGC9Ss3ep9PUg1SN+1my9rtHHF805BcX4hwo80sKJiGdv0EeMBoZk0i9fnm7q7WuiPKiIaYG1AxN1TbNQPysh+ST8W/o3PGohJ89wZr9xZrd2lPBT7MGfUh6V2UvWXw5+y7n9bo7LH7vvPeJuc5iLoEFWxCGobCfrXRiBEjyvTUZGdn07x581qMKDw9f/3rbFi1Gdjfg7EvMVk+7w8mPfghQ/93fWl7bepKJRnR8VHc/cZNdL/yLAA8Hg9vPfQR01+ZGaCacfVI257uN3kpyCvku0nzmTFpLru37iWhXhy9rjuHi27vRUKKDEOKQ4cuWonOuBF0LuWWE3tlA8ep4PBS1Tac2JqCewPB7SBtWsld3H0oo2wpB+3Zg856uKTHpYKzL8xdqOJfwGEt/9ZmLhR+jXavt3pkInpaOz57m4PnXgOeTf6vr3PB9T0ESDS1mQ75n6OLfgDtAedpqKiBYbG1QEiTl5SUFGw2G7t2lV02u2vXLho18t4l2KhRowq1j4iIICKi5kveH0p2btrFT18t9fn3p03NzEnzGPL4AGISrO7TE05vxbwPFgd9jyFPDqR1p1a06XZCmSGo1+9+l2/emF2hib6lyxwrwd92DjkZudx3zmNs+nOLtRxaQ2FuIR8+8TkzJs7lpR+eoPFRDX2eL0S40GY6OuMG0PmU/cM/8N8Ka0KtBjzgOA2V9HrYb/anoq5E51Rkzo4bXfQ7KrJb6RFrvs5V4NlGhROXfdfIew+irgLXLHTmg4CLfT9vnTcJHJ0g6TWUcdBrlpkW3A0CtNNFy6zifLqA0udQvBKd93bJkvi6PSwe0tVGTqeTdu3aMX/+/NJjpmkyf/58Onf2Pvegc+fOZdoDzJ0712d7UXV/fP93wL+/osJi1vy6ofT7bgPPCPr63QaewdUPX8Zp555cJnHZuWkXX1cgcVGGomWbI0lsYPWA2By20l2yA56rFEedfAQtTvTdK/fGsMls/murlRgdtHdR5u4sxl31v+ACFaKuy/+8JHHx1XtqWL0sUf0hejAqeQoq+cPyb6ThKLo/ONoQ9I7OAO6DpjkUfFpSP6ayvcUaPJvRrh/RmcOwEheNtcR63xYJv6Ezhpb/oGYE+QHK8D1nyEpebwJdSNkXfw/gQWfdjy7+O9gnUytCPmw0fPhwBg8eTPv27enYsSMvv/wyeXl5XHedVb550KBBNG3alHHjxgFw991307VrV1544QX69OnDlClT+O2335g4serFg4R3wZbX1wcME8UlxVK/eb2Au2ofdfIR3P+O9y0UFn7yE4YR3LyZhJQ4+t3Zm/739cWwGfz45VL+XPw3a5dt4N9VmwNfQ8Etzw/y+akxKy2bBR//6PM6HrfJml/Xs2HVpnI1YoQIN9q1CP+TVk3wbMGoN6WGIqo5SkVA0rvo9MHg/iO4kwoXQ+ytpd/q/E+pbI9LGflvYSVR3v6/8EDxUiheAc4DlozbW4H9OHCv93EeoGLRzjN8p2f5n5UkLr5+BxQ6731U4tPBPY9aEPI6LwMGDOD5559n1KhRtG3bllWrVjFr1qzSSblbtmxh586dpe27dOnCxx9/zMSJE2nTpg2ff/4506dP56STTvJ1C1FFrTu3CtjGZrfRqn3Z8twDHujn9xxlKMbOfLjMRN8DZadlYwSxuaJhMzihy3FcM/JynJFOcjPyaNv9JO4cfyOn92lHML3Yd79+E+16tvH5+L+rNgde4aRgzS/rA99MiDoviHogOkB12DCmjBhUwmPBn+D+DTPjdsyiVZjZj+2v2ltpBtjbQdES/Pfe2NGFs8scUUqh4kdiJT0+Xvx0Luy9EF3sfWGMdn2P/+TVA65Ffh6vfTUyYfeOO+7gjjvu8PrYokWLyh3r378//fv3D3FUYp8jWzejbfeT+HPx33jc5X+hDZtB96vPLDdhtc/NPVg6cwXLZq0qnSOyr71pmgx78xZSmtbzed8GR9THHcSSaNNj8ss3y/nwic+Z9e4CdpXsmdT8+CaceUknrzEfKKVZMhfceK7fNkENP2kC7n4tRFhwnArFf+H7jdMGjrY1GFDNU46T0I7TofiX4E5wzQfXPKx5KYFet2yAA6tgnbe2JsRcCVnLA99X55c7pJwdIWkyOvuxkirBXni2o9OvgZRvvSw7D2bbhdAvoKgK2UVOAPDg+3fS4Ij6ZYZVlFKg4Og2LRj6cvldWu0OO2OmP8CtLwymUYsGJedA23NO5Jk5o7jghvIJg9aav5f8w7RXZlJc7A6651VrzXujp7Lrv/2bPW77ZwefjJtGYsMEbHbfv8oD7u+HYfj/VT+u4zFExQZYVqjg1HOlB1CEPxU9kECfvFXMoJoKp1pordEFMzHT+mGmtra+dnfFzH0d05OGLv4TXbwerQ/YhDXpFb9zQw66Q8l/g3lT90D8w9aSaOtOJf+1AQoV9wgqsjeoQHOIPCgfG1KqiE4Q431IvjQGnYvO/6D8Q452+K5uXBKn49QAsdWuGtnbqCYdjhV2C/Nd/PzVMvbuyCC5USJdLm5PVGzFC0nlZeUx860FzH53AZm7s6h/RAq9b+zBeYO7EhHlf0WX1trrjthFrmK+e2s+X78xm+3rd1pLrD1mlVYMedPgiBR2b0krrVFjsxt43CYX3d6LO169IagVEm+P+Iipz37lNS7DZtD5ovY89sX91RazELVJ539iVWgt05NQ8u+YmzDiau93XWsNRd+j86dYcztUHCqyD0RfXm7J8r72OvtRKPgs8MVtzVAxQ1HRlwFguvdA2nlAXjVFb4DzdFTS20ARFMxEu+ZZq3rsrVHRA1D2Fta9A1b7dZTsJVX+OQOYGbeBa6Gf8wGiUA1/Q6n9r8va/R86rZff81TSO6iIM/1ct/pV5P1bkpcw9+2bc5n4wPsU5BSWFo2LjIng+qeu4pK7gtvMMlRcBS4e7j2WPxf/XeE9jyrCZjc449LTOb3PaSya+jO5Gbk0P64JvW/uyQmnB57Ps4+72M3Yq17mhy9+LU1+9v1MW59+LOO+e6R0qbgQhwJd9Bs67x3YV6TOeRoqejAq0v8wa0hj0iY66yEonE7ZxEqBkYxK/qDctgG64Bt01r0Vu5HjNFTc/VbdGvcadPr1oNOrGL21fYKKfySoAnHWLt5Xepl8awAaFT8OFX2pz/PNvVdCcRBDT5F9MRJfKHvv/C/R2SNK7nVw8no7Rtw9ga9bzSR5OUySl9mTF/L89a/7fPyu12+i763n1WBE+635dT0v3TyBTX9WdWJbcFqc1JxJf7xY5etorVm1cDWz3lnAzo27SW6cSM9ru3LUyUfw64wVFOa5OPKEZnTqc5rMfxEiBHTeZHTOWB+P2sDWBJUyB6X2//2ZaZcHv3LoYI72qKTXrd6ePV2o3FwPA5xnohKfrfCO1NrMRee9AflTQOfsjyn2dp89H9rMhPzP0HkTQQdX2VfVm4ZynFj2OsV/oPMmg+sHSmv5xAxGRZxVoedQXSR5OQySF4/bw5XNbyFjl+9f3LjkWKZsn4gzwuGzTXXTWvPWgx/y6fNfV9s1o+Miyc8p9NvmhM6t+N9PldvQ0Z+iwiJevnUicz/4HqUUhqHwuE2SGiUy4sO7OLX7ydV+TyEOV1qb6D3dwPS+Hcw+KvFNVOQ5Jedo9K7WVGgn5zKsyckq+WOrYm7hdCqTwKiEZ1FR/SoZA2hdDOZeUJEoI9F3O/d/VoE8M43gl2vbIPpajPiHKx1fTajI+7dM2K2CvKw8/lm2gY1//IfHU7Mzs//8YY3fxAUgJz2XlfP/rKGILHPeW1StiUvvG8/l4jsu8LsaSClF1/6BdsOtnGcGvcq8DxeDturh7FvZlLk7i4d7j2X9io0hua8QhyJt5qILZ6Hzv0AXrSg/v8xMDZi4gB1dvKz0O2s+W1V6QT3W0EvxclTcfWBr7OV6/ubMGaASIfKCKsQASjlQtkb+Exet0Rm3gplOxerMaDD3BG4WRsJ+b6PakJ2ew6QHPmT+R4spdllLzlKa1eOqEZdw4a3n1Uj57Ky0nKDaZQfZrjporfn0ua+qZTJuQkocAx+6hMuGXcjeHel8/fpsCnILyxWRM2wGCSlxnDekW5Xu582/v29m8efel1Hum3j84ROfM2baA9V+byEOJVqbkDcenTsJOKAX1dYSEp5GOdvua1m5GzjPhKKFVYjQji6cY/VM1PscnTseCj4vKZ2vwHmG9e/i5Vif+fe9DllLolXSeKv4XagV/eJ7abRfKvjKvGFCkpcKysvKY9hZI9m2bmeZN9K0bXt5Zehb7N66lxvGXhXyOBod1aBa21WHzD3ZbFmzvdLnj/nqQSIiHUREOTmu4zGlq5ZSmtbj2XmjePTCcWTsysLmsD4VeYo91G9ej7EzHyE2sfon0i6a8lPpxF1vTI/Jkm9+oyCvkKiYQ3f3ViGqSuc8B/lvl3/Asxmdfi3U+xTlaA1GY2vpst/eFzfK0aHMERV7Izq9KskL4NmDzp2E1hkoWwt0ylwUJqhYlBGL1m4o/Aad9yF4NgBRENUHFT0IZT+yavcOVvFvBFdn5mAeVJTvib/hSJKXCvripRls+2cHpo+S+lOensZ5g7vS/DjfOxdXh1btWnLkCc3Ysna71/L+ylA0atGAk848PqRxHCiYMv9eKeg15Bw6X9jOZ69Vq3ZH89F/b/DTtKWs/nEtSinadj+J0y9sF7KJsznpuQQq36tNTX52gSQvQvigPamQ/46PR03Ajc79HyppAkoZ6KhLIO8N3xc0mkLE2WUOKWcHdNxjkPNYJaN0g2sG2vUdYKDxQM44iLsfFWPVuFLKDlGXoKIuqeQ9alHUFShH8Csvw4HMeamgb9+c4zNxAWvZ7qy3F4Q8DqUUd71+EzabUa7EvjKsiaV3v3FTje4Am9QwgfrNfVfU9SY2KYYhYwYybOItAWN1OB10G3AGd7x6A0NfuZ4z+nUM6YqfRi0bogMkZBHREcTXiw1ZDEKEvYJv8D9nxAOuhWizZA6f6wf/7R2ty6w02seIuQpSZoPzLKCyHyasZGrfJok6Zxw6/4tKXisEnJ2oUK+LioaYoaj4MSELqbZIz0sFFBcVB5wka3o0OzftqpF4Tjn7BJ5b8BgT73ufNb/u33PnmFOP4pbnBtGm24l+zq5+hmFw6d19mHj/Bz7nvDicdm5/eQiNj25EZEwkx7ZrWaOroSqi56CuvPvoJ/gahzdsBr2GdCtTlE8IUZY207CGOvx9ENBgZqA9W8G92v8FXYvQZobXwm2G/ShItoantGcnOncCFHyB772cFIHm2ejc/0FUP68JU41zdLA2ZnT/i/ckRkH0EJSzHagIcHRAGdE1HWWNkOSlAuwOO84oJ0UFvjc1M2wGcUk190n8pDOO55UlY9m2fid7d6ST3CixQkNWWutq7Z255O7erPllHYs//6W0wBtYP5eIaCfPzBlF607HVtv9Qqle4yRuHHc1Ex8oX17bsBnUa5LENSMvr4XIhAgfytbQGobxywCjHhT+ROCEwg3FayGic4D7NkYljEHHPwo6B60V5L8L+Z/sr41iOwo8AVYMmqlQvBqcvjd2rSlKKUh8w9qzyExl/8+p5GemEgA7OE5E2UI7daG2SfJSAUopul95JnPfX+RzEqfH7eGcK2u2pDJAs2Mb0+zYxkG13b5hJ589/w0LPvmBgpxCGhyZwgU3nMvRbVuAhqNOPqJ0r6KKstlsPDJlGGd9toSvXp/Nf6u3EhkbwTkDz+Tiob1ocET9wBepQ/rfdxGJDRJ4f8ynpG7aDVhDg2df3plbXhhMUsPE2g1QiLou8iLIec5PAxtEnIcy4tDYCGrFkQr+rUspB6hkayAqbjg69i4wM0BFQsF0dM6Tge+pc4O+X6gpe3NI+RYKplmTh83NlMavMyH/HXT+u5D4Iiry/FqMNLSkSF0FbVu3g9vaPUBRYRGmp+yPzrAZnHL2CTw7b1SNzjWpiDW/rueBHmModhX73o1ZQYdebblnws0hTTay0rLJ3ptDcqPEOl923zRN/vt7G4V5Lpoe04j4enG1HZIQYUPnvo7OfdnLIzZQUah6n6PsLdHuLei0nvhNJlQMqsHPKFXx/dvKxeX6EZ1xfcB2KmW+lTTUIdqzA73nPKydqw/+eSnAQKV8U24rhbpMitSFULNWTXh23mhSmloTU212A1UyYbZz3/aMmf5AnU1cPB4Pj/d/gaJCP4kLgIblc//gri6PkLErs9rjWLt0PQ/1eoLLG9zA9a3v4bL61/PkwBfZvmFntd+ruhiGwVEnHUHrTsdK4iJERcXchop71CrmdiDHKajkKSh7SwCU/QiI6IHvonMKogdVS+ICgLOztTzb51uhDZxdKpy4aE8qZs6LmHu6Y+7qhLn3KnTBt2hdfcVMdf4n7J9cXO5R63/zPqy2+9U10vNSSR6Ph+Vz/uDfVZtxRjro1Oc0mrVqErL7VYdfvl3OyIueDrq9YbMm4N7y/KBqi2HVwtWMOP9JzJIib6X3shtEx0Xxys9PhXyZuRCidmhdBEXLQeeBvYXXXgFt5qAzboDiVewvCFdS2ySiNyrxeWvZcnXF5PqlpPfFpOykYpvVy1Pv09LkKqjrFf9t1a7ReQdcr+R5RPRAJb5SLfGbaReDe43/RkZTjAZVrH9Tg2Rvo8Ngb6PK+GDMZ3z01Bd43MFn/zEJ0UxLn1wtvUmmaXLt0XewZ2ua19o0hs2gTbcTeXbuqCrfSwgRvrR2W6uKCqZbe/jYmqOi+1urZ0LQs62LfreGtYp+Kjlig4heqLhhFSpAp7UbveecklL83nq3FSp2OCr2lirHbKb1KdmN2g+jIUaDH6p8r5pSkfdvmbB7GHFE2Ctctj8vKx9XQRGR0VUvfb1qwWp2/+d7fw3TY7Jy/p+kbt5d6QnDQojwp5QdInugInvUzP2cbVDJ76LNdGsyr1EfZVTiw69rAZj+SmVodP77EHNj1ZdeOzuAeyO+677YwNmxaveow2TOy2GkY+/TKlwFNyLKiTOyeuqYbP1nR1Cfmravr7tzX4QQdYvWLnTxanTxX9awVBUoIxllP7pyiQugi1YQsE/A3BPE5pOBqair8V87x4OKvrbK96mrJHk5jLQ85UhO63mK3x2aD2TYDc4b3A3DqJ5fk+j4qKB6fqLjD82iSkKI6qN1EWbOS+jdXdB7L0XvvQS9uwtmzstoXVw7QQXdm1L111TlOBYV/zjWyqID72v9W8U9eMCGl4ceSV4OM498cg+t2lmTzwy77//7DZtBTFwUAx7sV2337tTnNOxO/59KUpom06p98JPjhBCHH6096Iw7IG8C6JwDHsiGvDfQmXdaO1nXMOXsjLUCyGcLsDW3Np+sIm3mgb0lxI+BiJ6gkqzVXBHnopI/QsXcUOV71GUy5+UwE58cx8s/Pclvs3/n+09/Jjs9hz1b97L5r62YByyfbnnKkTz0wZ00PLL66rzEJ8dx2T19mPrcVz7LOAweMwCbrQ6U4RZC1F2uuVC0yMeD2pp74poHkefVZFTg7AK2o8GzGe9zUTQq5sYqTTrW2oXOeQHypwCFJUcjIXoAKu5elDo8NomV1UYCsArGLZ/zO0WFxbRscySt2h0dkvt4PB7evPd9pr/6XekGkh6Pic1mcP3Yq+l/b9+Q3FcIcegw0wdD0a/4nvNhA+cZGMlv1WRYAFahvfRrwdzJ/q0OSpZ6R12Dih9Z6eRFazc640Yo+oXyz90AZ0dU0jvVupS8JslSaUle6rzdW9NY+MlPZO3JosER9el+1ZlS/E0IERRzdzcwd/hvZDsCo/68GonnYNrMg8Jv0IUzwMwGeytU9EBrw8SqXLfwO3Tm3X7bqIQXUVEXVuk+tUWWSos6r0HzFAY8cHFthyGECEdGYoDkRVltaokyYiB6ICp6YLVeV+dPZX/hPm8MdP7UsE1eKkIm7AohhAgrKupirCEZP20iD8EPR55t+F8ebZa0OfRJz4sQXmxbt4Nv35zLhlWbiIyOoMtFHTjnqjOJijk8JsMJUadFXQ5575UUhDt4YqwNbI0g6pLaiCy0jGTwbMX3xpUKjKSajKjWSPIixEG+fHkGb9w7GcMwMD0mSil+nbGC98d8ynPzR8veS0LUMmXEQb2PrOXS7r/YP4hggv0EVNKrKCO2NkMMCRV1Cbp4VcA2hwOZsCvEAZZ+t5JH+oz1+phhM0hpmszkda/gcFZP1WEhROVpraH4dyhaCiirHL7jlJDsf1QXaDMfvbdfSe+Ltx6npqh608M2cavI+7fMeRHiAFOfne6zArHpMdm9JY2fpy+r4aiEEN4opVDOtqjYm1GxN1l7FB2iiQuAMqJRyR+Bo/2+I5TO/XGcahWnC9PEpaJk2EiIEsVFxfzx/d9+29jsBstmraLrFV1qKCohhNhP2eqj6n2ALl5X0uOkwdkB5Ti+tkOrUZK8CFEimE0rtQa321/5byGECD3laAWOVrUdRq0J2bBReno6V199NfHx8SQmJnLDDTeQm5vrt/2dd97JcccdR1RUFEcccQR33XUXWVlZoQpRiDIioiJodlwT/PU6a1NzXPtjai4oIYQQ5YQsebn66qv566+/mDt3Lt9++y2LFy/m5ptv9tl+x44d7Nixg+eff57Vq1czefJkZs2axQ03HNqbS4m65dK7++BrCrtS4Ixy0HNQ15oNSgghRBkhWW20Zs0aTjjhBJYtW0b79tbEolmzZtG7d2+2bdtGkyZNgrrOZ599xjXXXENeXh52e3AjXOG02khrzR/f/82MiXPZsnY78cmxdL/qLLoNPIPI6IjaDu+w5PF4ePqaV1g09WcMQ2Ga1p+HzW6glOKxaQ/QqfdptRylEEIcemp9e4AlS5aQmJhYmrgA9OjRA8Mw+PXXX7nkkuDWoe97Av4SF5fLhcvlKv0+Ozu78oHXINM0eenmCcx6ZyE2u4HHbaIMxcoFq/nk6Wm8sPAxUprWq+0wDzs2m40RH91Npz7t+Oq179j05xYcEQ7OvLQTl97Th6NOOqK2QxRCiMNeSJKX1NRUGjRoUPZGdjvJycmkpqYGdY20tDSeeOIJv0NNAOPGjWPMmDGVjrW2fPHSDGa9sxAAj9uaKKpLPuXv2ryb0Zc8x2u/jvO57E9rzZpf1/PTtKUU5hXS4sTmdL/6LGLio2vmCRzCDMOgxzVn0+Oas2s7FCGEEF5UaM7LQw89ZK2r9/O1du3aKgeVnZ1Nnz59OOGEE3jsscf8th0xYgRZWVmlX1u3bq3y/UPN4/bw2Qtf+3ncZN1v/7Lml3VeH8/JyOX+HmO4u8sjfPHSt8ycNI9X7niLAY1v4vtPfw5V2EIIIUSdUKGel3vvvZchQ4b4bdOyZUsaNWrE7t27yxx3u92kp6fTqFEjv+fn5ORw/vnnExcXx7Rp03A4/FcyjYiIICIivOaHbFu/k4zUTL9tDLvBinl/ckLn48oc11rz2KXPsfpHK0n0uPdXWXQVFvHUVS+T1CiRU84+odrjFkIIIeqCCiUv9evXp379+gHbde7cmczMTJYvX067du0AWLBgAaZp0qlTJ5/nZWdn06tXLyIiIvj666+JjDw0N8ELpp6IQnlt99fP//gupKZBGYqPn/pCkhchhKhFWruh+C/QhWA/GmVLqe2QDikhWSrdunVrzj//fG666SaWLl3KTz/9xB133MHAgQNLVxpt376d448/nqVLlwJW4vL/9u4/KMp63wP4+9mFXTR3QVQEFHMQFDXU0iMXfxwt8cfIWE7OwdTDVY+mXbEa9WYUddEsM/M23uNQHs3S5qIMdrHxJPkjlSlNsYMwMWE4BP0wWw1RdhF/APu5fzTuhMLKs+6vZ32/ZvaPvs/32d4fl3mezz4/9pk0aRKuXr2Kbdu2wWq1wmKxwGKxoKXl9mc4aFuv+Ch0CXvA6ZyW5hYMHj3gjvFjBcXQB+nbXc/eYkfJoW/QaLt2zzmJiEgdEYFc/Qjy258hdX+BXE6H/DYG9svPQ1ou3v0NqEM89jsvubm5SEhIwIQJEzB16lSMGTMGW7ZscSxvampCZWUlGhsbAQCnT59GcXExysvLERcXh6ioKMdLC9exqGEwBmPaf0yComv7YlydXofouEg8PCHxjmXXG6536P9xo/HG3ScREZFbScN/Q2yvA/baP4zagRsHIZfSIPY6n2ULJB57PEB4eDh27tzZ7vK+ffvijz8xM378eATYA66d+ut//QVn//U9Sg59A0WnOO400ul1MHV9AKv3rIROd2dv2Wdgb9jtzk87mbo+AHM3k0dyExFR26T5R+DqlnaWtgD2C5CGLVDMmV7NFYj4VGkfMRiD8ca+l7Fy+1IkjIyHuZsJUbE9MSdrBraWv4O+g2PaXC8l/c9OTxvp9DqkLp7kdA4REbmfXPs/AM62vS3AtXyI3P26R3KOD2b0IX2QHhP/fZyqn5s3dzNh2T8W4+2/5UCn6FodhdHpdeg7OAZPZU73QFoiInKq5TyAu5xBkAZAGgGli1ciBSo2Lxo0ae54dI0Mw661BSj/8gwA4IHQzkhdNBGzs570mx+qa25qxunPy3HlYj269+6GoeMHQa/nESEiClC6MABOnuwKAAgGlMC8k9ab2Lxo1J8mD8OfJg+Dtc6G61dvIDwyDEHB/vNxHtxRhC0vfIT6WptjrHvvblj6979h9PSRPkxGROQZSsg0SONHTmbogZBpUBT/2VZrFa950ThzuAkRMd39qnE5sP0o3p6f06pxAYDaXy5h1Yy3ceKf//JRMiIiDwoeAhgnoO1dqw5QDFC6OH/kDXUMmxdyq6abTfjHf7bzzUN+P6C6ecWO++rOMiK6PyiKAiVsIxDyBH7fvSpw7Gb1vaGE/y+UoFjfBQwg/vN1nQJCycFvYKtraHe5CHC+yoLKr6uQMDLei8mIiDxPUYxQwt6CtCwDbhQBcgMI6g8Y/q3dB+2SemxeyK3q7vLMJrXziIi0SNFHAp2f8nWMgMXTRuRW3aK7dmhe917hHk5CRESBis0LudXwiUMQFhHa7nJFURCT0Avxj/C8LxERuYbNC7lVUHAQlmyc3+YyRVEABcj4n/k890tERC7jNS/kdo8+NRr6IB02r9iB336+5BiPjo/E0r8vwPCJQ32YjoiItE6RALtn1Wq1IjQ0FPX19TCbzb6Oc1+z2+349nil4xd2E0bG8YgLERG1Sc3+m0deyGN0Oh0Sxw70dQwiIgowvOaFiIiINIXNCxEREWkKmxciIiLSFDYvREREpClsXoiIiEhT2LwQERGRprB5ISIiIk1h80JERESawuaFiIiINCXgfmH31tMOrFarj5MQERFRR93ab3fkqUUB17zYbDYAQExMjI+TEBERkVo2mw2hoaFO5wTcgxntdjvOnz8Pk8mkuYcAWq1WxMTE4Oeffw74h0qy1sBzv9QJsNZAxVp9S0Rgs9kQHR0Nnc75VS0Bd+RFp9Ohd+/evo5xT8xms9/8MXkaaw0890udAGsNVKzVd+52xOUWXrBLREREmsLmhYiIiDSFzYsfMRqNyM7OhtFo9HUUj2Otged+qRNgrYGKtWpHwF2wS0RERIGNR16IiIhIU9i8EBERkaaweSEiIiJNYfNCREREmsLmxcfq6uowZ84cmM1mhIWFYcGCBWhoaHA6/9lnn8WAAQPQqVMn9OnTB8899xzq6+u9mLpjcnJy0LdvX4SEhCApKQmnTp1yOn/37t1ISEhASEgIEhMTUVhY6KWk905NrVu3bsXYsWPRtWtXdO3aFSkpKXf9t/EXaj/TW/Ly8qAoCqZPn+7ZgG6kttYrV64gIyMDUVFRMBqN6N+/v2b+htXWunHjRsc2KCYmBsuWLcP169e9lNZ1X3zxBaZNm4bo6GgoioJPPvnkrusUFRXhkUcegdFoRFxcHLZv3+7xnO6gttaCggJMnDgRPXr0gNlsRnJyMg4cOOCdsK4Q8qkpU6bI0KFD5eTJk/Lll19KXFyczJo1q9355eXl8uSTT8revXulqqpKDh8+LPHx8TJjxgwvpr67vLw8MRgM8sEHH8i3334rTz/9tISFhcmFCxfanH/8+HHR6/Wyfv16qaiokFdeeUWCg4OlvLzcy8nVU1vr7NmzJScnR0pLS+XMmTMyb948CQ0NlXPnznk5uTpq67ylpqZGevXqJWPHjpUnnnjCO2Hvkdpab9y4ISNGjJCpU6fKsWPHpKamRoqKiqSsrMzLydVTW2tubq4YjUbJzc2VmpoaOXDggERFRcmyZcu8nFy9wsJCycrKkoKCAgEge/bscTq/urpaOnfuLMuXL5eKigrZtGmT6PV62b9/v3cC3wO1tT7//PPy1ltvyalTp+Ts2bPy0ksvSXBwsJw+fdo7gVVi8+JDFRUVAkC+/vprx9hnn30miqLIL7/80uH3yc/PF4PBIE1NTZ6I6ZKRI0dKRkaG479bWlokOjpa3nzzzTbnp6WlSWpqaquxpKQkWbx4sUdzuoPaWm/X3NwsJpNJduzY4amIbuFKnc3NzTJq1Ch5//33Ze7cuZppXtTW+t5770lsbKzcvHnTWxHdRm2tGRkZ8thjj7UaW758uYwePdqjOd2tIzv0lStXyuDBg1uNzZw5UyZPnuzBZO7XkVrbMmjQIFm9erX7A7kBTxv50IkTJxAWFoYRI0Y4xlJSUqDT6VBcXNzh96mvr4fZbEZQkH88qurmzZsoKSlBSkqKY0yn0yElJQUnTpxoc50TJ060mg8AkydPbne+v3Cl1ts1NjaiqakJ4eHhnop5z1yt87XXXkNERAQWLFjgjZhu4Uqte/fuRXJyMjIyMtCzZ0889NBDWLt2LVpaWrwV2yWu1Dpq1CiUlJQ4Ti1VV1ejsLAQU6dO9Upmb9Lqdskd7HY7bDab326X/GNvd5+yWCyIiIhoNRYUFITw8HBYLJYOvUdtbS3WrFmDRYsWeSKiS2pra9HS0oKePXu2Gu/Zsye+++67NtexWCxtzu/ov4OvuFLr7V588UVER0ffsZH0J67UeezYMWzbtg1lZWVeSOg+rtRaXV2NI0eOYM6cOSgsLERVVRWWLFmCpqYmZGdneyO2S1ypdfbs2aitrcWYMWMgImhubsYzzzyDl19+2RuRvaq97ZLVasW1a9fQqVMnHyXzvA0bNqChoQFpaWm+jtImHnnxgMzMTCiK4vTV0R2bM1arFampqRg0aBBWrVp178HJ69atW4e8vDzs2bMHISEhvo7jNjabDenp6di6dSu6d+/u6zgeZ7fbERERgS1btmD48OGYOXMmsrKysHnzZl9Hc7uioiKsXbsW7777Lk6fPo2CggLs27cPa9as8XU0cpOdO3di9erVyM/Pv+MLtr/gkRcPWLFiBebNm+d0TmxsLCIjI3Hx4sVW483Nzairq0NkZKTT9W02G6ZMmQKTyYQ9e/YgODj4XmO7Tffu3aHX63HhwoVW4xcuXGi3rsjISFXz/YUrtd6yYcMGrFu3Dp9//jmGDBniyZj3TG2d33//PX744QdMmzbNMWa32wH8fnSxsrIS/fr182xoF7nymUZFRSE4OBh6vd4xNnDgQFgsFty8eRMGg8GjmV3lSq2vvvoq0tPTsXDhQgBAYmIirl69ikWLFiErKws6XeB8J25vu2Q2mwP2qEteXh4WLlyI3bt3+/XR4MD5K/MjPXr0QEJCgtOXwWBAcnIyrly5gpKSEse6R44cgd1uR1JSUrvvb7VaMWnSJBgMBuzdu9fvvrEbDAYMHz4chw8fdozZ7XYcPnwYycnJba6TnJzcaj4AHDp0qN35/sKVWgFg/fr1WLNmDfbv39/qmid/pbbOhIQElJeXo6yszPF6/PHH8eijj6KsrAwxMTHejK+KK5/p6NGjUVVV5WjQAODs2bOIiory28YFcK3WxsbGOxqUW02bBNij8rS6XXLVrl27MH/+fOzatQupqam+juOcr68Yvt9NmTJFHn74YSkuLpZjx45JfHx8q1ulz507JwMGDJDi4mIREamvr5ekpCRJTEyUqqoq+fXXXx2v5uZmX5Vxh7y8PDEajbJ9+3apqKiQRYsWSVhYmFgsFhERSU9Pl8zMTMf848ePS1BQkGzYsEHOnDkj2dnZmrpVWk2t69atE4PBIB9//HGrz89ms/mqhA5RW+fttHS3kdpaf/rpJzGZTLJ06VKprKyUTz/9VCIiIuT111/3VQkdprbW7OxsMZlMsmvXLqmurpaDBw9Kv379JC0tzVcldJjNZpPS0lIpLS0VAPLOO+9IaWmp/PjjjyIikpmZKenp6Y75t26VfuGFF+TMmTOSk5OjmVul1daam5srQUFBkpOT02q7dOXKFV+V4BSbFx+7dOmSzJo1S7p06SJms1nmz5/faidWU1MjAOTo0aMiInL06FEB0OarpqbGN0W0Y9OmTdKnTx8xGAwycuRIOXnypGPZuHHjZO7cua3m5+fnS//+/cVgMMjgwYNl3759Xk7sOjW1Pvjgg21+ftnZ2d4PrpLaz/SPtNS8iKiv9auvvpKkpCQxGo0SGxsrb7zxhl99oXBGTa1NTU2yatUq6devn4SEhEhMTIwsWbJELl++7P3gKrW3/bxV39y5c2XcuHF3rDNs2DAxGAwSGxsrH374oddzu0JtrePGjXM6398oIgF2nI+IiIgCGq95ISIiIk1h80JERESawuaFiIiINIXNCxEREWkKmxciIiLSFDYvREREpClsXoiIiEhT2LwQERGRprB5ISIiIk1h80JERESawuaFiIiINIXNCxEREWnK/wO64zy6Z1wRjgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.scatter(X[:,0],X[:,1],c=y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Q9Y4g0NYvpy"
      },
      "source": [
        "## Steps:\n",
        "* build train and test sets\n",
        "* write MLP class in Pytorch with two layers with adjustable number of perceptrons\n",
        "* use nn.linear and nn.Sigmoid() units\n",
        "* train your model\n",
        "* test your model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "\n",
        "x_train = torch.FloatTensor(x_train)\n",
        "x_test = torch.FloatTensor(x_test)\n",
        "y_train = torch.FloatTensor(y_train)\n",
        "y_test = torch.FloatTensor(y_test)"
      ],
      "metadata": {
        "id": "piXsTVTrZssQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "id": "MGwlyPvALJvY",
        "outputId": "6a415201-9f56-4dd3-bc14-a3f6f1da08c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
              "        1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0.,\n",
              "        0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
              "        1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0.,\n",
              "        1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0.,\n",
              "        1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0.,\n",
              "        1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0.,\n",
              "        0., 1., 0., 1., 0., 1., 1., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "class MLP(torch.nn.Module): #all nets inherit from nn.Module\n",
        "    def __init__(self, hidden_sizes, input_size, output_size): #define layer types\n",
        "        super(MLP, self).__init__()\n",
        "        #hidden_sizes=[64, 32]\n",
        "        #self.fc1 = torch.nn.Linear(2,2,bias=False) # Perceptron is single neuron \"fully connected\" (fc) -> linear unit with 2 inputs and 1 output\n",
        "        #self.fc2 = torch.nn.Linear(2,2,bias=False)\n",
        "        #self.fc3 = torch.nn.Linear(2,1,bias=False) # Perceptron is single neuron \"fully connected\" (fc) -> linear unit with 2 inputs and 1 output\n",
        "\n",
        "        #self.fc1 = torch.nn.Linear(2,hidden_sizes[0],bias=False)\n",
        "        #self.fc2 = torch.nn.Linear(hidden_sizes[0],hidden_sizes[1],bias=False)\n",
        "        #self.fc3 = torch.nn.Linear(hidden_sizes[1],1,bias=False)\n",
        "\n",
        "        self.fc1 = torch.nn.Linear(input_size, hidden_sizes[0])\n",
        "        self.fc2 = torch.nn.Linear(hidden_sizes[0], output_size)\n",
        "\n",
        "        self.non_linear = torch.nn.Sigmoid() #non-linear activation\n",
        "\n",
        "    def forward(self, x): #build network\n",
        "        #output = self.fc1(x) #w*X\n",
        "        #output = self.fc2(x) #w*X\n",
        "        #output = self.fc3(output) #w*X\n",
        "        #output = self.non_linear(output) # activation\n",
        "        #return output\n",
        "        x = self.non_linear(self.fc1(x))\n",
        "        x = self.non_linear(self.fc2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "EaSKRu1lGiF6"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get instance of perceptron model\n",
        "model = MLP([2, 2], 2, 1)\n",
        "\n",
        "#define loss function\n",
        "criterion = torch.nn.BCELoss()\n",
        "\n",
        "#define optimizer -> SGD with learning rate lr\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1)"
      ],
      "metadata": {
        "id": "GQIEin9nGnhU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "model(x_test)\n",
        "print()"
      ],
      "metadata": {
        "id": "1LksIOlRH2eB",
        "outputId": "45729779-d4c3-4ca7-cfa8-3bc00217c6de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.train() #set to train mode\n",
        "epoch = 20000\n",
        "for epoch in range(epoch):\n",
        "    optimizer.zero_grad()\n",
        "    # Forward pass\n",
        "    y_pred = model(x_train)\n",
        "    # Compute Loss\n",
        "    loss = criterion(y_pred.squeeze(), y_train)\n",
        "\n",
        "    print('Epoch {}: train loss: {}'.format(epoch, loss.item()))\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    #make gradient update\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "2McooTiKH6sm",
        "outputId": "a818c1ef-29ac-4aa6-d426-64a9a450c8c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch 15000: train loss: 0.02389441803097725\n",
            "Epoch 15001: train loss: 0.023889407515525818\n",
            "Epoch 15002: train loss: 0.023884398862719536\n",
            "Epoch 15003: train loss: 0.023879388347268105\n",
            "Epoch 15004: train loss: 0.02387438900768757\n",
            "Epoch 15005: train loss: 0.023869387805461884\n",
            "Epoch 15006: train loss: 0.0238643866032362\n",
            "Epoch 15007: train loss: 0.023859387263655663\n",
            "Epoch 15008: train loss: 0.023854391649365425\n",
            "Epoch 15009: train loss: 0.02384939417243004\n",
            "Epoch 15010: train loss: 0.023844407871365547\n",
            "Epoch 15011: train loss: 0.023839419707655907\n",
            "Epoch 15012: train loss: 0.023834431543946266\n",
            "Epoch 15013: train loss: 0.023829448968172073\n",
            "Epoch 15014: train loss: 0.023824471980333328\n",
            "Epoch 15015: train loss: 0.023819485679268837\n",
            "Epoch 15016: train loss: 0.023814504966139793\n",
            "Epoch 15017: train loss: 0.023809531703591347\n",
            "Epoch 15018: train loss: 0.023804549127817154\n",
            "Epoch 15019: train loss: 0.023799574002623558\n",
            "Epoch 15020: train loss: 0.023794595152139664\n",
            "Epoch 15021: train loss: 0.023789629340171814\n",
            "Epoch 15022: train loss: 0.023784665390849113\n",
            "Epoch 15023: train loss: 0.023779699578881264\n",
            "Epoch 15024: train loss: 0.02377474121749401\n",
            "Epoch 15025: train loss: 0.02376977913081646\n",
            "Epoch 15026: train loss: 0.02376481704413891\n",
            "Epoch 15027: train loss: 0.023759860545396805\n",
            "Epoch 15028: train loss: 0.023754907771945\n",
            "Epoch 15029: train loss: 0.023749949410557747\n",
            "Epoch 15030: train loss: 0.023745005950331688\n",
            "Epoch 15031: train loss: 0.023740047588944435\n",
            "Epoch 15032: train loss: 0.023735100403428078\n",
            "Epoch 15033: train loss: 0.02373015135526657\n",
            "Epoch 15034: train loss: 0.02372521162033081\n",
            "Epoch 15035: train loss: 0.02372026816010475\n",
            "Epoch 15036: train loss: 0.023715324699878693\n",
            "Epoch 15037: train loss: 0.02371039241552353\n",
            "Epoch 15038: train loss: 0.023705460131168365\n",
            "Epoch 15039: train loss: 0.023700525984168053\n",
            "Epoch 15040: train loss: 0.02369559556245804\n",
            "Epoch 15041: train loss: 0.023690665140748024\n",
            "Epoch 15042: train loss: 0.02368573471903801\n",
            "Epoch 15043: train loss: 0.023680811747908592\n",
            "Epoch 15044: train loss: 0.02367587760090828\n",
            "Epoch 15045: train loss: 0.02367095649242401\n",
            "Epoch 15046: train loss: 0.02366604097187519\n",
            "Epoch 15047: train loss: 0.023661121726036072\n",
            "Epoch 15048: train loss: 0.02365620620548725\n",
            "Epoch 15049: train loss: 0.02365128882229328\n",
            "Epoch 15050: train loss: 0.02364637888967991\n",
            "Epoch 15051: train loss: 0.023641472682356834\n",
            "Epoch 15052: train loss: 0.023636560887098312\n",
            "Epoch 15053: train loss: 0.023631660267710686\n",
            "Epoch 15054: train loss: 0.02362675406038761\n",
            "Epoch 15055: train loss: 0.023621853440999985\n",
            "Epoch 15056: train loss: 0.02361694909632206\n",
            "Epoch 15057: train loss: 0.023612044751644135\n",
            "Epoch 15058: train loss: 0.023607149720191956\n",
            "Epoch 15059: train loss: 0.023602258414030075\n",
            "Epoch 15060: train loss: 0.023597361519932747\n",
            "Epoch 15061: train loss: 0.023592479526996613\n",
            "Epoch 15062: train loss: 0.023587584495544434\n",
            "Epoch 15063: train loss: 0.02358269691467285\n",
            "Epoch 15064: train loss: 0.023577816784381866\n",
            "Epoch 15065: train loss: 0.023572932928800583\n",
            "Epoch 15066: train loss: 0.02356805093586445\n",
            "Epoch 15067: train loss: 0.023563167080283165\n",
            "Epoch 15068: train loss: 0.023558292537927628\n",
            "Epoch 15069: train loss: 0.02355341427028179\n",
            "Epoch 15070: train loss: 0.023548536002635956\n",
            "Epoch 15071: train loss: 0.023543670773506165\n",
            "Epoch 15072: train loss: 0.023538794368505478\n",
            "Epoch 15073: train loss: 0.023533929139375687\n",
            "Epoch 15074: train loss: 0.023529060184955597\n",
            "Epoch 15075: train loss: 0.023524200543761253\n",
            "Epoch 15076: train loss: 0.02351933717727661\n",
            "Epoch 15077: train loss: 0.023514479398727417\n",
            "Epoch 15078: train loss: 0.023509623482823372\n",
            "Epoch 15079: train loss: 0.02350476384162903\n",
            "Epoch 15080: train loss: 0.023499907925724983\n",
            "Epoch 15081: train loss: 0.023495055735111237\n",
            "Epoch 15082: train loss: 0.023490210995078087\n",
            "Epoch 15083: train loss: 0.02348536066710949\n",
            "Epoch 15084: train loss: 0.023480506613850594\n",
            "Epoch 15085: train loss: 0.02347566932439804\n",
            "Epoch 15086: train loss: 0.02347082458436489\n",
            "Epoch 15087: train loss: 0.023465989157557487\n",
            "Epoch 15088: train loss: 0.023461150005459785\n",
            "Epoch 15089: train loss: 0.023456312716007233\n",
            "Epoch 15090: train loss: 0.023451467975974083\n",
            "Epoch 15091: train loss: 0.023446645587682724\n",
            "Epoch 15092: train loss: 0.023441806435585022\n",
            "Epoch 15093: train loss: 0.023436978459358215\n",
            "Epoch 15094: train loss: 0.02343214862048626\n",
            "Epoch 15095: train loss: 0.0234273299574852\n",
            "Epoch 15096: train loss: 0.023422500118613243\n",
            "Epoch 15097: train loss: 0.023417681455612183\n",
            "Epoch 15098: train loss: 0.02341286651790142\n",
            "Epoch 15099: train loss: 0.02340804412961006\n",
            "Epoch 15100: train loss: 0.02340323105454445\n",
            "Epoch 15101: train loss: 0.023398419842123985\n",
            "Epoch 15102: train loss: 0.023393606767058372\n",
            "Epoch 15103: train loss: 0.02338879182934761\n",
            "Epoch 15104: train loss: 0.023383978754281998\n",
            "Epoch 15105: train loss: 0.02337918058037758\n",
            "Epoch 15106: train loss: 0.023374374955892563\n",
            "Epoch 15107: train loss: 0.023369573056697845\n",
            "Epoch 15108: train loss: 0.023364774882793427\n",
            "Epoch 15109: train loss: 0.02335996739566326\n",
            "Epoch 15110: train loss: 0.023355167359113693\n",
            "Epoch 15111: train loss: 0.02335037849843502\n",
            "Epoch 15112: train loss: 0.0233455877751112\n",
            "Epoch 15113: train loss: 0.023340798914432526\n",
            "Epoch 15114: train loss: 0.023336011916399002\n",
            "Epoch 15115: train loss: 0.02333121933043003\n",
            "Epoch 15116: train loss: 0.023326432332396507\n",
            "Epoch 15117: train loss: 0.023321647197008133\n",
            "Epoch 15118: train loss: 0.023316867649555206\n",
            "Epoch 15119: train loss: 0.02331208996474743\n",
            "Epoch 15120: train loss: 0.023307304829359055\n",
            "Epoch 15121: train loss: 0.023302534595131874\n",
            "Epoch 15122: train loss: 0.023297756910324097\n",
            "Epoch 15123: train loss: 0.02329297736287117\n",
            "Epoch 15124: train loss: 0.023288214579224586\n",
            "Epoch 15125: train loss: 0.023283442482352257\n",
            "Epoch 15126: train loss: 0.023278675973415375\n",
            "Epoch 15127: train loss: 0.023273911327123642\n",
            "Epoch 15128: train loss: 0.02326914854347706\n",
            "Epoch 15129: train loss: 0.023264387622475624\n",
            "Epoch 15130: train loss: 0.02325962856411934\n",
            "Epoch 15131: train loss: 0.0232548750936985\n",
            "Epoch 15132: train loss: 0.023250121623277664\n",
            "Epoch 15133: train loss: 0.023245366290211678\n",
            "Epoch 15134: train loss: 0.023240620270371437\n",
            "Epoch 15135: train loss: 0.023235861212015152\n",
            "Epoch 15136: train loss: 0.023231107741594315\n",
            "Epoch 15137: train loss: 0.023226365447044373\n",
            "Epoch 15138: train loss: 0.02322162128984928\n",
            "Epoch 15139: train loss: 0.02321687713265419\n",
            "Epoch 15140: train loss: 0.023212134838104248\n",
            "Epoch 15141: train loss: 0.023207399994134903\n",
            "Epoch 15142: train loss: 0.023202667012810707\n",
            "Epoch 15143: train loss: 0.023197924718260765\n",
            "Epoch 15144: train loss: 0.023193195462226868\n",
            "Epoch 15145: train loss: 0.02318846806883812\n",
            "Epoch 15146: train loss: 0.023183735087513924\n",
            "Epoch 15147: train loss: 0.023179009556770325\n",
            "Epoch 15148: train loss: 0.023174282163381577\n",
            "Epoch 15149: train loss: 0.02316955104470253\n",
            "Epoch 15150: train loss: 0.02316483110189438\n",
            "Epoch 15151: train loss: 0.023160114884376526\n",
            "Epoch 15152: train loss: 0.023155391216278076\n",
            "Epoch 15153: train loss: 0.023150676861405373\n",
            "Epoch 15154: train loss: 0.023145968094468117\n",
            "Epoch 15155: train loss: 0.023141253739595413\n",
            "Epoch 15156: train loss: 0.02313654124736786\n",
            "Epoch 15157: train loss: 0.0231318362057209\n",
            "Epoch 15158: train loss: 0.023127123713493347\n",
            "Epoch 15159: train loss: 0.023122413083910942\n",
            "Epoch 15160: train loss: 0.02311771735548973\n",
            "Epoch 15161: train loss: 0.023113010451197624\n",
            "Epoch 15162: train loss: 0.023108309134840965\n",
            "Epoch 15163: train loss: 0.023103611543774605\n",
            "Epoch 15164: train loss: 0.023098912090063095\n",
            "Epoch 15165: train loss: 0.023094216361641884\n",
            "Epoch 15166: train loss: 0.023089520633220673\n",
            "Epoch 15167: train loss: 0.02308483235538006\n",
            "Epoch 15168: train loss: 0.023080145940184593\n",
            "Epoch 15169: train loss: 0.02307545766234398\n",
            "Epoch 15170: train loss: 0.02307077869772911\n",
            "Epoch 15171: train loss: 0.023066097870469093\n",
            "Epoch 15172: train loss: 0.02306140959262848\n",
            "Epoch 15173: train loss: 0.02305673249065876\n",
            "Epoch 15174: train loss: 0.023052049800753593\n",
            "Epoch 15175: train loss: 0.023047368973493576\n",
            "Epoch 15176: train loss: 0.023042693734169006\n",
            "Epoch 15177: train loss: 0.023038020357489586\n",
            "Epoch 15178: train loss: 0.023033348843455315\n",
            "Epoch 15179: train loss: 0.02302868291735649\n",
            "Epoch 15180: train loss: 0.02302401326596737\n",
            "Epoch 15181: train loss: 0.023019352927803993\n",
            "Epoch 15182: train loss: 0.023014690726995468\n",
            "Epoch 15183: train loss: 0.023010028526186943\n",
            "Epoch 15184: train loss: 0.02300536073744297\n",
            "Epoch 15185: train loss: 0.023000704124569893\n",
            "Epoch 15186: train loss: 0.022996047511696815\n",
            "Epoch 15187: train loss: 0.022991396486759186\n",
            "Epoch 15188: train loss: 0.02298673987388611\n",
            "Epoch 15189: train loss: 0.02298208698630333\n",
            "Epoch 15190: train loss: 0.02297743782401085\n",
            "Epoch 15191: train loss: 0.022972794249653816\n",
            "Epoch 15192: train loss: 0.022968148812651634\n",
            "Epoch 15193: train loss: 0.0229635052382946\n",
            "Epoch 15194: train loss: 0.022958865389227867\n",
            "Epoch 15195: train loss: 0.02295423112809658\n",
            "Epoch 15196: train loss: 0.02294958382844925\n",
            "Epoch 15197: train loss: 0.022944951429963112\n",
            "Epoch 15198: train loss: 0.022940315306186676\n",
            "Epoch 15199: train loss: 0.02293567545711994\n",
            "Epoch 15200: train loss: 0.0229310505092144\n",
            "Epoch 15201: train loss: 0.022926419973373413\n",
            "Epoch 15202: train loss: 0.022921787574887276\n",
            "Epoch 15203: train loss: 0.02291715517640114\n",
            "Epoch 15204: train loss: 0.022912533953785896\n",
            "Epoch 15205: train loss: 0.022907914593815804\n",
            "Epoch 15206: train loss: 0.02290329895913601\n",
            "Epoch 15207: train loss: 0.022898679599165916\n",
            "Epoch 15208: train loss: 0.022894062101840973\n",
            "Epoch 15209: train loss: 0.022889450192451477\n",
            "Epoch 15210: train loss: 0.022884830832481384\n",
            "Epoch 15211: train loss: 0.022880220785737038\n",
            "Epoch 15212: train loss: 0.022875605151057243\n",
            "Epoch 15213: train loss: 0.022871000692248344\n",
            "Epoch 15214: train loss: 0.02286638878285885\n",
            "Epoch 15215: train loss: 0.0228617824614048\n",
            "Epoch 15216: train loss: 0.0228571817278862\n",
            "Epoch 15217: train loss: 0.022852590307593346\n",
            "Epoch 15218: train loss: 0.022847987711429596\n",
            "Epoch 15219: train loss: 0.022843390703201294\n",
            "Epoch 15220: train loss: 0.02283879742026329\n",
            "Epoch 15221: train loss: 0.022834211587905884\n",
            "Epoch 15222: train loss: 0.022829614579677582\n",
            "Epoch 15223: train loss: 0.022825023159384727\n",
            "Epoch 15224: train loss: 0.02282043546438217\n",
            "Epoch 15225: train loss: 0.022815844044089317\n",
            "Epoch 15226: train loss: 0.02281126379966736\n",
            "Epoch 15227: train loss: 0.022806670516729355\n",
            "Epoch 15228: train loss: 0.0228020828217268\n",
            "Epoch 15229: train loss: 0.02279750630259514\n",
            "Epoch 15230: train loss: 0.02279292605817318\n",
            "Epoch 15231: train loss: 0.02278835140168667\n",
            "Epoch 15232: train loss: 0.022783782333135605\n",
            "Epoch 15233: train loss: 0.022779207676649094\n",
            "Epoch 15234: train loss: 0.02277463860809803\n",
            "Epoch 15235: train loss: 0.022770071402192116\n",
            "Epoch 15236: train loss: 0.0227655041962862\n",
            "Epoch 15237: train loss: 0.022760942578315735\n",
            "Epoch 15238: train loss: 0.022756369784474373\n",
            "Epoch 15239: train loss: 0.022751810029149055\n",
            "Epoch 15240: train loss: 0.022747253999114037\n",
            "Epoch 15241: train loss: 0.022742679342627525\n",
            "Epoch 15242: train loss: 0.022738127037882805\n",
            "Epoch 15243: train loss: 0.022733574733138084\n",
            "Epoch 15244: train loss: 0.022729022428393364\n",
            "Epoch 15245: train loss: 0.022724471986293793\n",
            "Epoch 15246: train loss: 0.02271992340683937\n",
            "Epoch 15247: train loss: 0.022715380415320396\n",
            "Epoch 15248: train loss: 0.022710831835865974\n",
            "Epoch 15249: train loss: 0.02270628698170185\n",
            "Epoch 15250: train loss: 0.022701747715473175\n",
            "Epoch 15251: train loss: 0.0226972047239542\n",
            "Epoch 15252: train loss: 0.022692663595080376\n",
            "Epoch 15253: train loss: 0.02268812246620655\n",
            "Epoch 15254: train loss: 0.022683585062623024\n",
            "Epoch 15255: train loss: 0.022679051384329796\n",
            "Epoch 15256: train loss: 0.022674517706036568\n",
            "Epoch 15257: train loss: 0.02266998216509819\n",
            "Epoch 15258: train loss: 0.022665461525321007\n",
            "Epoch 15259: train loss: 0.022660931572318077\n",
            "Epoch 15260: train loss: 0.022656405344605446\n",
            "Epoch 15261: train loss: 0.02265188656747341\n",
            "Epoch 15262: train loss: 0.02264736033976078\n",
            "Epoch 15263: train loss: 0.022642839699983597\n",
            "Epoch 15264: train loss: 0.022638317197561264\n",
            "Epoch 15265: train loss: 0.022633805871009827\n",
            "Epoch 15266: train loss: 0.022629287093877792\n",
            "Epoch 15267: train loss: 0.022624770179390907\n",
            "Epoch 15268: train loss: 0.02262026071548462\n",
            "Epoch 15269: train loss: 0.022615747526288033\n",
            "Epoch 15270: train loss: 0.02261124551296234\n",
            "Epoch 15271: train loss: 0.022606737911701202\n",
            "Epoch 15272: train loss: 0.02260223589837551\n",
            "Epoch 15273: train loss: 0.02259773202240467\n",
            "Epoch 15274: train loss: 0.02259323187172413\n",
            "Epoch 15275: train loss: 0.022588733583688736\n",
            "Epoch 15276: train loss: 0.022584237158298492\n",
            "Epoch 15277: train loss: 0.02257973700761795\n",
            "Epoch 15278: train loss: 0.022575238719582558\n",
            "Epoch 15279: train loss: 0.02257074974477291\n",
            "Epoch 15280: train loss: 0.02256625145673752\n",
            "Epoch 15281: train loss: 0.02256176993250847\n",
            "Epoch 15282: train loss: 0.022557279095053673\n",
            "Epoch 15283: train loss: 0.022552795708179474\n",
            "Epoch 15284: train loss: 0.022548308596014977\n",
            "Epoch 15285: train loss: 0.022543828934431076\n",
            "Epoch 15286: train loss: 0.022539347410202026\n",
            "Epoch 15287: train loss: 0.022534869611263275\n",
            "Epoch 15288: train loss: 0.02253039926290512\n",
            "Epoch 15289: train loss: 0.02252592332661152\n",
            "Epoch 15290: train loss: 0.022521445527672768\n",
            "Epoch 15291: train loss: 0.022516973316669464\n",
            "Epoch 15292: train loss: 0.02251249924302101\n",
            "Epoch 15293: train loss: 0.022508030757308006\n",
            "Epoch 15294: train loss: 0.02250356413424015\n",
            "Epoch 15295: train loss: 0.022499103099107742\n",
            "Epoch 15296: train loss: 0.022494638338685036\n",
            "Epoch 15297: train loss: 0.02249016985297203\n",
            "Epoch 15298: train loss: 0.02248571254312992\n",
            "Epoch 15299: train loss: 0.02248125523328781\n",
            "Epoch 15300: train loss: 0.022476794198155403\n",
            "Epoch 15301: train loss: 0.02247234620153904\n",
            "Epoch 15302: train loss: 0.022467894479632378\n",
            "Epoch 15303: train loss: 0.022463440895080566\n",
            "Epoch 15304: train loss: 0.022458991035819054\n",
            "Epoch 15305: train loss: 0.022454537451267242\n",
            "Epoch 15306: train loss: 0.02245008572936058\n",
            "Epoch 15307: train loss: 0.022445648908615112\n",
            "Epoch 15308: train loss: 0.022441208362579346\n",
            "Epoch 15309: train loss: 0.02243676595389843\n",
            "Epoch 15310: train loss: 0.022432323545217514\n",
            "Epoch 15311: train loss: 0.022427888587117195\n",
            "Epoch 15312: train loss: 0.02242344804108143\n",
            "Epoch 15313: train loss: 0.02241901308298111\n",
            "Epoch 15314: train loss: 0.02241458185017109\n",
            "Epoch 15315: train loss: 0.02241014689207077\n",
            "Epoch 15316: train loss: 0.02240571938455105\n",
            "Epoch 15317: train loss: 0.02240128628909588\n",
            "Epoch 15318: train loss: 0.022396858781576157\n",
            "Epoch 15319: train loss: 0.022392436861991882\n",
            "Epoch 15320: train loss: 0.022388014942407608\n",
            "Epoch 15321: train loss: 0.022383596748113632\n",
            "Epoch 15322: train loss: 0.022379178553819656\n",
            "Epoch 15323: train loss: 0.02237475849688053\n",
            "Epoch 15324: train loss: 0.022370345890522003\n",
            "Epoch 15325: train loss: 0.022365925833582878\n",
            "Epoch 15326: train loss: 0.02236151322722435\n",
            "Epoch 15327: train loss: 0.022357098758220673\n",
            "Epoch 15328: train loss: 0.022352691739797592\n",
            "Epoch 15329: train loss: 0.022348279133439064\n",
            "Epoch 15330: train loss: 0.022343870252370834\n",
            "Epoch 15331: train loss: 0.0223394688218832\n",
            "Epoch 15332: train loss: 0.02233506739139557\n",
            "Epoch 15333: train loss: 0.022330669686198235\n",
            "Epoch 15334: train loss: 0.022326266393065453\n",
            "Epoch 15335: train loss: 0.02232186868786812\n",
            "Epoch 15336: train loss: 0.022317472845315933\n",
            "Epoch 15337: train loss: 0.022313077002763748\n",
            "Epoch 15338: train loss: 0.02230868674814701\n",
            "Epoch 15339: train loss: 0.022304292768239975\n",
            "Epoch 15340: train loss: 0.02229989878833294\n",
            "Epoch 15341: train loss: 0.02229551039636135\n",
            "Epoch 15342: train loss: 0.022291122004389763\n",
            "Epoch 15343: train loss: 0.022286739200353622\n",
            "Epoch 15344: train loss: 0.022282356396317482\n",
            "Epoch 15345: train loss: 0.02227797545492649\n",
            "Epoch 15346: train loss: 0.022273598238825798\n",
            "Epoch 15347: train loss: 0.022269217297434807\n",
            "Epoch 15348: train loss: 0.022264841943979263\n",
            "Epoch 15349: train loss: 0.022260474041104317\n",
            "Epoch 15350: train loss: 0.022256098687648773\n",
            "Epoch 15351: train loss: 0.02225172147154808\n",
            "Epoch 15352: train loss: 0.022247349843382835\n",
            "Epoch 15353: train loss: 0.02224298194050789\n",
            "Epoch 15354: train loss: 0.022238610312342644\n",
            "Epoch 15355: train loss: 0.022234244272112846\n",
            "Epoch 15356: train loss: 0.022229881957173347\n",
            "Epoch 15357: train loss: 0.022225521504878998\n",
            "Epoch 15358: train loss: 0.0222211591899395\n",
            "Epoch 15359: train loss: 0.022216804325580597\n",
            "Epoch 15360: train loss: 0.022212449461221695\n",
            "Epoch 15361: train loss: 0.022208096459507942\n",
            "Epoch 15362: train loss: 0.022203747183084488\n",
            "Epoch 15363: train loss: 0.022199394181370735\n",
            "Epoch 15364: train loss: 0.022195041179656982\n",
            "Epoch 15365: train loss: 0.022190691903233528\n",
            "Epoch 15366: train loss: 0.022186346352100372\n",
            "Epoch 15367: train loss: 0.02218199521303177\n",
            "Epoch 15368: train loss: 0.022177647799253464\n",
            "Epoch 15369: train loss: 0.022173307836055756\n",
            "Epoch 15370: train loss: 0.02216896414756775\n",
            "Epoch 15371: train loss: 0.022164631634950638\n",
            "Epoch 15372: train loss: 0.022160295397043228\n",
            "Epoch 15373: train loss: 0.02215595729649067\n",
            "Epoch 15374: train loss: 0.022151626646518707\n",
            "Epoch 15375: train loss: 0.022147299721837044\n",
            "Epoch 15376: train loss: 0.022142969071865082\n",
            "Epoch 15377: train loss: 0.02213863842189312\n",
            "Epoch 15378: train loss: 0.022134315222501755\n",
            "Epoch 15379: train loss: 0.022129986435174942\n",
            "Epoch 15380: train loss: 0.022125661373138428\n",
            "Epoch 15381: train loss: 0.022121336311101913\n",
            "Epoch 15382: train loss: 0.022117016837000847\n",
            "Epoch 15383: train loss: 0.022112691774964333\n",
            "Epoch 15384: train loss: 0.022108376026153564\n",
            "Epoch 15385: train loss: 0.022104064002633095\n",
            "Epoch 15386: train loss: 0.022099744528532028\n",
            "Epoch 15387: train loss: 0.02209543064236641\n",
            "Epoch 15388: train loss: 0.02209111675620079\n",
            "Epoch 15389: train loss: 0.022086817771196365\n",
            "Epoch 15390: train loss: 0.022082505747675896\n",
            "Epoch 15391: train loss: 0.022078199312090874\n",
            "Epoch 15392: train loss: 0.0220738984644413\n",
            "Epoch 15393: train loss: 0.022069590166211128\n",
            "Epoch 15394: train loss: 0.022065287455916405\n",
            "Epoch 15395: train loss: 0.02206098847091198\n",
            "Epoch 15396: train loss: 0.022056685760617256\n",
            "Epoch 15397: train loss: 0.022052381187677383\n",
            "Epoch 15398: train loss: 0.022048095241189003\n",
            "Epoch 15399: train loss: 0.022043803706765175\n",
            "Epoch 15400: train loss: 0.022039512172341347\n",
            "Epoch 15401: train loss: 0.022035222500562668\n",
            "Epoch 15402: train loss: 0.022030936554074287\n",
            "Epoch 15403: train loss: 0.02202664501965046\n",
            "Epoch 15404: train loss: 0.022022364661097527\n",
            "Epoch 15405: train loss: 0.022018082439899445\n",
            "Epoch 15406: train loss: 0.022013798356056213\n",
            "Epoch 15407: train loss: 0.022009512409567833\n",
            "Epoch 15408: train loss: 0.02200523018836975\n",
            "Epoch 15409: train loss: 0.022000949829816818\n",
            "Epoch 15410: train loss: 0.021996673196554184\n",
            "Epoch 15411: train loss: 0.0219923947006464\n",
            "Epoch 15412: train loss: 0.021988127380609512\n",
            "Epoch 15413: train loss: 0.021983860060572624\n",
            "Epoch 15414: train loss: 0.021979590877890587\n",
            "Epoch 15415: train loss: 0.021975327283143997\n",
            "Epoch 15416: train loss: 0.02197105810046196\n",
            "Epoch 15417: train loss: 0.02196679264307022\n",
            "Epoch 15418: train loss: 0.02196253463625908\n",
            "Epoch 15419: train loss: 0.021958274766802788\n",
            "Epoch 15420: train loss: 0.021954013034701347\n",
            "Epoch 15421: train loss: 0.021949749439954758\n",
            "Epoch 15422: train loss: 0.021945495158433914\n",
            "Epoch 15423: train loss: 0.021941235288977623\n",
            "Epoch 15424: train loss: 0.021936975419521332\n",
            "Epoch 15425: train loss: 0.021932724863290787\n",
            "Epoch 15426: train loss: 0.021928468719124794\n",
            "Epoch 15427: train loss: 0.021924223750829697\n",
            "Epoch 15428: train loss: 0.02191997691988945\n",
            "Epoch 15429: train loss: 0.0219157375395298\n",
            "Epoch 15430: train loss: 0.021911488845944405\n",
            "Epoch 15431: train loss: 0.021907245740294456\n",
            "Epoch 15432: train loss: 0.021903010085225105\n",
            "Epoch 15433: train loss: 0.021898770704865456\n",
            "Epoch 15434: train loss: 0.021894535049796104\n",
            "Epoch 15435: train loss: 0.021890293806791306\n",
            "Epoch 15436: train loss: 0.021886056289076805\n",
            "Epoch 15437: train loss: 0.0218818262219429\n",
            "Epoch 15438: train loss: 0.02187759056687355\n",
            "Epoch 15439: train loss: 0.0218733549118042\n",
            "Epoch 15440: train loss: 0.021869126707315445\n",
            "Epoch 15441: train loss: 0.021864907816052437\n",
            "Epoch 15442: train loss: 0.021860681474208832\n",
            "Epoch 15443: train loss: 0.021856460720300674\n",
            "Epoch 15444: train loss: 0.021852239966392517\n",
            "Epoch 15445: train loss: 0.021848022937774658\n",
            "Epoch 15446: train loss: 0.02184380032122135\n",
            "Epoch 15447: train loss: 0.021839581429958344\n",
            "Epoch 15448: train loss: 0.021835362538695335\n",
            "Epoch 15449: train loss: 0.021831156685948372\n",
            "Epoch 15450: train loss: 0.021826935932040215\n",
            "Epoch 15451: train loss: 0.021822728216648102\n",
            "Epoch 15452: train loss: 0.021818509325385094\n",
            "Epoch 15453: train loss: 0.02181430719792843\n",
            "Epoch 15454: train loss: 0.02181009203195572\n",
            "Epoch 15455: train loss: 0.021805889904499054\n",
            "Epoch 15456: train loss: 0.02180168591439724\n",
            "Epoch 15457: train loss: 0.021797489374876022\n",
            "Epoch 15458: train loss: 0.021793296560645103\n",
            "Epoch 15459: train loss: 0.021789096295833588\n",
            "Epoch 15460: train loss: 0.02178489789366722\n",
            "Epoch 15461: train loss: 0.021780705079436302\n",
            "Epoch 15462: train loss: 0.021776504814624786\n",
            "Epoch 15463: train loss: 0.021772313863039017\n",
            "Epoch 15464: train loss: 0.021768121048808098\n",
            "Epoch 15465: train loss: 0.021763930097222328\n",
            "Epoch 15466: train loss: 0.021759744733572006\n",
            "Epoch 15467: train loss: 0.021755551919341087\n",
            "Epoch 15468: train loss: 0.021751360967755318\n",
            "Epoch 15469: train loss: 0.021747181192040443\n",
            "Epoch 15470: train loss: 0.02174299955368042\n",
            "Epoch 15471: train loss: 0.021738821640610695\n",
            "Epoch 15472: train loss: 0.02173464372754097\n",
            "Epoch 15473: train loss: 0.021730469539761543\n",
            "Epoch 15474: train loss: 0.021726295351982117\n",
            "Epoch 15475: train loss: 0.02172212116420269\n",
            "Epoch 15476: train loss: 0.021717943251132965\n",
            "Epoch 15477: train loss: 0.021713772788643837\n",
            "Epoch 15478: train loss: 0.021709606051445007\n",
            "Epoch 15479: train loss: 0.02170543558895588\n",
            "Epoch 15480: train loss: 0.0217012707144022\n",
            "Epoch 15481: train loss: 0.02169709838926792\n",
            "Epoch 15482: train loss: 0.02169293724000454\n",
            "Epoch 15483: train loss: 0.021688776090741158\n",
            "Epoch 15484: train loss: 0.021684614941477776\n",
            "Epoch 15485: train loss: 0.02168045938014984\n",
            "Epoch 15486: train loss: 0.021676305681467056\n",
            "Epoch 15487: train loss: 0.021672146394848824\n",
            "Epoch 15488: train loss: 0.02166799269616604\n",
            "Epoch 15489: train loss: 0.0216638445854187\n",
            "Epoch 15490: train loss: 0.021659687161445618\n",
            "Epoch 15491: train loss: 0.02165554091334343\n",
            "Epoch 15492: train loss: 0.021651387214660645\n",
            "Epoch 15493: train loss: 0.021647246554493904\n",
            "Epoch 15494: train loss: 0.021643102169036865\n",
            "Epoch 15495: train loss: 0.021638955920934677\n",
            "Epoch 15496: train loss: 0.02163480967283249\n",
            "Epoch 15497: train loss: 0.021630670875310898\n",
            "Epoch 15498: train loss: 0.021626530215144157\n",
            "Epoch 15499: train loss: 0.021622397005558014\n",
            "Epoch 15500: train loss: 0.021618258208036423\n",
            "Epoch 15501: train loss: 0.021614128723740578\n",
            "Epoch 15502: train loss: 0.021610001102089882\n",
            "Epoch 15503: train loss: 0.02160586602985859\n",
            "Epoch 15504: train loss: 0.021601732820272446\n",
            "Epoch 15505: train loss: 0.02159760147333145\n",
            "Epoch 15506: train loss: 0.021593475714325905\n",
            "Epoch 15507: train loss: 0.02158934995532036\n",
            "Epoch 15508: train loss: 0.02158522792160511\n",
            "Epoch 15509: train loss: 0.021581096574664116\n",
            "Epoch 15510: train loss: 0.021576980128884315\n",
            "Epoch 15511: train loss: 0.021572863683104515\n",
            "Epoch 15512: train loss: 0.021568745374679565\n",
            "Epoch 15513: train loss: 0.021564628928899765\n",
            "Epoch 15514: train loss: 0.021560516208410263\n",
            "Epoch 15515: train loss: 0.021556397899985313\n",
            "Epoch 15516: train loss: 0.02155228890478611\n",
            "Epoch 15517: train loss: 0.021548181772232056\n",
            "Epoch 15518: train loss: 0.02154407650232315\n",
            "Epoch 15519: train loss: 0.0215399619191885\n",
            "Epoch 15520: train loss: 0.021535858511924744\n",
            "Epoch 15521: train loss: 0.021531760692596436\n",
            "Epoch 15522: train loss: 0.02152765356004238\n",
            "Epoch 15523: train loss: 0.021523550152778625\n",
            "Epoch 15524: train loss: 0.021519450470805168\n",
            "Epoch 15525: train loss: 0.02151535078883171\n",
            "Epoch 15526: train loss: 0.0215112566947937\n",
            "Epoch 15527: train loss: 0.02150716446340084\n",
            "Epoch 15528: train loss: 0.02150307036936283\n",
            "Epoch 15529: train loss: 0.02149897627532482\n",
            "Epoch 15530: train loss: 0.02149488590657711\n",
            "Epoch 15531: train loss: 0.0214907955378294\n",
            "Epoch 15532: train loss: 0.021486708894371986\n",
            "Epoch 15533: train loss: 0.02148263156414032\n",
            "Epoch 15534: train loss: 0.02147853933274746\n",
            "Epoch 15535: train loss: 0.021474458277225494\n",
            "Epoch 15536: train loss: 0.02147037349641323\n",
            "Epoch 15537: train loss: 0.021466290578246117\n",
            "Epoch 15538: train loss: 0.02146221324801445\n",
            "Epoch 15539: train loss: 0.021458137780427933\n",
            "Epoch 15540: train loss: 0.021454060450196266\n",
            "Epoch 15541: train loss: 0.021449988707900047\n",
            "Epoch 15542: train loss: 0.02144591137766838\n",
            "Epoch 15543: train loss: 0.021441837772727013\n",
            "Epoch 15544: train loss: 0.021437766030430794\n",
            "Epoch 15545: train loss: 0.021433701738715172\n",
            "Epoch 15546: train loss: 0.02142963744699955\n",
            "Epoch 15547: train loss: 0.021425573155283928\n",
            "Epoch 15548: train loss: 0.021421508863568306\n",
            "Epoch 15549: train loss: 0.021417448297142982\n",
            "Epoch 15550: train loss: 0.02141338586807251\n",
            "Epoch 15551: train loss: 0.021409327164292336\n",
            "Epoch 15552: train loss: 0.02140527032315731\n",
            "Epoch 15553: train loss: 0.021401215344667435\n",
            "Epoch 15554: train loss: 0.021397164091467857\n",
            "Epoch 15555: train loss: 0.02139311097562313\n",
            "Epoch 15556: train loss: 0.021389054134488106\n",
            "Epoch 15557: train loss: 0.021385006606578827\n",
            "Epoch 15558: train loss: 0.021380960941314697\n",
            "Epoch 15559: train loss: 0.021376900374889374\n",
            "Epoch 15560: train loss: 0.021372854709625244\n",
            "Epoch 15561: train loss: 0.021368809044361115\n",
            "Epoch 15562: train loss: 0.021364767104387283\n",
            "Epoch 15563: train loss: 0.02136072888970375\n",
            "Epoch 15564: train loss: 0.02135668508708477\n",
            "Epoch 15565: train loss: 0.021352652460336685\n",
            "Epoch 15566: train loss: 0.021348612383008003\n",
            "Epoch 15567: train loss: 0.021344579756259918\n",
            "Epoch 15568: train loss: 0.021340545266866684\n",
            "Epoch 15569: train loss: 0.021336514502763748\n",
            "Epoch 15570: train loss: 0.021332483738660812\n",
            "Epoch 15571: train loss: 0.021328454837203026\n",
            "Epoch 15572: train loss: 0.02132442034780979\n",
            "Epoch 15573: train loss: 0.021320393308997154\n",
            "Epoch 15574: train loss: 0.021316364407539368\n",
            "Epoch 15575: train loss: 0.021312344819307327\n",
            "Epoch 15576: train loss: 0.021308323368430138\n",
            "Epoch 15577: train loss: 0.0213043000549078\n",
            "Epoch 15578: train loss: 0.021300282329320908\n",
            "Epoch 15579: train loss: 0.021296262741088867\n",
            "Epoch 15580: train loss: 0.021292248740792274\n",
            "Epoch 15581: train loss: 0.021288232877850533\n",
            "Epoch 15582: train loss: 0.02128421701490879\n",
            "Epoch 15583: train loss: 0.021280212327837944\n",
            "Epoch 15584: train loss: 0.021276196464896202\n",
            "Epoch 15585: train loss: 0.021272188052535057\n",
            "Epoch 15586: train loss: 0.021268179640173912\n",
            "Epoch 15587: train loss: 0.021264171227812767\n",
            "Epoch 15588: train loss: 0.021260160952806473\n",
            "Epoch 15589: train loss: 0.021256161853671074\n",
            "Epoch 15590: train loss: 0.021252155303955078\n",
            "Epoch 15591: train loss: 0.02124815247952938\n",
            "Epoch 15592: train loss: 0.02124415524303913\n",
            "Epoch 15593: train loss: 0.02124016173183918\n",
            "Epoch 15594: train loss: 0.021236160770058632\n",
            "Epoch 15595: train loss: 0.021232163533568382\n",
            "Epoch 15596: train loss: 0.02122817561030388\n",
            "Epoch 15597: train loss: 0.021224183961749077\n",
            "Epoch 15598: train loss: 0.021220192313194275\n",
            "Epoch 15599: train loss: 0.02121620811522007\n",
            "Epoch 15600: train loss: 0.021212222054600716\n",
            "Epoch 15601: train loss: 0.021208230406045914\n",
            "Epoch 15602: train loss: 0.02120424434542656\n",
            "Epoch 15603: train loss: 0.021200254559516907\n",
            "Epoch 15604: train loss: 0.02119627594947815\n",
            "Epoch 15605: train loss: 0.021192291751503944\n",
            "Epoch 15606: train loss: 0.021188311278820038\n",
            "Epoch 15607: train loss: 0.021184338256716728\n",
            "Epoch 15608: train loss: 0.02118036150932312\n",
            "Epoch 15609: train loss: 0.021176384761929512\n",
            "Epoch 15610: train loss: 0.021172409877181053\n",
            "Epoch 15611: train loss: 0.02116844244301319\n",
            "Epoch 15612: train loss: 0.02116447500884533\n",
            "Epoch 15613: train loss: 0.021160507574677467\n",
            "Epoch 15614: train loss: 0.021156538277864456\n",
            "Epoch 15615: train loss: 0.021152570843696594\n",
            "Epoch 15616: train loss: 0.02114860527217388\n",
            "Epoch 15617: train loss: 0.02114463970065117\n",
            "Epoch 15618: train loss: 0.021140674129128456\n",
            "Epoch 15619: train loss: 0.021136712282896042\n",
            "Epoch 15620: train loss: 0.021132750436663628\n",
            "Epoch 15621: train loss: 0.02112879790365696\n",
            "Epoch 15622: train loss: 0.021124839782714844\n",
            "Epoch 15623: train loss: 0.021120883524417877\n",
            "Epoch 15624: train loss: 0.021116938441991806\n",
            "Epoch 15625: train loss: 0.02111298404633999\n",
            "Epoch 15626: train loss: 0.02110903337597847\n",
            "Epoch 15627: train loss: 0.02110508643090725\n",
            "Epoch 15628: train loss: 0.02110113762319088\n",
            "Epoch 15629: train loss: 0.021097194403409958\n",
            "Epoch 15630: train loss: 0.021093247458338737\n",
            "Epoch 15631: train loss: 0.021089298650622368\n",
            "Epoch 15632: train loss: 0.021085361018776894\n",
            "Epoch 15633: train loss: 0.02108142338693142\n",
            "Epoch 15634: train loss: 0.0210774727165699\n",
            "Epoch 15635: train loss: 0.021073533222079277\n",
            "Epoch 15636: train loss: 0.021069597452878952\n",
            "Epoch 15637: train loss: 0.021065665408968925\n",
            "Epoch 15638: train loss: 0.021061737090349197\n",
            "Epoch 15639: train loss: 0.02105780504643917\n",
            "Epoch 15640: train loss: 0.021053871139883995\n",
            "Epoch 15641: train loss: 0.021049942821264267\n",
            "Epoch 15642: train loss: 0.021046016365289688\n",
            "Epoch 15643: train loss: 0.02104209177196026\n",
            "Epoch 15644: train loss: 0.02103816159069538\n",
            "Epoch 15645: train loss: 0.02103424444794655\n",
            "Epoch 15646: train loss: 0.021030321717262268\n",
            "Epoch 15647: train loss: 0.021026398986577988\n",
            "Epoch 15648: train loss: 0.021022478118538857\n",
            "Epoch 15649: train loss: 0.021018559113144875\n",
            "Epoch 15650: train loss: 0.021014638245105743\n",
            "Epoch 15651: train loss: 0.021010715514421463\n",
            "Epoch 15652: train loss: 0.021006803959608078\n",
            "Epoch 15653: train loss: 0.021002894267439842\n",
            "Epoch 15654: train loss: 0.020998986437916756\n",
            "Epoch 15655: train loss: 0.02099507674574852\n",
            "Epoch 15656: train loss: 0.020991167053580284\n",
            "Epoch 15657: train loss: 0.020987268537282944\n",
            "Epoch 15658: train loss: 0.020983360707759857\n",
            "Epoch 15659: train loss: 0.02097945474088192\n",
            "Epoch 15660: train loss: 0.020975550636649132\n",
            "Epoch 15661: train loss: 0.020971648395061493\n",
            "Epoch 15662: train loss: 0.020967751741409302\n",
            "Epoch 15663: train loss: 0.020963849499821663\n",
            "Epoch 15664: train loss: 0.020959950983524323\n",
            "Epoch 15665: train loss: 0.02095605432987213\n",
            "Epoch 15666: train loss: 0.02095215581357479\n",
            "Epoch 15667: train loss: 0.020948262885212898\n",
            "Epoch 15668: train loss: 0.020944369956851006\n",
            "Epoch 15669: train loss: 0.02094048261642456\n",
            "Epoch 15670: train loss: 0.020936593413352966\n",
            "Epoch 15671: train loss: 0.02093271166086197\n",
            "Epoch 15672: train loss: 0.020928824320435524\n",
            "Epoch 15673: train loss: 0.02092493698000908\n",
            "Epoch 15674: train loss: 0.020921053364872932\n",
            "Epoch 15675: train loss: 0.02091718092560768\n",
            "Epoch 15676: train loss: 0.020913293585181236\n",
            "Epoch 15677: train loss: 0.020909415557980537\n",
            "Epoch 15678: train loss: 0.02090553753077984\n",
            "Epoch 15679: train loss: 0.02090166136622429\n",
            "Epoch 15680: train loss: 0.02089778520166874\n",
            "Epoch 15681: train loss: 0.02089391089975834\n",
            "Epoch 15682: train loss: 0.02089003100991249\n",
            "Epoch 15683: train loss: 0.020886162295937538\n",
            "Epoch 15684: train loss: 0.020882295444607735\n",
            "Epoch 15685: train loss: 0.02087842859327793\n",
            "Epoch 15686: train loss: 0.020874563604593277\n",
            "Epoch 15687: train loss: 0.020870696753263474\n",
            "Epoch 15688: train loss: 0.020866837352514267\n",
            "Epoch 15689: train loss: 0.02086297608911991\n",
            "Epoch 15690: train loss: 0.020859114825725555\n",
            "Epoch 15691: train loss: 0.02085525542497635\n",
            "Epoch 15692: train loss: 0.020851394161581993\n",
            "Epoch 15693: train loss: 0.020847536623477936\n",
            "Epoch 15694: train loss: 0.020843684673309326\n",
            "Epoch 15695: train loss: 0.02083982713520527\n",
            "Epoch 15696: train loss: 0.02083597332239151\n",
            "Epoch 15697: train loss: 0.02083211950957775\n",
            "Epoch 15698: train loss: 0.02082826755940914\n",
            "Epoch 15699: train loss: 0.020824415609240532\n",
            "Epoch 15700: train loss: 0.02082057110965252\n",
            "Epoch 15701: train loss: 0.020816722884774208\n",
            "Epoch 15702: train loss: 0.020812883973121643\n",
            "Epoch 15703: train loss: 0.02080904133617878\n",
            "Epoch 15704: train loss: 0.020805196836590767\n",
            "Epoch 15705: train loss: 0.02080136351287365\n",
            "Epoch 15706: train loss: 0.020797522738575935\n",
            "Epoch 15707: train loss: 0.02079368568956852\n",
            "Epoch 15708: train loss: 0.020789844915270805\n",
            "Epoch 15709: train loss: 0.020786011591553688\n",
            "Epoch 15710: train loss: 0.020782172679901123\n",
            "Epoch 15711: train loss: 0.020778339356184006\n",
            "Epoch 15712: train loss: 0.02077450416982174\n",
            "Epoch 15713: train loss: 0.02077067829668522\n",
            "Epoch 15714: train loss: 0.02076685056090355\n",
            "Epoch 15715: train loss: 0.02076302096247673\n",
            "Epoch 15716: train loss: 0.02075919881463051\n",
            "Epoch 15717: train loss: 0.020755372941493988\n",
            "Epoch 15718: train loss: 0.020751550793647766\n",
            "Epoch 15719: train loss: 0.02074773795902729\n",
            "Epoch 15720: train loss: 0.02074391022324562\n",
            "Epoch 15721: train loss: 0.020740095525979996\n",
            "Epoch 15722: train loss: 0.020736277103424072\n",
            "Epoch 15723: train loss: 0.020732466131448746\n",
            "Epoch 15724: train loss: 0.020728643983602524\n",
            "Epoch 15725: train loss: 0.0207248292863369\n",
            "Epoch 15726: train loss: 0.020721012726426125\n",
            "Epoch 15727: train loss: 0.020717203617095947\n",
            "Epoch 15728: train loss: 0.020713388919830322\n",
            "Epoch 15729: train loss: 0.020709581673145294\n",
            "Epoch 15730: train loss: 0.02070576883852482\n",
            "Epoch 15731: train loss: 0.020701967179775238\n",
            "Epoch 15732: train loss: 0.020698167383670807\n",
            "Epoch 15733: train loss: 0.020694363862276077\n",
            "Epoch 15734: train loss: 0.020690571516752243\n",
            "Epoch 15735: train loss: 0.020686767995357513\n",
            "Epoch 15736: train loss: 0.020682960748672485\n",
            "Epoch 15737: train loss: 0.02067917212843895\n",
            "Epoch 15738: train loss: 0.020675377920269966\n",
            "Epoch 15739: train loss: 0.020671581849455833\n",
            "Epoch 15740: train loss: 0.02066778764128685\n",
            "Epoch 15741: train loss: 0.020663991570472717\n",
            "Epoch 15742: train loss: 0.020660199224948883\n",
            "Epoch 15743: train loss: 0.020656408742070198\n",
            "Epoch 15744: train loss: 0.020652618259191513\n",
            "Epoch 15745: train loss: 0.020648831501603127\n",
            "Epoch 15746: train loss: 0.02064504288136959\n",
            "Epoch 15747: train loss: 0.020641259849071503\n",
            "Epoch 15748: train loss: 0.020637482404708862\n",
            "Epoch 15749: train loss: 0.020633704960346222\n",
            "Epoch 15750: train loss: 0.020629923790693283\n",
            "Epoch 15751: train loss: 0.020626148208975792\n",
            "Epoch 15752: train loss: 0.02062237076461315\n",
            "Epoch 15753: train loss: 0.02061859518289566\n",
            "Epoch 15754: train loss: 0.02061481587588787\n",
            "Epoch 15755: train loss: 0.020611044019460678\n",
            "Epoch 15756: train loss: 0.020607279613614082\n",
            "Epoch 15757: train loss: 0.020603500306606293\n",
            "Epoch 15758: train loss: 0.02059972658753395\n",
            "Epoch 15759: train loss: 0.020595964044332504\n",
            "Epoch 15760: train loss: 0.02059219591319561\n",
            "Epoch 15761: train loss: 0.020588425919413567\n",
            "Epoch 15762: train loss: 0.02058465965092182\n",
            "Epoch 15763: train loss: 0.020580900833010674\n",
            "Epoch 15764: train loss: 0.02057713456451893\n",
            "Epoch 15765: train loss: 0.02057337947189808\n",
            "Epoch 15766: train loss: 0.02056962251663208\n",
            "Epoch 15767: train loss: 0.02056586928665638\n",
            "Epoch 15768: train loss: 0.02056211419403553\n",
            "Epoch 15769: train loss: 0.02055835723876953\n",
            "Epoch 15770: train loss: 0.02055460400879383\n",
            "Epoch 15771: train loss: 0.02055084891617298\n",
            "Epoch 15772: train loss: 0.02054709382355213\n",
            "Epoch 15773: train loss: 0.02054334245622158\n",
            "Epoch 15774: train loss: 0.020539596676826477\n",
            "Epoch 15775: train loss: 0.020535852760076523\n",
            "Epoch 15776: train loss: 0.02053210698068142\n",
            "Epoch 15777: train loss: 0.020528366789221764\n",
            "Epoch 15778: train loss: 0.02052462287247181\n",
            "Epoch 15779: train loss: 0.020520877093076706\n",
            "Epoch 15780: train loss: 0.02051713690161705\n",
            "Epoch 15781: train loss: 0.020513402298092842\n",
            "Epoch 15782: train loss: 0.020509663969278336\n",
            "Epoch 15783: train loss: 0.02050592750310898\n",
            "Epoch 15784: train loss: 0.02050219289958477\n",
            "Epoch 15785: train loss: 0.02049846015870571\n",
            "Epoch 15786: train loss: 0.020494727417826653\n",
            "Epoch 15787: train loss: 0.020490994676947594\n",
            "Epoch 15788: train loss: 0.020487265661358833\n",
            "Epoch 15789: train loss: 0.020483529195189476\n",
            "Epoch 15790: train loss: 0.02047981135547161\n",
            "Epoch 15791: train loss: 0.0204760804772377\n",
            "Epoch 15792: train loss: 0.02047235518693924\n",
            "Epoch 15793: train loss: 0.020468629896640778\n",
            "Epoch 15794: train loss: 0.020464912056922913\n",
            "Epoch 15795: train loss: 0.0204611886292696\n",
            "Epoch 15796: train loss: 0.020457467064261436\n",
            "Epoch 15797: train loss: 0.02045375108718872\n",
            "Epoch 15798: train loss: 0.020450029522180557\n",
            "Epoch 15799: train loss: 0.02044632099568844\n",
            "Epoch 15800: train loss: 0.020442599430680275\n",
            "Epoch 15801: train loss: 0.020438889041543007\n",
            "Epoch 15802: train loss: 0.02043517865240574\n",
            "Epoch 15803: train loss: 0.02043147198855877\n",
            "Epoch 15804: train loss: 0.020427759736776352\n",
            "Epoch 15805: train loss: 0.020424051210284233\n",
            "Epoch 15806: train loss: 0.020420348271727562\n",
            "Epoch 15807: train loss: 0.020416637882590294\n",
            "Epoch 15808: train loss: 0.020412934944033623\n",
            "Epoch 15809: train loss: 0.0204092375934124\n",
            "Epoch 15810: train loss: 0.02040553092956543\n",
            "Epoch 15811: train loss: 0.020401831716299057\n",
            "Epoch 15812: train loss: 0.020398128777742386\n",
            "Epoch 15813: train loss: 0.020394429564476013\n",
            "Epoch 15814: train loss: 0.020390741527080536\n",
            "Epoch 15815: train loss: 0.020387038588523865\n",
            "Epoch 15816: train loss: 0.02038334682583809\n",
            "Epoch 15817: train loss: 0.020379656925797462\n",
            "Epoch 15818: train loss: 0.020375965163111687\n",
            "Epoch 15819: train loss: 0.020372280851006508\n",
            "Epoch 15820: train loss: 0.020368585363030434\n",
            "Epoch 15821: train loss: 0.020364895462989807\n",
            "Epoch 15822: train loss: 0.02036121115088463\n",
            "Epoch 15823: train loss: 0.020357538014650345\n",
            "Epoch 15824: train loss: 0.020353853702545166\n",
            "Epoch 15825: train loss: 0.020350173115730286\n",
            "Epoch 15826: train loss: 0.02034648507833481\n",
            "Epoch 15827: train loss: 0.020342806354165077\n",
            "Epoch 15828: train loss: 0.0203391220420599\n",
            "Epoch 15829: train loss: 0.020335450768470764\n",
            "Epoch 15830: train loss: 0.020331768319010735\n",
            "Epoch 15831: train loss: 0.02032809518277645\n",
            "Epoch 15832: train loss: 0.020324427634477615\n",
            "Epoch 15833: train loss: 0.020320750772953033\n",
            "Epoch 15834: train loss: 0.02031708136200905\n",
            "Epoch 15835: train loss: 0.020313404500484467\n",
            "Epoch 15836: train loss: 0.02030974254012108\n",
            "Epoch 15837: train loss: 0.020306076854467392\n",
            "Epoch 15838: train loss: 0.020302409306168556\n",
            "Epoch 15839: train loss: 0.020298749208450317\n",
            "Epoch 15840: train loss: 0.02029508538544178\n",
            "Epoch 15841: train loss: 0.02029142715036869\n",
            "Epoch 15842: train loss: 0.02028776705265045\n",
            "Epoch 15843: train loss: 0.020284108817577362\n",
            "Epoch 15844: train loss: 0.020280448719859123\n",
            "Epoch 15845: train loss: 0.020276794210076332\n",
            "Epoch 15846: train loss: 0.020273134112358093\n",
            "Epoch 15847: train loss: 0.020269479602575302\n",
            "Epoch 15848: train loss: 0.02026583068072796\n",
            "Epoch 15849: train loss: 0.020262178033590317\n",
            "Epoch 15850: train loss: 0.020258523523807526\n",
            "Epoch 15851: train loss: 0.02025487646460533\n",
            "Epoch 15852: train loss: 0.020251234993338585\n",
            "Epoch 15853: train loss: 0.020247582346200943\n",
            "Epoch 15854: train loss: 0.020243940874934196\n",
            "Epoch 15855: train loss: 0.02024029940366745\n",
            "Epoch 15856: train loss: 0.020236654207110405\n",
            "Epoch 15857: train loss: 0.020233014598488808\n",
            "Epoch 15858: train loss: 0.02022937498986721\n",
            "Epoch 15859: train loss: 0.020225731655955315\n",
            "Epoch 15860: train loss: 0.020222093909978867\n",
            "Epoch 15861: train loss: 0.02021845616400242\n",
            "Epoch 15862: train loss: 0.02021482214331627\n",
            "Epoch 15863: train loss: 0.02021118998527527\n",
            "Epoch 15864: train loss: 0.020207559689879417\n",
            "Epoch 15865: train loss: 0.02020392380654812\n",
            "Epoch 15866: train loss: 0.020200295373797417\n",
            "Epoch 15867: train loss: 0.020196663215756416\n",
            "Epoch 15868: train loss: 0.02019304223358631\n",
            "Epoch 15869: train loss: 0.02018941007554531\n",
            "Epoch 15870: train loss: 0.020185790956020355\n",
            "Epoch 15871: train loss: 0.0201821681112051\n",
            "Epoch 15872: train loss: 0.020178545266389847\n",
            "Epoch 15873: train loss: 0.02017492614686489\n",
            "Epoch 15874: train loss: 0.020171301439404488\n",
            "Epoch 15875: train loss: 0.02016768790781498\n",
            "Epoch 15876: train loss: 0.020164063200354576\n",
            "Epoch 15877: train loss: 0.02016044594347477\n",
            "Epoch 15878: train loss: 0.020156828686594963\n",
            "Epoch 15879: train loss: 0.020153215155005455\n",
            "Epoch 15880: train loss: 0.020149601623415947\n",
            "Epoch 15881: train loss: 0.02014598809182644\n",
            "Epoch 15882: train loss: 0.02014238014817238\n",
            "Epoch 15883: train loss: 0.02013876475393772\n",
            "Epoch 15884: train loss: 0.020135164260864258\n",
            "Epoch 15885: train loss: 0.020131558179855347\n",
            "Epoch 15886: train loss: 0.020127957686781883\n",
            "Epoch 15887: train loss: 0.02012435719370842\n",
            "Epoch 15888: train loss: 0.02012075111269951\n",
            "Epoch 15889: train loss: 0.020117150619626045\n",
            "Epoch 15890: train loss: 0.02011355385184288\n",
            "Epoch 15891: train loss: 0.020109957084059715\n",
            "Epoch 15892: train loss: 0.020106354728341103\n",
            "Epoch 15893: train loss: 0.02010275609791279\n",
            "Epoch 15894: train loss: 0.020099157467484474\n",
            "Epoch 15895: train loss: 0.020095566287636757\n",
            "Epoch 15896: train loss: 0.020091969519853592\n",
            "Epoch 15897: train loss: 0.020088372752070427\n",
            "Epoch 15898: train loss: 0.020084789022803307\n",
            "Epoch 15899: train loss: 0.02008119598031044\n",
            "Epoch 15900: train loss: 0.020077606663107872\n",
            "Epoch 15901: train loss: 0.02007402293384075\n",
            "Epoch 15902: train loss: 0.02007044106721878\n",
            "Epoch 15903: train loss: 0.02006685920059681\n",
            "Epoch 15904: train loss: 0.02006327547132969\n",
            "Epoch 15905: train loss: 0.020059697329998016\n",
            "Epoch 15906: train loss: 0.020056115463376045\n",
            "Epoch 15907: train loss: 0.020052539184689522\n",
            "Epoch 15908: train loss: 0.0200489554554224\n",
            "Epoch 15909: train loss: 0.020045382902026176\n",
            "Epoch 15910: train loss: 0.020041802898049355\n",
            "Epoch 15911: train loss: 0.02003822848200798\n",
            "Epoch 15912: train loss: 0.020034654065966606\n",
            "Epoch 15913: train loss: 0.020031077787280083\n",
            "Epoch 15914: train loss: 0.02002750337123871\n",
            "Epoch 15915: train loss: 0.020023943856358528\n",
            "Epoch 15916: train loss: 0.020020371302962303\n",
            "Epoch 15917: train loss: 0.020016806200146675\n",
            "Epoch 15918: train loss: 0.020013248547911644\n",
            "Epoch 15919: train loss: 0.020009681582450867\n",
            "Epoch 15920: train loss: 0.02000611647963524\n",
            "Epoch 15921: train loss: 0.02000255510210991\n",
            "Epoch 15922: train loss: 0.019998997449874878\n",
            "Epoch 15923: train loss: 0.0199954342097044\n",
            "Epoch 15924: train loss: 0.019991876557469368\n",
            "Epoch 15925: train loss: 0.019988318905234337\n",
            "Epoch 15926: train loss: 0.019984759390354156\n",
            "Epoch 15927: train loss: 0.019981205463409424\n",
            "Epoch 15928: train loss: 0.01997765712440014\n",
            "Epoch 15929: train loss: 0.01997409388422966\n",
            "Epoch 15930: train loss: 0.019970549270510674\n",
            "Epoch 15931: train loss: 0.019966989755630493\n",
            "Epoch 15932: train loss: 0.019963445141911507\n",
            "Epoch 15933: train loss: 0.01995990425348282\n",
            "Epoch 15934: train loss: 0.01995636150240898\n",
            "Epoch 15935: train loss: 0.019952818751335144\n",
            "Epoch 15936: train loss: 0.01994927227497101\n",
            "Epoch 15937: train loss: 0.01994573138654232\n",
            "Epoch 15938: train loss: 0.019942190498113632\n",
            "Epoch 15939: train loss: 0.019938651472330093\n",
            "Epoch 15940: train loss: 0.019935110583901405\n",
            "Epoch 15941: train loss: 0.019931575283408165\n",
            "Epoch 15942: train loss: 0.019928034394979477\n",
            "Epoch 15943: train loss: 0.019924499094486237\n",
            "Epoch 15944: train loss: 0.019920963793992996\n",
            "Epoch 15945: train loss: 0.019917424768209457\n",
            "Epoch 15946: train loss: 0.019913895055651665\n",
            "Epoch 15947: train loss: 0.019910361617803574\n",
            "Epoch 15948: train loss: 0.01990683190524578\n",
            "Epoch 15949: train loss: 0.019903304055333138\n",
            "Epoch 15950: train loss: 0.019899781793355942\n",
            "Epoch 15951: train loss: 0.019896257668733597\n",
            "Epoch 15952: train loss: 0.01989273726940155\n",
            "Epoch 15953: train loss: 0.019889213144779205\n",
            "Epoch 15954: train loss: 0.01988569274544716\n",
            "Epoch 15955: train loss: 0.01988217420876026\n",
            "Epoch 15956: train loss: 0.019878650084137917\n",
            "Epoch 15957: train loss: 0.01987513154745102\n",
            "Epoch 15958: train loss: 0.01987161673605442\n",
            "Epoch 15959: train loss: 0.019868098199367523\n",
            "Epoch 15960: train loss: 0.019864579662680626\n",
            "Epoch 15961: train loss: 0.019861064851284027\n",
            "Epoch 15962: train loss: 0.019857553765177727\n",
            "Epoch 15963: train loss: 0.01985403336584568\n",
            "Epoch 15964: train loss: 0.01985051855444908\n",
            "Epoch 15965: train loss: 0.019847014918923378\n",
            "Epoch 15966: train loss: 0.019843511283397675\n",
            "Epoch 15967: train loss: 0.01984001137316227\n",
            "Epoch 15968: train loss: 0.019836505874991417\n",
            "Epoch 15969: train loss: 0.01983300782740116\n",
            "Epoch 15970: train loss: 0.01982949674129486\n",
            "Epoch 15971: train loss: 0.019825998693704605\n",
            "Epoch 15972: train loss: 0.019822504371404648\n",
            "Epoch 15973: train loss: 0.019819002598524094\n",
            "Epoch 15974: train loss: 0.019815504550933838\n",
            "Epoch 15975: train loss: 0.01981200836598873\n",
            "Epoch 15976: train loss: 0.019808506593108177\n",
            "Epoch 15977: train loss: 0.01980501413345337\n",
            "Epoch 15978: train loss: 0.019801517948508263\n",
            "Epoch 15979: train loss: 0.019798019900918007\n",
            "Epoch 15980: train loss: 0.0197945274412632\n",
            "Epoch 15981: train loss: 0.019791031256318092\n",
            "Epoch 15982: train loss: 0.01978754997253418\n",
            "Epoch 15983: train loss: 0.019784066826105118\n",
            "Epoch 15984: train loss: 0.019780581817030907\n",
            "Epoch 15985: train loss: 0.019777102395892143\n",
            "Epoch 15986: train loss: 0.019773615524172783\n",
            "Epoch 15987: train loss: 0.01977013237774372\n",
            "Epoch 15988: train loss: 0.019766652956604958\n",
            "Epoch 15989: train loss: 0.019763173535466194\n",
            "Epoch 15990: train loss: 0.019759690389037132\n",
            "Epoch 15991: train loss: 0.019756218418478966\n",
            "Epoch 15992: train loss: 0.01975274085998535\n",
            "Epoch 15993: train loss: 0.019749261438846588\n",
            "Epoch 15994: train loss: 0.019745782017707825\n",
            "Epoch 15995: train loss: 0.019742310047149658\n",
            "Epoch 15996: train loss: 0.019738828763365746\n",
            "Epoch 15997: train loss: 0.019735360518097878\n",
            "Epoch 15998: train loss: 0.01973188854753971\n",
            "Epoch 15999: train loss: 0.019728414714336395\n",
            "Epoch 16000: train loss: 0.019724957644939423\n",
            "Epoch 16001: train loss: 0.019721487537026405\n",
            "Epoch 16002: train loss: 0.019718024879693985\n",
            "Epoch 16003: train loss: 0.019714565947651863\n",
            "Epoch 16004: train loss: 0.019711105152964592\n",
            "Epoch 16005: train loss: 0.01970764435827732\n",
            "Epoch 16006: train loss: 0.019704187288880348\n",
            "Epoch 16007: train loss: 0.019700726494193077\n",
            "Epoch 16008: train loss: 0.019697269424796104\n",
            "Epoch 16009: train loss: 0.019693808630108833\n",
            "Epoch 16010: train loss: 0.01969035156071186\n",
            "Epoch 16011: train loss: 0.019686894491314888\n",
            "Epoch 16012: train loss: 0.019683443009853363\n",
            "Epoch 16013: train loss: 0.01967998407781124\n",
            "Epoch 16014: train loss: 0.019676536321640015\n",
            "Epoch 16015: train loss: 0.01967308297753334\n",
            "Epoch 16016: train loss: 0.019669633358716965\n",
            "Epoch 16017: train loss: 0.01966618187725544\n",
            "Epoch 16018: train loss: 0.01966274529695511\n",
            "Epoch 16019: train loss: 0.01965929940342903\n",
            "Epoch 16020: train loss: 0.019655855372548103\n",
            "Epoch 16021: train loss: 0.019652415066957474\n",
            "Epoch 16022: train loss: 0.019648974761366844\n",
            "Epoch 16023: train loss: 0.019645538181066513\n",
            "Epoch 16024: train loss: 0.019642094150185585\n",
            "Epoch 16025: train loss: 0.019638659432530403\n",
            "Epoch 16026: train loss: 0.019635220989584923\n",
            "Epoch 16027: train loss: 0.019631780683994293\n",
            "Epoch 16028: train loss: 0.01962834596633911\n",
            "Epoch 16029: train loss: 0.01962490752339363\n",
            "Epoch 16030: train loss: 0.0196214746683836\n",
            "Epoch 16031: train loss: 0.019618045538663864\n",
            "Epoch 16032: train loss: 0.01961461268365383\n",
            "Epoch 16033: train loss: 0.019611183553934097\n",
            "Epoch 16034: train loss: 0.01960776187479496\n",
            "Epoch 16035: train loss: 0.019604329019784927\n",
            "Epoch 16036: train loss: 0.019600914791226387\n",
            "Epoch 16037: train loss: 0.019597481936216354\n",
            "Epoch 16038: train loss: 0.019594060257077217\n",
            "Epoch 16039: train loss: 0.01959064044058323\n",
            "Epoch 16040: train loss: 0.01958722062408924\n",
            "Epoch 16041: train loss: 0.019583800807595253\n",
            "Epoch 16042: train loss: 0.019580384716391563\n",
            "Epoch 16043: train loss: 0.019576966762542725\n",
            "Epoch 16044: train loss: 0.019573548808693886\n",
            "Epoch 16045: train loss: 0.019570128992199898\n",
            "Epoch 16046: train loss: 0.019566716626286507\n",
            "Epoch 16047: train loss: 0.019563300535082817\n",
            "Epoch 16048: train loss: 0.019559888169169426\n",
            "Epoch 16049: train loss: 0.019556481391191483\n",
            "Epoch 16050: train loss: 0.01955307088792324\n",
            "Epoch 16051: train loss: 0.019549665972590446\n",
            "Epoch 16052: train loss: 0.019546251744031906\n",
            "Epoch 16053: train loss: 0.019542846828699112\n",
            "Epoch 16054: train loss: 0.019539441913366318\n",
            "Epoch 16055: train loss: 0.01953604817390442\n",
            "Epoch 16056: train loss: 0.019532639533281326\n",
            "Epoch 16057: train loss: 0.01952924020588398\n",
            "Epoch 16058: train loss: 0.019525840878486633\n",
            "Epoch 16059: train loss: 0.01952243782579899\n",
            "Epoch 16060: train loss: 0.01951904036104679\n",
            "Epoch 16061: train loss: 0.019515644758939743\n",
            "Epoch 16062: train loss: 0.01951223984360695\n",
            "Epoch 16063: train loss: 0.01950884610414505\n",
            "Epoch 16064: train loss: 0.0195054542273283\n",
            "Epoch 16065: train loss: 0.0195020642131567\n",
            "Epoch 16066: train loss: 0.01949867233633995\n",
            "Epoch 16067: train loss: 0.0194952804595232\n",
            "Epoch 16068: train loss: 0.01949189230799675\n",
            "Epoch 16069: train loss: 0.019488507881760597\n",
            "Epoch 16070: train loss: 0.019485117867588997\n",
            "Epoch 16071: train loss: 0.019481731578707695\n",
            "Epoch 16072: train loss: 0.01947835646569729\n",
            "Epoch 16073: train loss: 0.019474968314170837\n",
            "Epoch 16074: train loss: 0.019471585750579834\n",
            "Epoch 16075: train loss: 0.019468210637569427\n",
            "Epoch 16076: train loss: 0.019464829936623573\n",
            "Epoch 16077: train loss: 0.01946144737303257\n",
            "Epoch 16078: train loss: 0.019458070397377014\n",
            "Epoch 16079: train loss: 0.01945468597114086\n",
            "Epoch 16080: train loss: 0.019451310858130455\n",
            "Epoch 16081: train loss: 0.019447937607765198\n",
            "Epoch 16082: train loss: 0.01944456435739994\n",
            "Epoch 16083: train loss: 0.019441189244389534\n",
            "Epoch 16084: train loss: 0.019437823444604874\n",
            "Epoch 16085: train loss: 0.019434450194239616\n",
            "Epoch 16086: train loss: 0.019431084394454956\n",
            "Epoch 16087: train loss: 0.019427720457315445\n",
            "Epoch 16088: train loss: 0.019424352794885635\n",
            "Epoch 16089: train loss: 0.019420986995100975\n",
            "Epoch 16090: train loss: 0.019417624920606613\n",
            "Epoch 16091: train loss: 0.0194142647087574\n",
            "Epoch 16092: train loss: 0.01941090263426304\n",
            "Epoch 16093: train loss: 0.019407538697123528\n",
            "Epoch 16094: train loss: 0.019404185935854912\n",
            "Epoch 16095: train loss: 0.01940082013607025\n",
            "Epoch 16096: train loss: 0.019397465512156487\n",
            "Epoch 16097: train loss: 0.019394107162952423\n",
            "Epoch 16098: train loss: 0.01939074508845806\n",
            "Epoch 16099: train loss: 0.019387394189834595\n",
            "Epoch 16100: train loss: 0.019384047016501427\n",
            "Epoch 16101: train loss: 0.01938069611787796\n",
            "Epoch 16102: train loss: 0.019377343356609344\n",
            "Epoch 16103: train loss: 0.01937398687005043\n",
            "Epoch 16104: train loss: 0.01937064714729786\n",
            "Epoch 16105: train loss: 0.019367294386029243\n",
            "Epoch 16106: train loss: 0.019363949075341225\n",
            "Epoch 16107: train loss: 0.019360603764653206\n",
            "Epoch 16108: train loss: 0.019357258453965187\n",
            "Epoch 16109: train loss: 0.019353920593857765\n",
            "Epoch 16110: train loss: 0.019350579008460045\n",
            "Epoch 16111: train loss: 0.019347237423062325\n",
            "Epoch 16112: train loss: 0.019343897700309753\n",
            "Epoch 16113: train loss: 0.019340554252266884\n",
            "Epoch 16114: train loss: 0.019337214529514313\n",
            "Epoch 16115: train loss: 0.019333885982632637\n",
            "Epoch 16116: train loss: 0.019330546259880066\n",
            "Epoch 16117: train loss: 0.01932721585035324\n",
            "Epoch 16118: train loss: 0.019323885440826416\n",
            "Epoch 16119: train loss: 0.01932055503129959\n",
            "Epoch 16120: train loss: 0.019317220896482468\n",
            "Epoch 16121: train loss: 0.019313890486955643\n",
            "Epoch 16122: train loss: 0.019310563802719116\n",
            "Epoch 16123: train loss: 0.01930723525583744\n",
            "Epoch 16124: train loss: 0.019303910434246063\n",
            "Epoch 16125: train loss: 0.019300585612654686\n",
            "Epoch 16126: train loss: 0.01929725892841816\n",
            "Epoch 16127: train loss: 0.01929393783211708\n",
            "Epoch 16128: train loss: 0.019290616735816002\n",
            "Epoch 16129: train loss: 0.019287290051579475\n",
            "Epoch 16130: train loss: 0.019283976405858994\n",
            "Epoch 16131: train loss: 0.019280655309557915\n",
            "Epoch 16132: train loss: 0.019277332350611687\n",
            "Epoch 16133: train loss: 0.019274022430181503\n",
            "Epoch 16134: train loss: 0.019270701333880424\n",
            "Epoch 16135: train loss: 0.019267385825514793\n",
            "Epoch 16136: train loss: 0.019264083355665207\n",
            "Epoch 16137: train loss: 0.019260762259364128\n",
            "Epoch 16138: train loss: 0.019257457926869392\n",
            "Epoch 16139: train loss: 0.01925414614379406\n",
            "Epoch 16140: train loss: 0.019250838086009026\n",
            "Epoch 16141: train loss: 0.019247528165578842\n",
            "Epoch 16142: train loss: 0.019244221970438957\n",
            "Epoch 16143: train loss: 0.01924091763794422\n",
            "Epoch 16144: train loss: 0.019237611442804337\n",
            "Epoch 16145: train loss: 0.01923430897295475\n",
            "Epoch 16146: train loss: 0.019231008365750313\n",
            "Epoch 16147: train loss: 0.019227707758545876\n",
            "Epoch 16148: train loss: 0.01922440528869629\n",
            "Epoch 16149: train loss: 0.01922110840678215\n",
            "Epoch 16150: train loss: 0.019217807799577713\n",
            "Epoch 16151: train loss: 0.019214510917663574\n",
            "Epoch 16152: train loss: 0.019211214035749435\n",
            "Epoch 16153: train loss: 0.019207928329706192\n",
            "Epoch 16154: train loss: 0.019204631447792053\n",
            "Epoch 16155: train loss: 0.019201338291168213\n",
            "Epoch 16156: train loss: 0.019198046997189522\n",
            "Epoch 16157: train loss: 0.01919475942850113\n",
            "Epoch 16158: train loss: 0.01919146440923214\n",
            "Epoch 16159: train loss: 0.019188178703188896\n",
            "Epoch 16160: train loss: 0.019184887409210205\n",
            "Epoch 16161: train loss: 0.01918160542845726\n",
            "Epoch 16162: train loss: 0.019178321585059166\n",
            "Epoch 16163: train loss: 0.019175034016370773\n",
            "Epoch 16164: train loss: 0.019171757623553276\n",
            "Epoch 16165: train loss: 0.019168468192219734\n",
            "Epoch 16166: train loss: 0.019165191799402237\n",
            "Epoch 16167: train loss: 0.01916191168129444\n",
            "Epoch 16168: train loss: 0.019158637151122093\n",
            "Epoch 16169: train loss: 0.019155357033014297\n",
            "Epoch 16170: train loss: 0.019152086228132248\n",
            "Epoch 16171: train loss: 0.0191488116979599\n",
            "Epoch 16172: train loss: 0.0191455390304327\n",
            "Epoch 16173: train loss: 0.0191422700881958\n",
            "Epoch 16174: train loss: 0.019138997420668602\n",
            "Epoch 16175: train loss: 0.0191357284784317\n",
            "Epoch 16176: train loss: 0.019132452085614204\n",
            "Epoch 16177: train loss: 0.019129183143377304\n",
            "Epoch 16178: train loss: 0.019125910475850105\n",
            "Epoch 16179: train loss: 0.019122643396258354\n",
            "Epoch 16180: train loss: 0.019119378179311752\n",
            "Epoch 16181: train loss: 0.019116107374429703\n",
            "Epoch 16182: train loss: 0.019112853333353996\n",
            "Epoch 16183: train loss: 0.019109588116407394\n",
            "Epoch 16184: train loss: 0.01910633035004139\n",
            "Epoch 16185: train loss: 0.019103068858385086\n",
            "Epoch 16186: train loss: 0.01909981109201908\n",
            "Epoch 16187: train loss: 0.019096553325653076\n",
            "Epoch 16188: train loss: 0.019093303009867668\n",
            "Epoch 16189: train loss: 0.019090047106146812\n",
            "Epoch 16190: train loss: 0.019086791202425957\n",
            "Epoch 16191: train loss: 0.0190835390239954\n",
            "Epoch 16192: train loss: 0.019080281257629395\n",
            "Epoch 16193: train loss: 0.019077032804489136\n",
            "Epoch 16194: train loss: 0.019073784351348877\n",
            "Epoch 16195: train loss: 0.01907053031027317\n",
            "Epoch 16196: train loss: 0.01906728185713291\n",
            "Epoch 16197: train loss: 0.019064027816057205\n",
            "Epoch 16198: train loss: 0.019060784950852394\n",
            "Epoch 16199: train loss: 0.019057540223002434\n",
            "Epoch 16200: train loss: 0.019054295495152473\n",
            "Epoch 16201: train loss: 0.01905105449259281\n",
            "Epoch 16202: train loss: 0.019047811627388\n",
            "Epoch 16203: train loss: 0.01904457062482834\n",
            "Epoch 16204: train loss: 0.019041335210204124\n",
            "Epoch 16205: train loss: 0.01903810165822506\n",
            "Epoch 16206: train loss: 0.019034862518310547\n",
            "Epoch 16207: train loss: 0.019031623378396034\n",
            "Epoch 16208: train loss: 0.01902838610112667\n",
            "Epoch 16209: train loss: 0.019025152549147606\n",
            "Epoch 16210: train loss: 0.01902192085981369\n",
            "Epoch 16211: train loss: 0.019018687307834625\n",
            "Epoch 16212: train loss: 0.01901545189321041\n",
            "Epoch 16213: train loss: 0.019012227654457092\n",
            "Epoch 16214: train loss: 0.019008992239832878\n",
            "Epoch 16215: train loss: 0.01900576241314411\n",
            "Epoch 16216: train loss: 0.019002530723810196\n",
            "Epoch 16217: train loss: 0.018999308347702026\n",
            "Epoch 16218: train loss: 0.01899608224630356\n",
            "Epoch 16219: train loss: 0.01899285800755024\n",
            "Epoch 16220: train loss: 0.01898963749408722\n",
            "Epoch 16221: train loss: 0.018986418843269348\n",
            "Epoch 16222: train loss: 0.018983202055096626\n",
            "Epoch 16223: train loss: 0.018979983404278755\n",
            "Epoch 16224: train loss: 0.018976768478751183\n",
            "Epoch 16225: train loss: 0.01897355169057846\n",
            "Epoch 16226: train loss: 0.018970336765050888\n",
            "Epoch 16227: train loss: 0.018967116251587868\n",
            "Epoch 16228: train loss: 0.018963906913995743\n",
            "Epoch 16229: train loss: 0.01896069385111332\n",
            "Epoch 16230: train loss: 0.01895747520029545\n",
            "Epoch 16231: train loss: 0.018954264000058174\n",
            "Epoch 16232: train loss: 0.0189510527998209\n",
            "Epoch 16233: train loss: 0.018947839736938477\n",
            "Epoch 16234: train loss: 0.0189446322619915\n",
            "Epoch 16235: train loss: 0.018941417336463928\n",
            "Epoch 16236: train loss: 0.018938211724162102\n",
            "Epoch 16237: train loss: 0.018935011699795723\n",
            "Epoch 16238: train loss: 0.018931809812784195\n",
            "Epoch 16239: train loss: 0.01892860420048237\n",
            "Epoch 16240: train loss: 0.018925407901406288\n",
            "Epoch 16241: train loss: 0.01892220601439476\n",
            "Epoch 16242: train loss: 0.01891900785267353\n",
            "Epoch 16243: train loss: 0.0189158134162426\n",
            "Epoch 16244: train loss: 0.01891261525452137\n",
            "Epoch 16245: train loss: 0.01890941895544529\n",
            "Epoch 16246: train loss: 0.018906226381659508\n",
            "Epoch 16247: train loss: 0.018903033807873726\n",
            "Epoch 16248: train loss: 0.018899835646152496\n",
            "Epoch 16249: train loss: 0.018896641209721565\n",
            "Epoch 16250: train loss: 0.018893452361226082\n",
            "Epoch 16251: train loss: 0.01889025792479515\n",
            "Epoch 16252: train loss: 0.01888706348836422\n",
            "Epoch 16253: train loss: 0.018883878365159035\n",
            "Epoch 16254: train loss: 0.018880683928728104\n",
            "Epoch 16255: train loss: 0.018877500668168068\n",
            "Epoch 16256: train loss: 0.018874308094382286\n",
            "Epoch 16257: train loss: 0.018871136009693146\n",
            "Epoch 16258: train loss: 0.018867960199713707\n",
            "Epoch 16259: train loss: 0.018864769488573074\n",
            "Epoch 16260: train loss: 0.018861591815948486\n",
            "Epoch 16261: train loss: 0.0188584141433239\n",
            "Epoch 16262: train loss: 0.01885523460805416\n",
            "Epoch 16263: train loss: 0.018852055072784424\n",
            "Epoch 16264: train loss: 0.018848884850740433\n",
            "Epoch 16265: train loss: 0.018845712766051292\n",
            "Epoch 16266: train loss: 0.018842531368136406\n",
            "Epoch 16267: train loss: 0.018839359283447266\n",
            "Epoch 16268: train loss: 0.018836181610822678\n",
            "Epoch 16269: train loss: 0.018833013251423836\n",
            "Epoch 16270: train loss: 0.018829837441444397\n",
            "Epoch 16271: train loss: 0.018826663494110107\n",
            "Epoch 16272: train loss: 0.018823495134711266\n",
            "Epoch 16273: train loss: 0.018820328637957573\n",
            "Epoch 16274: train loss: 0.01881716586649418\n",
            "Epoch 16275: train loss: 0.018813997507095337\n",
            "Epoch 16276: train loss: 0.01881084032356739\n",
            "Epoch 16277: train loss: 0.018807673826813698\n",
            "Epoch 16278: train loss: 0.018804511055350304\n",
            "Epoch 16279: train loss: 0.018801352009177208\n",
            "Epoch 16280: train loss: 0.01879819668829441\n",
            "Epoch 16281: train loss: 0.018795033916831017\n",
            "Epoch 16282: train loss: 0.01879187673330307\n",
            "Epoch 16283: train loss: 0.018788713961839676\n",
            "Epoch 16284: train loss: 0.01878555864095688\n",
            "Epoch 16285: train loss: 0.01878240332007408\n",
            "Epoch 16286: train loss: 0.018779249861836433\n",
            "Epoch 16287: train loss: 0.018776098266243935\n",
            "Epoch 16288: train loss: 0.01877293922007084\n",
            "Epoch 16289: train loss: 0.01876979134976864\n",
            "Epoch 16290: train loss: 0.01876663975417614\n",
            "Epoch 16291: train loss: 0.018763484433293343\n",
            "Epoch 16292: train loss: 0.01876034401357174\n",
            "Epoch 16293: train loss: 0.01875719241797924\n",
            "Epoch 16294: train loss: 0.018754053860902786\n",
            "Epoch 16295: train loss: 0.018750909715890884\n",
            "Epoch 16296: train loss: 0.01874776929616928\n",
            "Epoch 16297: train loss: 0.01874462328851223\n",
            "Epoch 16298: train loss: 0.018741484731435776\n",
            "Epoch 16299: train loss: 0.018738340586423874\n",
            "Epoch 16300: train loss: 0.01873520202934742\n",
            "Epoch 16301: train loss: 0.018732065334916115\n",
            "Epoch 16302: train loss: 0.01872892864048481\n",
            "Epoch 16303: train loss: 0.018725788220763206\n",
            "Epoch 16304: train loss: 0.018722647801041603\n",
            "Epoch 16305: train loss: 0.018719511106610298\n",
            "Epoch 16306: train loss: 0.018716376274824142\n",
            "Epoch 16307: train loss: 0.018713243305683136\n",
            "Epoch 16308: train loss: 0.01871011033654213\n",
            "Epoch 16309: train loss: 0.018706977367401123\n",
            "Epoch 16310: train loss: 0.018703846260905266\n",
            "Epoch 16311: train loss: 0.01870071515440941\n",
            "Epoch 16312: train loss: 0.018697589635849\n",
            "Epoch 16313: train loss: 0.01869446411728859\n",
            "Epoch 16314: train loss: 0.01869134046137333\n",
            "Epoch 16315: train loss: 0.018688220530748367\n",
            "Epoch 16316: train loss: 0.018685096874833107\n",
            "Epoch 16317: train loss: 0.018681975081562996\n",
            "Epoch 16318: train loss: 0.018678855150938034\n",
            "Epoch 16319: train loss: 0.018675733357667923\n",
            "Epoch 16320: train loss: 0.01867261528968811\n",
            "Epoch 16321: train loss: 0.01866949535906315\n",
            "Epoch 16322: train loss: 0.018666373565793037\n",
            "Epoch 16323: train loss: 0.018663257360458374\n",
            "Epoch 16324: train loss: 0.018660137429833412\n",
            "Epoch 16325: train loss: 0.018657024949789047\n",
            "Epoch 16326: train loss: 0.018653908744454384\n",
            "Epoch 16327: train loss: 0.01865079440176487\n",
            "Epoch 16328: train loss: 0.018647681921720505\n",
            "Epoch 16329: train loss: 0.01864456757903099\n",
            "Epoch 16330: train loss: 0.018641460686922073\n",
            "Epoch 16331: train loss: 0.018638350069522858\n",
            "Epoch 16332: train loss: 0.018635248765349388\n",
            "Epoch 16333: train loss: 0.018632138147950172\n",
            "Epoch 16334: train loss: 0.018629036843776703\n",
            "Epoch 16335: train loss: 0.018625933676958084\n",
            "Epoch 16336: train loss: 0.018622830510139465\n",
            "Epoch 16337: train loss: 0.018619725480675697\n",
            "Epoch 16338: train loss: 0.018616626039147377\n",
            "Epoch 16339: train loss: 0.018613522872328758\n",
            "Epoch 16340: train loss: 0.018610423430800438\n",
            "Epoch 16341: train loss: 0.018607323989272118\n",
            "Epoch 16342: train loss: 0.018604226410388947\n",
            "Epoch 16343: train loss: 0.018601126968860626\n",
            "Epoch 16344: train loss: 0.018598033115267754\n",
            "Epoch 16345: train loss: 0.018594937399029732\n",
            "Epoch 16346: train loss: 0.01859183982014656\n",
            "Epoch 16347: train loss: 0.01858874410390854\n",
            "Epoch 16348: train loss: 0.01858564466238022\n",
            "Epoch 16349: train loss: 0.01858256198465824\n",
            "Epoch 16350: train loss: 0.018579473719000816\n",
            "Epoch 16351: train loss: 0.018576383590698242\n",
            "Epoch 16352: train loss: 0.018573300912976265\n",
            "Epoch 16353: train loss: 0.01857021264731884\n",
            "Epoch 16354: train loss: 0.018567131832242012\n",
            "Epoch 16355: train loss: 0.018564043566584587\n",
            "Epoch 16356: train loss: 0.01856096275150776\n",
            "Epoch 16357: train loss: 0.018557880073785782\n",
            "Epoch 16358: train loss: 0.018554793670773506\n",
            "Epoch 16359: train loss: 0.018551718443632126\n",
            "Epoch 16360: train loss: 0.01854863204061985\n",
            "Epoch 16361: train loss: 0.01854555681347847\n",
            "Epoch 16362: train loss: 0.018542475998401642\n",
            "Epoch 16363: train loss: 0.018539398908615112\n",
            "Epoch 16364: train loss: 0.018536323681473732\n",
            "Epoch 16365: train loss: 0.018533246591687202\n",
            "Epoch 16366: train loss: 0.018530165776610374\n",
            "Epoch 16367: train loss: 0.018527094274759293\n",
            "Epoch 16368: train loss: 0.01852402463555336\n",
            "Epoch 16369: train loss: 0.01852095127105713\n",
            "Epoch 16370: train loss: 0.018517881631851196\n",
            "Epoch 16371: train loss: 0.018514813855290413\n",
            "Epoch 16372: train loss: 0.01851174421608448\n",
            "Epoch 16373: train loss: 0.018508683890104294\n",
            "Epoch 16374: train loss: 0.01850561983883381\n",
            "Epoch 16375: train loss: 0.018502553924918175\n",
            "Epoch 16376: train loss: 0.01849948801100254\n",
            "Epoch 16377: train loss: 0.018496420234441757\n",
            "Epoch 16378: train loss: 0.01849336177110672\n",
            "Epoch 16379: train loss: 0.018490305170416832\n",
            "Epoch 16380: train loss: 0.018487241119146347\n",
            "Epoch 16381: train loss: 0.01848418079316616\n",
            "Epoch 16382: train loss: 0.018481116741895676\n",
            "Epoch 16383: train loss: 0.018478060141205788\n",
            "Epoch 16384: train loss: 0.018474997952580452\n",
            "Epoch 16385: train loss: 0.018471945077180862\n",
            "Epoch 16386: train loss: 0.018468888476490974\n",
            "Epoch 16387: train loss: 0.018465833738446236\n",
            "Epoch 16388: train loss: 0.018462779000401497\n",
            "Epoch 16389: train loss: 0.018459739163517952\n",
            "Epoch 16390: train loss: 0.018456684425473213\n",
            "Epoch 16391: train loss: 0.018453635275363922\n",
            "Epoch 16392: train loss: 0.018450593575835228\n",
            "Epoch 16393: train loss: 0.01844754070043564\n",
            "Epoch 16394: train loss: 0.018444500863552094\n",
            "Epoch 16395: train loss: 0.018441451713442802\n",
            "Epoch 16396: train loss: 0.01843840256333351\n",
            "Epoch 16397: train loss: 0.018435364589095116\n",
            "Epoch 16398: train loss: 0.018432321026921272\n",
            "Epoch 16399: train loss: 0.018429279327392578\n",
            "Epoch 16400: train loss: 0.018426237627863884\n",
            "Epoch 16401: train loss: 0.01842319592833519\n",
            "Epoch 16402: train loss: 0.018420157954096794\n",
            "Epoch 16403: train loss: 0.018417108803987503\n",
            "Epoch 16404: train loss: 0.018414076417684555\n",
            "Epoch 16405: train loss: 0.01841103844344616\n",
            "Epoch 16406: train loss: 0.018408004194498062\n",
            "Epoch 16407: train loss: 0.018404973670840263\n",
            "Epoch 16408: train loss: 0.018401935696601868\n",
            "Epoch 16409: train loss: 0.01839890331029892\n",
            "Epoch 16410: train loss: 0.01839587837457657\n",
            "Epoch 16411: train loss: 0.01839284598827362\n",
            "Epoch 16412: train loss: 0.01838981918990612\n",
            "Epoch 16413: train loss: 0.018386786803603172\n",
            "Epoch 16414: train loss: 0.01838376186788082\n",
            "Epoch 16415: train loss: 0.01838074065744877\n",
            "Epoch 16416: train loss: 0.01837771199643612\n",
            "Epoch 16417: train loss: 0.018374688923358917\n",
            "Epoch 16418: train loss: 0.018371662124991417\n",
            "Epoch 16419: train loss: 0.018368642777204514\n",
            "Epoch 16420: train loss: 0.018365615978837013\n",
            "Epoch 16421: train loss: 0.01836259663105011\n",
            "Epoch 16422: train loss: 0.01835957169532776\n",
            "Epoch 16423: train loss: 0.018356556072831154\n",
            "Epoch 16424: train loss: 0.01835354045033455\n",
            "Epoch 16425: train loss: 0.018350526690483093\n",
            "Epoch 16426: train loss: 0.01834750361740589\n",
            "Epoch 16427: train loss: 0.018344493582844734\n",
            "Epoch 16428: train loss: 0.01834147796034813\n",
            "Epoch 16429: train loss: 0.018338462337851524\n",
            "Epoch 16430: train loss: 0.018335452303290367\n",
            "Epoch 16431: train loss: 0.01833244040608406\n",
            "Epoch 16432: train loss: 0.018329432234168053\n",
            "Epoch 16433: train loss: 0.018326425924897194\n",
            "Epoch 16434: train loss: 0.018323415890336037\n",
            "Epoch 16435: train loss: 0.018320409581065178\n",
            "Epoch 16436: train loss: 0.01831740327179432\n",
            "Epoch 16437: train loss: 0.01831439509987831\n",
            "Epoch 16438: train loss: 0.01831139251589775\n",
            "Epoch 16439: train loss: 0.018308386206626892\n",
            "Epoch 16440: train loss: 0.018305376172065735\n",
            "Epoch 16441: train loss: 0.018302379176020622\n",
            "Epoch 16442: train loss: 0.01829938217997551\n",
            "Epoch 16443: train loss: 0.018296383321285248\n",
            "Epoch 16444: train loss: 0.018293384462594986\n",
            "Epoch 16445: train loss: 0.018290387466549873\n",
            "Epoch 16446: train loss: 0.01828739047050476\n",
            "Epoch 16447: train loss: 0.0182843916118145\n",
            "Epoch 16448: train loss: 0.018281400203704834\n",
            "Epoch 16449: train loss: 0.01827840320765972\n",
            "Epoch 16450: train loss: 0.018275413662195206\n",
            "Epoch 16451: train loss: 0.01827242039144039\n",
            "Epoch 16452: train loss: 0.018269430845975876\n",
            "Epoch 16453: train loss: 0.01826644502580166\n",
            "Epoch 16454: train loss: 0.018263449892401695\n",
            "Epoch 16455: train loss: 0.01826046220958233\n",
            "Epoch 16456: train loss: 0.01825748011469841\n",
            "Epoch 16457: train loss: 0.018254490569233894\n",
            "Epoch 16458: train loss: 0.01825149729847908\n",
            "Epoch 16459: train loss: 0.01824851892888546\n",
            "Epoch 16460: train loss: 0.018245533108711243\n",
            "Epoch 16461: train loss: 0.018242545425891876\n",
            "Epoch 16462: train loss: 0.018239568918943405\n",
            "Epoch 16463: train loss: 0.018236584961414337\n",
            "Epoch 16464: train loss: 0.018233606591820717\n",
            "Epoch 16465: train loss: 0.018230626359581947\n",
            "Epoch 16466: train loss: 0.018227647989988327\n",
            "Epoch 16467: train loss: 0.018224667757749557\n",
            "Epoch 16468: train loss: 0.01822168566286564\n",
            "Epoch 16469: train loss: 0.018218716606497765\n",
            "Epoch 16470: train loss: 0.01821574568748474\n",
            "Epoch 16471: train loss: 0.018212776631116867\n",
            "Epoch 16472: train loss: 0.018209800124168396\n",
            "Epoch 16473: train loss: 0.018206827342510223\n",
            "Epoch 16474: train loss: 0.018203863874077797\n",
            "Epoch 16475: train loss: 0.018200892955064774\n",
            "Epoch 16476: train loss: 0.0181979201734066\n",
            "Epoch 16477: train loss: 0.018194952979683876\n",
            "Epoch 16478: train loss: 0.01819198951125145\n",
            "Epoch 16479: train loss: 0.018189022317528725\n",
            "Epoch 16480: train loss: 0.018186049535870552\n",
            "Epoch 16481: train loss: 0.018183089792728424\n",
            "Epoch 16482: train loss: 0.018180128186941147\n",
            "Epoch 16483: train loss: 0.018177170306444168\n",
            "Epoch 16484: train loss: 0.01817420683801174\n",
            "Epoch 16485: train loss: 0.018171247094869614\n",
            "Epoch 16486: train loss: 0.01816827803850174\n",
            "Epoch 16487: train loss: 0.018165327608585358\n",
            "Epoch 16488: train loss: 0.018162358552217484\n",
            "Epoch 16489: train loss: 0.01815940998494625\n",
            "Epoch 16490: train loss: 0.01815645582973957\n",
            "Epoch 16491: train loss: 0.01815349981188774\n",
            "Epoch 16492: train loss: 0.01815054565668106\n",
            "Epoch 16493: train loss: 0.018147602677345276\n",
            "Epoch 16494: train loss: 0.018144648522138596\n",
            "Epoch 16495: train loss: 0.018141690641641617\n",
            "Epoch 16496: train loss: 0.018138743937015533\n",
            "Epoch 16497: train loss: 0.018135791644454002\n",
            "Epoch 16498: train loss: 0.018132846802473068\n",
            "Epoch 16499: train loss: 0.018129898235201836\n",
            "Epoch 16500: train loss: 0.018126945942640305\n",
            "Epoch 16501: train loss: 0.018124006688594818\n",
            "Epoch 16502: train loss: 0.018121061846613884\n",
            "Epoch 16503: train loss: 0.0181181188672781\n",
            "Epoch 16504: train loss: 0.018115174025297165\n",
            "Epoch 16505: train loss: 0.018112236633896828\n",
            "Epoch 16506: train loss: 0.018109293654561043\n",
            "Epoch 16507: train loss: 0.018106350675225258\n",
            "Epoch 16508: train loss: 0.01810341887176037\n",
            "Epoch 16509: train loss: 0.018100470304489136\n",
            "Epoch 16510: train loss: 0.0180975329130888\n",
            "Epoch 16511: train loss: 0.01809459738433361\n",
            "Epoch 16512: train loss: 0.01809166744351387\n",
            "Epoch 16513: train loss: 0.01808873564004898\n",
            "Epoch 16514: train loss: 0.018085798248648643\n",
            "Epoch 16515: train loss: 0.018082864582538605\n",
            "Epoch 16516: train loss: 0.018079932779073715\n",
            "Epoch 16517: train loss: 0.018077000975608826\n",
            "Epoch 16518: train loss: 0.018074072897434235\n",
            "Epoch 16519: train loss: 0.018071144819259644\n",
            "Epoch 16520: train loss: 0.018068211153149605\n",
            "Epoch 16521: train loss: 0.01806529052555561\n",
            "Epoch 16522: train loss: 0.01806236058473587\n",
            "Epoch 16523: train loss: 0.01805943250656128\n",
            "Epoch 16524: train loss: 0.018056510016322136\n",
            "Epoch 16525: train loss: 0.018053587526082993\n",
            "Epoch 16526: train loss: 0.01805066131055355\n",
            "Epoch 16527: train loss: 0.018047740682959557\n",
            "Epoch 16528: train loss: 0.018044820055365562\n",
            "Epoch 16529: train loss: 0.01804189942777157\n",
            "Epoch 16530: train loss: 0.018038984388113022\n",
            "Epoch 16531: train loss: 0.018036067485809326\n",
            "Epoch 16532: train loss: 0.01803315058350563\n",
            "Epoch 16533: train loss: 0.018030241131782532\n",
            "Epoch 16534: train loss: 0.018027320504188538\n",
            "Epoch 16535: train loss: 0.01802440732717514\n",
            "Epoch 16536: train loss: 0.018021490424871445\n",
            "Epoch 16537: train loss: 0.018018575385212898\n",
            "Epoch 16538: train loss: 0.01801566779613495\n",
            "Epoch 16539: train loss: 0.01801275461912155\n",
            "Epoch 16540: train loss: 0.018009847030043602\n",
            "Epoch 16541: train loss: 0.01800694316625595\n",
            "Epoch 16542: train loss: 0.018004029989242554\n",
            "Epoch 16543: train loss: 0.018001124262809753\n",
            "Epoch 16544: train loss: 0.017998214811086655\n",
            "Epoch 16545: train loss: 0.017995305359363556\n",
            "Epoch 16546: train loss: 0.017992407083511353\n",
            "Epoch 16547: train loss: 0.017989501357078552\n",
            "Epoch 16548: train loss: 0.01798659935593605\n",
            "Epoch 16549: train loss: 0.0179836954921484\n",
            "Epoch 16550: train loss: 0.017980793491005898\n",
            "Epoch 16551: train loss: 0.017977897077798843\n",
            "Epoch 16552: train loss: 0.01797500066459179\n",
            "Epoch 16553: train loss: 0.017972102388739586\n",
            "Epoch 16554: train loss: 0.017969204112887383\n",
            "Epoch 16555: train loss: 0.017966313287615776\n",
            "Epoch 16556: train loss: 0.017963409423828125\n",
            "Epoch 16557: train loss: 0.01796051673591137\n",
            "Epoch 16558: train loss: 0.017957618460059166\n",
            "Epoch 16559: train loss: 0.01795472763478756\n",
            "Epoch 16560: train loss: 0.0179518423974514\n",
            "Epoch 16561: train loss: 0.017948945984244347\n",
            "Epoch 16562: train loss: 0.017946062609553337\n",
            "Epoch 16563: train loss: 0.017943166196346283\n",
            "Epoch 16564: train loss: 0.017940286546945572\n",
            "Epoch 16565: train loss: 0.017937393859028816\n",
            "Epoch 16566: train loss: 0.01793450117111206\n",
            "Epoch 16567: train loss: 0.01793161779642105\n",
            "Epoch 16568: train loss: 0.01792873628437519\n",
            "Epoch 16569: train loss: 0.01792585290968418\n",
            "Epoch 16570: train loss: 0.017922967672348022\n",
            "Epoch 16571: train loss: 0.01792008802294731\n",
            "Epoch 16572: train loss: 0.01791720651090145\n",
            "Epoch 16573: train loss: 0.017914332449436188\n",
            "Epoch 16574: train loss: 0.017911450937390327\n",
            "Epoch 16575: train loss: 0.017908571287989616\n",
            "Epoch 16576: train loss: 0.017905691638588905\n",
            "Epoch 16577: train loss: 0.01790281943976879\n",
            "Epoch 16578: train loss: 0.01789993979036808\n",
            "Epoch 16579: train loss: 0.017897069454193115\n",
            "Epoch 16580: train loss: 0.017894195392727852\n",
            "Epoch 16581: train loss: 0.01789131574332714\n",
            "Epoch 16582: train loss: 0.017888447269797325\n",
            "Epoch 16583: train loss: 0.01788557507097721\n",
            "Epoch 16584: train loss: 0.017882710322737694\n",
            "Epoch 16585: train loss: 0.01787983626127243\n",
            "Epoch 16586: train loss: 0.017876969650387764\n",
            "Epoch 16587: train loss: 0.01787409745156765\n",
            "Epoch 16588: train loss: 0.01787123829126358\n",
            "Epoch 16589: train loss: 0.017868366092443466\n",
            "Epoch 16590: train loss: 0.0178654994815588\n",
            "Epoch 16591: train loss: 0.017862629145383835\n",
            "Epoch 16592: train loss: 0.017859773710370064\n",
            "Epoch 16593: train loss: 0.017856910824775696\n",
            "Epoch 16594: train loss: 0.017854051664471626\n",
            "Epoch 16595: train loss: 0.01785118877887726\n",
            "Epoch 16596: train loss: 0.01784832775592804\n",
            "Epoch 16597: train loss: 0.01784547045826912\n",
            "Epoch 16598: train loss: 0.0178426131606102\n",
            "Epoch 16599: train loss: 0.01783975400030613\n",
            "Epoch 16600: train loss: 0.017836900427937508\n",
            "Epoch 16601: train loss: 0.017834046855568886\n",
            "Epoch 16602: train loss: 0.017831193283200264\n",
            "Epoch 16603: train loss: 0.017828335985541344\n",
            "Epoch 16604: train loss: 0.017825482413172722\n",
            "Epoch 16605: train loss: 0.017822636291384697\n",
            "Epoch 16606: train loss: 0.017819782719016075\n",
            "Epoch 16607: train loss: 0.01781693659722805\n",
            "Epoch 16608: train loss: 0.017814090475440025\n",
            "Epoch 16609: train loss: 0.017811235040426254\n",
            "Epoch 16610: train loss: 0.01780838891863823\n",
            "Epoch 16611: train loss: 0.017805540934205055\n",
            "Epoch 16612: train loss: 0.01780269294977188\n",
            "Epoch 16613: train loss: 0.017799846827983856\n",
            "Epoch 16614: train loss: 0.017797008156776428\n",
            "Epoch 16615: train loss: 0.017794158309698105\n",
            "Epoch 16616: train loss: 0.017791319638490677\n",
            "Epoch 16617: train loss: 0.017788484692573547\n",
            "Epoch 16618: train loss: 0.017785638570785522\n",
            "Epoch 16619: train loss: 0.017782798036932945\n",
            "Epoch 16620: train loss: 0.017779964953660965\n",
            "Epoch 16621: train loss: 0.017777124419808388\n",
            "Epoch 16622: train loss: 0.01777428761124611\n",
            "Epoch 16623: train loss: 0.017771458253264427\n",
            "Epoch 16624: train loss: 0.017768623307347298\n",
            "Epoch 16625: train loss: 0.01776578463613987\n",
            "Epoch 16626: train loss: 0.017762957140803337\n",
            "Epoch 16627: train loss: 0.01776011846959591\n",
            "Epoch 16628: train loss: 0.01775728538632393\n",
            "Epoch 16629: train loss: 0.017754459753632545\n",
            "Epoch 16630: train loss: 0.017751628533005714\n",
            "Epoch 16631: train loss: 0.017748793587088585\n",
            "Epoch 16632: train loss: 0.017745964229106903\n",
            "Epoch 16633: train loss: 0.01774313487112522\n",
            "Epoch 16634: train loss: 0.017740309238433838\n",
            "Epoch 16635: train loss: 0.017737481743097305\n",
            "Epoch 16636: train loss: 0.01773466169834137\n",
            "Epoch 16637: train loss: 0.017731837928295135\n",
            "Epoch 16638: train loss: 0.017729010432958603\n",
            "Epoch 16639: train loss: 0.017726188525557518\n",
            "Epoch 16640: train loss: 0.01772337034344673\n",
            "Epoch 16641: train loss: 0.017720555886626244\n",
            "Epoch 16642: train loss: 0.01771773397922516\n",
            "Epoch 16643: train loss: 0.017714913934469223\n",
            "Epoch 16644: train loss: 0.017712095752358437\n",
            "Epoch 16645: train loss: 0.017709283158183098\n",
            "Epoch 16646: train loss: 0.01770646683871746\n",
            "Epoch 16647: train loss: 0.017703654244542122\n",
            "Epoch 16648: train loss: 0.01770084537565708\n",
            "Epoch 16649: train loss: 0.017698029056191444\n",
            "Epoch 16650: train loss: 0.01769520901143551\n",
            "Epoch 16651: train loss: 0.01769239827990532\n",
            "Epoch 16652: train loss: 0.01768958382308483\n",
            "Epoch 16653: train loss: 0.01768677681684494\n",
            "Epoch 16654: train loss: 0.0176839679479599\n",
            "Epoch 16655: train loss: 0.017681153491139412\n",
            "Epoch 16656: train loss: 0.01767835021018982\n",
            "Epoch 16657: train loss: 0.01767554320394993\n",
            "Epoch 16658: train loss: 0.017672736197710037\n",
            "Epoch 16659: train loss: 0.017669931054115295\n",
            "Epoch 16660: train loss: 0.017667127773165703\n",
            "Epoch 16661: train loss: 0.01766432635486126\n",
            "Epoch 16662: train loss: 0.017661523073911667\n",
            "Epoch 16663: train loss: 0.01765872724354267\n",
            "Epoch 16664: train loss: 0.017655929550528526\n",
            "Epoch 16665: train loss: 0.017653124406933784\n",
            "Epoch 16666: train loss: 0.017650330439209938\n",
            "Epoch 16667: train loss: 0.017647529020905495\n",
            "Epoch 16668: train loss: 0.017644742503762245\n",
            "Epoch 16669: train loss: 0.01764194294810295\n",
            "Epoch 16670: train loss: 0.017639141529798508\n",
            "Epoch 16671: train loss: 0.017636355012655258\n",
            "Epoch 16672: train loss: 0.017633555456995964\n",
            "Epoch 16673: train loss: 0.01763075962662697\n",
            "Epoch 16674: train loss: 0.01762796752154827\n",
            "Epoch 16675: train loss: 0.017625177279114723\n",
            "Epoch 16676: train loss: 0.017622381448745728\n",
            "Epoch 16677: train loss: 0.017619596794247627\n",
            "Epoch 16678: train loss: 0.01761680468916893\n",
            "Epoch 16679: train loss: 0.01761402003467083\n",
            "Epoch 16680: train loss: 0.017611229792237282\n",
            "Epoch 16681: train loss: 0.017608454450964928\n",
            "Epoch 16682: train loss: 0.017605656757950783\n",
            "Epoch 16683: train loss: 0.01760287955403328\n",
            "Epoch 16684: train loss: 0.01760009303689003\n",
            "Epoch 16685: train loss: 0.017597315832972527\n",
            "Epoch 16686: train loss: 0.017594529315829277\n",
            "Epoch 16687: train loss: 0.017591752111911774\n",
            "Epoch 16688: train loss: 0.017588971182703972\n",
            "Epoch 16689: train loss: 0.017586195841431618\n",
            "Epoch 16690: train loss: 0.017583411186933517\n",
            "Epoch 16691: train loss: 0.017580639570951462\n",
            "Epoch 16692: train loss: 0.01757785491645336\n",
            "Epoch 16693: train loss: 0.017575087025761604\n",
            "Epoch 16694: train loss: 0.017572304233908653\n",
            "Epoch 16695: train loss: 0.01756952702999115\n",
            "Epoch 16696: train loss: 0.017566749826073647\n",
            "Epoch 16697: train loss: 0.01756398007273674\n",
            "Epoch 16698: train loss: 0.017561208456754684\n",
            "Epoch 16699: train loss: 0.017558438703417778\n",
            "Epoch 16700: train loss: 0.01755566895008087\n",
            "Epoch 16701: train loss: 0.017552906647324562\n",
            "Epoch 16702: train loss: 0.017550140619277954\n",
            "Epoch 16703: train loss: 0.017547370865941048\n",
            "Epoch 16704: train loss: 0.01754460670053959\n",
            "Epoch 16705: train loss: 0.01754184626042843\n",
            "Epoch 16706: train loss: 0.01753907836973667\n",
            "Epoch 16707: train loss: 0.01753631792962551\n",
            "Epoch 16708: train loss: 0.017533551901578903\n",
            "Epoch 16709: train loss: 0.017530791461467743\n",
            "Epoch 16710: train loss: 0.017528031021356583\n",
            "Epoch 16711: train loss: 0.017525263130664825\n",
            "Epoch 16712: train loss: 0.017522506415843964\n",
            "Epoch 16713: train loss: 0.017519745975732803\n",
            "Epoch 16714: train loss: 0.01751699112355709\n",
            "Epoch 16715: train loss: 0.01751422882080078\n",
            "Epoch 16716: train loss: 0.017511466518044472\n",
            "Epoch 16717: train loss: 0.017508715391159058\n",
            "Epoch 16718: train loss: 0.017505956813693047\n",
            "Epoch 16719: train loss: 0.017503198236227036\n",
            "Epoch 16720: train loss: 0.017500443384051323\n",
            "Epoch 16721: train loss: 0.017497694119811058\n",
            "Epoch 16722: train loss: 0.017494941130280495\n",
            "Epoch 16723: train loss: 0.017492195591330528\n",
            "Epoch 16724: train loss: 0.01748945191502571\n",
            "Epoch 16725: train loss: 0.017486700788140297\n",
            "Epoch 16726: train loss: 0.01748395338654518\n",
            "Epoch 16727: train loss: 0.017481211572885513\n",
            "Epoch 16728: train loss: 0.017478466033935547\n",
            "Epoch 16729: train loss: 0.01747572422027588\n",
            "Epoch 16730: train loss: 0.017472976818680763\n",
            "Epoch 16731: train loss: 0.017470233142375946\n",
            "Epoch 16732: train loss: 0.017467491328716278\n",
            "Epoch 16733: train loss: 0.017464742064476013\n",
            "Epoch 16734: train loss: 0.017462003976106644\n",
            "Epoch 16735: train loss: 0.017459256574511528\n",
            "Epoch 16736: train loss: 0.017456520348787308\n",
            "Epoch 16737: train loss: 0.01745377853512764\n",
            "Epoch 16738: train loss: 0.01745103858411312\n",
            "Epoch 16739: train loss: 0.017448296770453453\n",
            "Epoch 16740: train loss: 0.017445558682084084\n",
            "Epoch 16741: train loss: 0.017442822456359863\n",
            "Epoch 16742: train loss: 0.01744009181857109\n",
            "Epoch 16743: train loss: 0.01743735745549202\n",
            "Epoch 16744: train loss: 0.017434626817703247\n",
            "Epoch 16745: train loss: 0.017431896179914474\n",
            "Epoch 16746: train loss: 0.0174291729927063\n",
            "Epoch 16747: train loss: 0.01742643490433693\n",
            "Epoch 16748: train loss: 0.017423711717128754\n",
            "Epoch 16749: train loss: 0.01742098294198513\n",
            "Epoch 16750: train loss: 0.017418259754776955\n",
            "Epoch 16751: train loss: 0.017415525391697884\n",
            "Epoch 16752: train loss: 0.017412804067134857\n",
            "Epoch 16753: train loss: 0.017410077154636383\n",
            "Epoch 16754: train loss: 0.01740735024213791\n",
            "Epoch 16755: train loss: 0.017404623329639435\n",
            "Epoch 16756: train loss: 0.017401905730366707\n",
            "Epoch 16757: train loss: 0.017399180680513382\n",
            "Epoch 16758: train loss: 0.017396453768014908\n",
            "Epoch 16759: train loss: 0.017393728718161583\n",
            "Epoch 16760: train loss: 0.017391009256243706\n",
            "Epoch 16761: train loss: 0.01738828234374523\n",
            "Epoch 16762: train loss: 0.017385568469762802\n",
            "Epoch 16763: train loss: 0.017382854595780373\n",
            "Epoch 16764: train loss: 0.017380138859152794\n",
            "Epoch 16765: train loss: 0.017377426847815514\n",
            "Epoch 16766: train loss: 0.017374711111187935\n",
            "Epoch 16767: train loss: 0.017371993511915207\n",
            "Epoch 16768: train loss: 0.017369288951158524\n",
            "Epoch 16769: train loss: 0.017366580665111542\n",
            "Epoch 16770: train loss: 0.017363866791129112\n",
            "Epoch 16771: train loss: 0.01736115664243698\n",
            "Epoch 16772: train loss: 0.017358452081680298\n",
            "Epoch 16773: train loss: 0.017355740070343018\n",
            "Epoch 16774: train loss: 0.017353029921650887\n",
            "Epoch 16775: train loss: 0.017350321635603905\n",
            "Epoch 16776: train loss: 0.017347611486911774\n",
            "Epoch 16777: train loss: 0.01734490878880024\n",
            "Epoch 16778: train loss: 0.01734219864010811\n",
            "Epoch 16779: train loss: 0.017339499667286873\n",
            "Epoch 16780: train loss: 0.017336789518594742\n",
            "Epoch 16781: train loss: 0.01733408495783806\n",
            "Epoch 16782: train loss: 0.017331374809145927\n",
            "Epoch 16783: train loss: 0.017328675836324692\n",
            "Epoch 16784: train loss: 0.017325975000858307\n",
            "Epoch 16785: train loss: 0.01732328161597252\n",
            "Epoch 16786: train loss: 0.017320582643151283\n",
            "Epoch 16787: train loss: 0.017317892983555794\n",
            "Epoch 16788: train loss: 0.01731519214808941\n",
            "Epoch 16789: train loss: 0.01731249690055847\n",
            "Epoch 16790: train loss: 0.017309803515672684\n",
            "Epoch 16791: train loss: 0.017307111993432045\n",
            "Epoch 16792: train loss: 0.017304422333836555\n",
            "Epoch 16793: train loss: 0.017301728948950768\n",
            "Epoch 16794: train loss: 0.01729903556406498\n",
            "Epoch 16795: train loss: 0.01729634590446949\n",
            "Epoch 16796: train loss: 0.017293652519583702\n",
            "Epoch 16797: train loss: 0.017290959134697914\n",
            "Epoch 16798: train loss: 0.017288269475102425\n",
            "Epoch 16799: train loss: 0.017285583540797234\n",
            "Epoch 16800: train loss: 0.017282895743846893\n",
            "Epoch 16801: train loss: 0.017280206084251404\n",
            "Epoch 16802: train loss: 0.017277516424655914\n",
            "Epoch 16803: train loss: 0.017274830490350723\n",
            "Epoch 16804: train loss: 0.017272142693400383\n",
            "Epoch 16805: train loss: 0.01726946048438549\n",
            "Epoch 16806: train loss: 0.01726677082479\n",
            "Epoch 16807: train loss: 0.01726408861577511\n",
            "Epoch 16808: train loss: 0.017261413857340813\n",
            "Epoch 16809: train loss: 0.01725873537361622\n",
            "Epoch 16810: train loss: 0.017256056889891624\n",
            "Epoch 16811: train loss: 0.017253383994102478\n",
            "Epoch 16812: train loss: 0.01725071109831333\n",
            "Epoch 16813: train loss: 0.01724802702665329\n",
            "Epoch 16814: train loss: 0.017245355993509293\n",
            "Epoch 16815: train loss: 0.017242679372429848\n",
            "Epoch 16816: train loss: 0.017240010201931\n",
            "Epoch 16817: train loss: 0.017237333580851555\n",
            "Epoch 16818: train loss: 0.017234664410352707\n",
            "Epoch 16819: train loss: 0.01723198965191841\n",
            "Epoch 16820: train loss: 0.017229316756129265\n",
            "Epoch 16821: train loss: 0.01722664199769497\n",
            "Epoch 16822: train loss: 0.01722397468984127\n",
            "Epoch 16823: train loss: 0.017221299931406975\n",
            "Epoch 16824: train loss: 0.017218634486198425\n",
            "Epoch 16825: train loss: 0.017215965315699577\n",
            "Epoch 16826: train loss: 0.01721329614520073\n",
            "Epoch 16827: train loss: 0.017210625112056732\n",
            "Epoch 16828: train loss: 0.01720796525478363\n",
            "Epoch 16829: train loss: 0.01720529980957508\n",
            "Epoch 16830: train loss: 0.01720263808965683\n",
            "Epoch 16831: train loss: 0.01719997450709343\n",
            "Epoch 16832: train loss: 0.017197318375110626\n",
            "Epoch 16833: train loss: 0.017194658517837524\n",
            "Epoch 16834: train loss: 0.017192000523209572\n",
            "Epoch 16835: train loss: 0.01718933694064617\n",
            "Epoch 16836: train loss: 0.017186682671308517\n",
            "Epoch 16837: train loss: 0.017184028401970863\n",
            "Epoch 16838: train loss: 0.01718136854469776\n",
            "Epoch 16839: train loss: 0.017178712412714958\n",
            "Epoch 16840: train loss: 0.017176056280732155\n",
            "Epoch 16841: train loss: 0.0171734020113945\n",
            "Epoch 16842: train loss: 0.017170744016766548\n",
            "Epoch 16843: train loss: 0.017168093472719193\n",
            "Epoch 16844: train loss: 0.01716543547809124\n",
            "Epoch 16845: train loss: 0.017162781208753586\n",
            "Epoch 16846: train loss: 0.01716012880206108\n",
            "Epoch 16847: train loss: 0.017157476395368576\n",
            "Epoch 16848: train loss: 0.017154831439256668\n",
            "Epoch 16849: train loss: 0.01715218648314476\n",
            "Epoch 16850: train loss: 0.017149539664387703\n",
            "Epoch 16851: train loss: 0.017146889120340347\n",
            "Epoch 16852: train loss: 0.017144249752163887\n",
            "Epoch 16853: train loss: 0.01714160293340683\n",
            "Epoch 16854: train loss: 0.01713896542787552\n",
            "Epoch 16855: train loss: 0.01713632419705391\n",
            "Epoch 16856: train loss: 0.01713368110358715\n",
            "Epoch 16857: train loss: 0.017131036147475243\n",
            "Epoch 16858: train loss: 0.017128394916653633\n",
            "Epoch 16859: train loss: 0.017125757411122322\n",
            "Epoch 16860: train loss: 0.017123116180300713\n",
            "Epoch 16861: train loss: 0.0171204786747694\n",
            "Epoch 16862: train loss: 0.017117837443947792\n",
            "Epoch 16863: train loss: 0.017115198075771332\n",
            "Epoch 16864: train loss: 0.01711256429553032\n",
            "Epoch 16865: train loss: 0.017109930515289307\n",
            "Epoch 16866: train loss: 0.017107291147112846\n",
            "Epoch 16867: train loss: 0.017104653641581535\n",
            "Epoch 16868: train loss: 0.017102019861340523\n",
            "Epoch 16869: train loss: 0.01709938608109951\n",
            "Epoch 16870: train loss: 0.017096757888793945\n",
            "Epoch 16871: train loss: 0.017094124108552933\n",
            "Epoch 16872: train loss: 0.01709149405360222\n",
            "Epoch 16873: train loss: 0.017088865861296654\n",
            "Epoch 16874: train loss: 0.017086239531636238\n",
            "Epoch 16875: train loss: 0.017083613201975822\n",
            "Epoch 16876: train loss: 0.01708097942173481\n",
            "Epoch 16877: train loss: 0.01707836054265499\n",
            "Epoch 16878: train loss: 0.017075734212994576\n",
            "Epoch 16879: train loss: 0.017073115333914757\n",
            "Epoch 16880: train loss: 0.017070487141609192\n",
            "Epoch 16881: train loss: 0.017067862674593925\n",
            "Epoch 16882: train loss: 0.017065243795514107\n",
            "Epoch 16883: train loss: 0.01706262119114399\n",
            "Epoch 16884: train loss: 0.01706000231206417\n",
            "Epoch 16885: train loss: 0.017057374119758606\n",
            "Epoch 16886: train loss: 0.017054760828614235\n",
            "Epoch 16887: train loss: 0.01705213636159897\n",
            "Epoch 16888: train loss: 0.0170495193451643\n",
            "Epoch 16889: train loss: 0.01704690232872963\n",
            "Epoch 16890: train loss: 0.01704428158700466\n",
            "Epoch 16891: train loss: 0.01704166643321514\n",
            "Epoch 16892: train loss: 0.01703905686736107\n",
            "Epoch 16893: train loss: 0.0170364398509264\n",
            "Epoch 16894: train loss: 0.017033830285072327\n",
            "Epoch 16895: train loss: 0.017031216993927956\n",
            "Epoch 16896: train loss: 0.017028598114848137\n",
            "Epoch 16897: train loss: 0.01702599599957466\n",
            "Epoch 16898: train loss: 0.017023378983139992\n",
            "Epoch 16899: train loss: 0.017020778730511665\n",
            "Epoch 16900: train loss: 0.017018167302012444\n",
            "Epoch 16901: train loss: 0.01701556146144867\n",
            "Epoch 16902: train loss: 0.017012957483530045\n",
            "Epoch 16903: train loss: 0.01701035164296627\n",
            "Epoch 16904: train loss: 0.0170077383518219\n",
            "Epoch 16905: train loss: 0.017005139961838722\n",
            "Epoch 16906: train loss: 0.017002537846565247\n",
            "Epoch 16907: train loss: 0.016999932006001472\n",
            "Epoch 16908: train loss: 0.016997333616018295\n",
            "Epoch 16909: train loss: 0.01699472777545452\n",
            "Epoch 16910: train loss: 0.016992123797535896\n",
            "Epoch 16911: train loss: 0.01698952354490757\n",
            "Epoch 16912: train loss: 0.016986923292279243\n",
            "Epoch 16913: train loss: 0.016984326764941216\n",
            "Epoch 16914: train loss: 0.016981733962893486\n",
            "Epoch 16915: train loss: 0.016979139298200607\n",
            "Epoch 16916: train loss: 0.01697653718292713\n",
            "Epoch 16917: train loss: 0.016973942518234253\n",
            "Epoch 16918: train loss: 0.016971349716186523\n",
            "Epoch 16919: train loss: 0.016968756914138794\n",
            "Epoch 16920: train loss: 0.016966164112091064\n",
            "Epoch 16921: train loss: 0.016963575035333633\n",
            "Epoch 16922: train loss: 0.016960978507995605\n",
            "Epoch 16923: train loss: 0.016958393156528473\n",
            "Epoch 16924: train loss: 0.016955802217125893\n",
            "Epoch 16925: train loss: 0.01695321314036846\n",
            "Epoch 16926: train loss: 0.01695062592625618\n",
            "Epoch 16927: train loss: 0.0169480349868536\n",
            "Epoch 16928: train loss: 0.016945451498031616\n",
            "Epoch 16929: train loss: 0.016942862421274185\n",
            "Epoch 16930: train loss: 0.016940275207161903\n",
            "Epoch 16931: train loss: 0.016937686130404472\n",
            "Epoch 16932: train loss: 0.016935104504227638\n",
            "Epoch 16933: train loss: 0.016932522878050804\n",
            "Epoch 16934: train loss: 0.016929931938648224\n",
            "Epoch 16935: train loss: 0.016927359625697136\n",
            "Epoch 16936: train loss: 0.016924774274230003\n",
            "Epoch 16937: train loss: 0.016922200098633766\n",
            "Epoch 16938: train loss: 0.01691962033510208\n",
            "Epoch 16939: train loss: 0.016917046159505844\n",
            "Epoch 16940: train loss: 0.01691446639597416\n",
            "Epoch 16941: train loss: 0.016911890357732773\n",
            "Epoch 16942: train loss: 0.016909310594201088\n",
            "Epoch 16943: train loss: 0.016906730830669403\n",
            "Epoch 16944: train loss: 0.016904164105653763\n",
            "Epoch 16945: train loss: 0.016901593655347824\n",
            "Epoch 16946: train loss: 0.016899017617106438\n",
            "Epoch 16947: train loss: 0.01689644530415535\n",
            "Epoch 16948: train loss: 0.016893871128559113\n",
            "Epoch 16949: train loss: 0.016891302540898323\n",
            "Epoch 16950: train loss: 0.016888732090592384\n",
            "Epoch 16951: train loss: 0.016886161640286446\n",
            "Epoch 16952: train loss: 0.016883593052625656\n",
            "Epoch 16953: train loss: 0.016881028190255165\n",
            "Epoch 16954: train loss: 0.01687845028936863\n",
            "Epoch 16955: train loss: 0.01687588356435299\n",
            "Epoch 16956: train loss: 0.01687331683933735\n",
            "Epoch 16957: train loss: 0.016870753839612007\n",
            "Epoch 16958: train loss: 0.016868188977241516\n",
            "Epoch 16959: train loss: 0.016865625977516174\n",
            "Epoch 16960: train loss: 0.01686306670308113\n",
            "Epoch 16961: train loss: 0.01686050370335579\n",
            "Epoch 16962: train loss: 0.016857944428920746\n",
            "Epoch 16963: train loss: 0.016855387017130852\n",
            "Epoch 16964: train loss: 0.01685282401740551\n",
            "Epoch 16965: train loss: 0.016850261017680168\n",
            "Epoch 16966: train loss: 0.01684771291911602\n",
            "Epoch 16967: train loss: 0.016845151782035828\n",
            "Epoch 16968: train loss: 0.016842598095536232\n",
            "Epoch 16969: train loss: 0.016840046271681786\n",
            "Epoch 16970: train loss: 0.01683748885989189\n",
            "Epoch 16971: train loss: 0.016834940761327744\n",
            "Epoch 16972: train loss: 0.01683237962424755\n",
            "Epoch 16973: train loss: 0.016829827800393105\n",
            "Epoch 16974: train loss: 0.01682727038860321\n",
            "Epoch 16975: train loss: 0.016824718564748764\n",
            "Epoch 16976: train loss: 0.016822170466184616\n",
            "Epoch 16977: train loss: 0.01681961677968502\n",
            "Epoch 16978: train loss: 0.01681707054376602\n",
            "Epoch 16979: train loss: 0.016814524307847023\n",
            "Epoch 16980: train loss: 0.016811972483992577\n",
            "Epoch 16981: train loss: 0.016809428110718727\n",
            "Epoch 16982: train loss: 0.01680688001215458\n",
            "Epoch 16983: train loss: 0.016804339364171028\n",
            "Epoch 16984: train loss: 0.01680178940296173\n",
            "Epoch 16985: train loss: 0.01679924689233303\n",
            "Epoch 16986: train loss: 0.016796700656414032\n",
            "Epoch 16987: train loss: 0.01679416000843048\n",
            "Epoch 16988: train loss: 0.01679161749780178\n",
            "Epoch 16989: train loss: 0.01678907312452793\n",
            "Epoch 16990: train loss: 0.01678653433918953\n",
            "Epoch 16991: train loss: 0.01678399369120598\n",
            "Epoch 16992: train loss: 0.016781454905867577\n",
            "Epoch 16993: train loss: 0.01677892357110977\n",
            "Epoch 16994: train loss: 0.01677638851106167\n",
            "Epoch 16995: train loss: 0.01677384413778782\n",
            "Epoch 16996: train loss: 0.016771310940384865\n",
            "Epoch 16997: train loss: 0.01676877774298191\n",
            "Epoch 16998: train loss: 0.01676623895764351\n",
            "Epoch 16999: train loss: 0.016763707622885704\n",
            "Epoch 17000: train loss: 0.0167611725628376\n",
            "Epoch 17001: train loss: 0.016758644953370094\n",
            "Epoch 17002: train loss: 0.01675611361861229\n",
            "Epoch 17003: train loss: 0.016753582283854485\n",
            "Epoch 17004: train loss: 0.016751054674386978\n",
            "Epoch 17005: train loss: 0.01674852892756462\n",
            "Epoch 17006: train loss: 0.01674599200487137\n",
            "Epoch 17007: train loss: 0.01674347184598446\n",
            "Epoch 17008: train loss: 0.016740942373871803\n",
            "Epoch 17009: train loss: 0.016738414764404297\n",
            "Epoch 17010: train loss: 0.01673589088022709\n",
            "Epoch 17011: train loss: 0.016733365133404732\n",
            "Epoch 17012: train loss: 0.016730839386582375\n",
            "Epoch 17013: train loss: 0.016728315502405167\n",
            "Epoch 17014: train loss: 0.016725795343518257\n",
            "Epoch 17015: train loss: 0.01672326773405075\n",
            "Epoch 17016: train loss: 0.01672075130045414\n",
            "Epoch 17017: train loss: 0.016718221828341484\n",
            "Epoch 17018: train loss: 0.01671570912003517\n",
            "Epoch 17019: train loss: 0.016713187098503113\n",
            "Epoch 17020: train loss: 0.016710670664906502\n",
            "Epoch 17021: train loss: 0.016708150506019592\n",
            "Epoch 17022: train loss: 0.01670563407242298\n",
            "Epoch 17023: train loss: 0.01670312136411667\n",
            "Epoch 17024: train loss: 0.01670060306787491\n",
            "Epoch 17025: train loss: 0.016698095947504044\n",
            "Epoch 17026: train loss: 0.016695581376552582\n",
            "Epoch 17027: train loss: 0.016693072393536568\n",
            "Epoch 17028: train loss: 0.016690559685230255\n",
            "Epoch 17029: train loss: 0.016688046976923943\n",
            "Epoch 17030: train loss: 0.01668553426861763\n",
            "Epoch 17031: train loss: 0.016683023422956467\n",
            "Epoch 17032: train loss: 0.016680516302585602\n",
            "Epoch 17033: train loss: 0.01667800545692444\n",
            "Epoch 17034: train loss: 0.016675502061843872\n",
            "Epoch 17035: train loss: 0.01667298562824726\n",
            "Epoch 17036: train loss: 0.016670482233166695\n",
            "Epoch 17037: train loss: 0.016667978838086128\n",
            "Epoch 17038: train loss: 0.016665473580360413\n",
            "Epoch 17039: train loss: 0.016662966459989548\n",
            "Epoch 17040: train loss: 0.01666046306490898\n",
            "Epoch 17041: train loss: 0.016657955944538116\n",
            "Epoch 17042: train loss: 0.016655458137392998\n",
            "Epoch 17043: train loss: 0.016652951017022133\n",
            "Epoch 17044: train loss: 0.016650453209877014\n",
            "Epoch 17045: train loss: 0.016647953540086746\n",
            "Epoch 17046: train loss: 0.01664545200765133\n",
            "Epoch 17047: train loss: 0.016642961651086807\n",
            "Epoch 17048: train loss: 0.01664046384394169\n",
            "Epoch 17049: train loss: 0.01663796976208687\n",
            "Epoch 17050: train loss: 0.0166354700922966\n",
            "Epoch 17051: train loss: 0.01663297414779663\n",
            "Epoch 17052: train loss: 0.016630487516522408\n",
            "Epoch 17053: train loss: 0.01662798970937729\n",
            "Epoch 17054: train loss: 0.016625499352812767\n",
            "Epoch 17055: train loss: 0.01662300154566765\n",
            "Epoch 17056: train loss: 0.016620509326457977\n",
            "Epoch 17057: train loss: 0.016618013381958008\n",
            "Epoch 17058: train loss: 0.016615521162748337\n",
            "Epoch 17059: train loss: 0.016613036394119263\n",
            "Epoch 17060: train loss: 0.01661054417490959\n",
            "Epoch 17061: train loss: 0.01660805754363537\n",
            "Epoch 17062: train loss: 0.016605569049715996\n",
            "Epoch 17063: train loss: 0.01660308614373207\n",
            "Epoch 17064: train loss: 0.0166005976498127\n",
            "Epoch 17065: train loss: 0.016598111018538475\n",
            "Epoch 17066: train loss: 0.01659562438726425\n",
            "Epoch 17067: train loss: 0.01659313775599003\n",
            "Epoch 17068: train loss: 0.016590654850006104\n",
            "Epoch 17069: train loss: 0.01658817008137703\n",
            "Epoch 17070: train loss: 0.01658570021390915\n",
            "Epoch 17071: train loss: 0.016583213582634926\n",
            "Epoch 17072: train loss: 0.016580738127231598\n",
            "Epoch 17073: train loss: 0.016578257083892822\n",
            "Epoch 17074: train loss: 0.016575779765844345\n",
            "Epoch 17075: train loss: 0.016573304310441017\n",
            "Epoch 17076: train loss: 0.01657082699239254\n",
            "Epoch 17077: train loss: 0.016568349674344063\n",
            "Epoch 17078: train loss: 0.016565874218940735\n",
            "Epoch 17079: train loss: 0.016563396900892258\n",
            "Epoch 17080: train loss: 0.01656092144548893\n",
            "Epoch 17081: train loss: 0.01655844785273075\n",
            "Epoch 17082: train loss: 0.016555964946746826\n",
            "Epoch 17083: train loss: 0.016553495079278946\n",
            "Epoch 17084: train loss: 0.016551021486520767\n",
            "Epoch 17085: train loss: 0.016548549756407738\n",
            "Epoch 17086: train loss: 0.016546078026294708\n",
            "Epoch 17087: train loss: 0.016543613746762276\n",
            "Epoch 17088: train loss: 0.016541143879294395\n",
            "Epoch 17089: train loss: 0.016538677737116814\n",
            "Epoch 17090: train loss: 0.016536209732294083\n",
            "Epoch 17091: train loss: 0.016533739864826202\n",
            "Epoch 17092: train loss: 0.01653127558529377\n",
            "Epoch 17093: train loss: 0.016528811305761337\n",
            "Epoch 17094: train loss: 0.016526347026228905\n",
            "Epoch 17095: train loss: 0.01652388833463192\n",
            "Epoch 17096: train loss: 0.016521424055099487\n",
            "Epoch 17097: train loss: 0.016518959775567055\n",
            "Epoch 17098: train loss: 0.01651649735867977\n",
            "Epoch 17099: train loss: 0.016514042392373085\n",
            "Epoch 17100: train loss: 0.016511578112840652\n",
            "Epoch 17101: train loss: 0.016509123146533966\n",
            "Epoch 17102: train loss: 0.016506658867001534\n",
            "Epoch 17103: train loss: 0.01650420017540455\n",
            "Epoch 17104: train loss: 0.016501743346452713\n",
            "Epoch 17105: train loss: 0.016499288380146027\n",
            "Epoch 17106: train loss: 0.016496829688549042\n",
            "Epoch 17107: train loss: 0.016494370996952057\n",
            "Epoch 17108: train loss: 0.016491912305355072\n",
            "Epoch 17109: train loss: 0.016489461064338684\n",
            "Epoch 17110: train loss: 0.016487007960677147\n",
            "Epoch 17111: train loss: 0.01648455671966076\n",
            "Epoch 17112: train loss: 0.016482099890708923\n",
            "Epoch 17113: train loss: 0.016479648649692535\n",
            "Epoch 17114: train loss: 0.016477202996611595\n",
            "Epoch 17115: train loss: 0.016474751755595207\n",
            "Epoch 17116: train loss: 0.01647229865193367\n",
            "Epoch 17117: train loss: 0.016469858586788177\n",
            "Epoch 17118: train loss: 0.01646740920841694\n",
            "Epoch 17119: train loss: 0.016464963555336\n",
            "Epoch 17120: train loss: 0.016462521627545357\n",
            "Epoch 17121: train loss: 0.016460077837109566\n",
            "Epoch 17122: train loss: 0.016457628458738327\n",
            "Epoch 17123: train loss: 0.016455186530947685\n",
            "Epoch 17124: train loss: 0.016452739015221596\n",
            "Epoch 17125: train loss: 0.016450293362140656\n",
            "Epoch 17126: train loss: 0.016447853296995163\n",
            "Epoch 17127: train loss: 0.01644541323184967\n",
            "Epoch 17128: train loss: 0.01644297130405903\n",
            "Epoch 17129: train loss: 0.01644052378833294\n",
            "Epoch 17130: train loss: 0.016438091173768044\n",
            "Epoch 17131: train loss: 0.0164356492459774\n",
            "Epoch 17132: train loss: 0.01643320545554161\n",
            "Epoch 17133: train loss: 0.016430765390396118\n",
            "Epoch 17134: train loss: 0.016428330913186073\n",
            "Epoch 17135: train loss: 0.016425900161266327\n",
            "Epoch 17136: train loss: 0.016423465684056282\n",
            "Epoch 17137: train loss: 0.01642102748155594\n",
            "Epoch 17138: train loss: 0.016418596729636192\n",
            "Epoch 17139: train loss: 0.016416167840361595\n",
            "Epoch 17140: train loss: 0.0164137352257967\n",
            "Epoch 17141: train loss: 0.01641131192445755\n",
            "Epoch 17142: train loss: 0.016408875584602356\n",
            "Epoch 17143: train loss: 0.016406454145908356\n",
            "Epoch 17144: train loss: 0.01640402339398861\n",
            "Epoch 17145: train loss: 0.016401588916778564\n",
            "Epoch 17146: train loss: 0.016399163752794266\n",
            "Epoch 17147: train loss: 0.01639673300087452\n",
            "Epoch 17148: train loss: 0.01639430783689022\n",
            "Epoch 17149: train loss: 0.016391875222325325\n",
            "Epoch 17150: train loss: 0.016389451920986176\n",
            "Epoch 17151: train loss: 0.01638702303171158\n",
            "Epoch 17152: train loss: 0.016384603455662727\n",
            "Epoch 17153: train loss: 0.01638217084109783\n",
            "Epoch 17154: train loss: 0.01637975499033928\n",
            "Epoch 17155: train loss: 0.01637732796370983\n",
            "Epoch 17156: train loss: 0.016374904662370682\n",
            "Epoch 17157: train loss: 0.016372479498386383\n",
            "Epoch 17158: train loss: 0.01637006737291813\n",
            "Epoch 17159: train loss: 0.016367649659514427\n",
            "Epoch 17160: train loss: 0.016365231946110725\n",
            "Epoch 17161: train loss: 0.016362810507416725\n",
            "Epoch 17162: train loss: 0.016360394656658173\n",
            "Epoch 17163: train loss: 0.016357986256480217\n",
            "Epoch 17164: train loss: 0.016355566680431366\n",
            "Epoch 17165: train loss: 0.016353150829672813\n",
            "Epoch 17166: train loss: 0.016350742429494858\n",
            "Epoch 17167: train loss: 0.016348332166671753\n",
            "Epoch 17168: train loss: 0.01634591817855835\n",
            "Epoch 17169: train loss: 0.016343504190444946\n",
            "Epoch 17170: train loss: 0.01634109765291214\n",
            "Epoch 17171: train loss: 0.016338683664798737\n",
            "Epoch 17172: train loss: 0.016336271539330482\n",
            "Epoch 17173: train loss: 0.016333868727087975\n",
            "Epoch 17174: train loss: 0.01633145473897457\n",
            "Epoch 17175: train loss: 0.016329042613506317\n",
            "Epoch 17176: train loss: 0.01632663421332836\n",
            "Epoch 17177: train loss: 0.016324225813150406\n",
            "Epoch 17178: train loss: 0.0163218155503273\n",
            "Epoch 17179: train loss: 0.01631939969956875\n",
            "Epoch 17180: train loss: 0.01631699502468109\n",
            "Epoch 17181: train loss: 0.01631459966301918\n",
            "Epoch 17182: train loss: 0.016312193125486374\n",
            "Epoch 17183: train loss: 0.016309794038534164\n",
            "Epoch 17184: train loss: 0.016307387501001358\n",
            "Epoch 17185: train loss: 0.016304990276694298\n",
            "Epoch 17186: train loss: 0.01630259118974209\n",
            "Epoch 17187: train loss: 0.016300197690725327\n",
            "Epoch 17188: train loss: 0.016297798603773117\n",
            "Epoch 17189: train loss: 0.01629539765417576\n",
            "Epoch 17190: train loss: 0.016293006017804146\n",
            "Epoch 17191: train loss: 0.016290610656142235\n",
            "Epoch 17192: train loss: 0.016288217157125473\n",
            "Epoch 17193: train loss: 0.016285819932818413\n",
            "Epoch 17194: train loss: 0.0162834282964468\n",
            "Epoch 17195: train loss: 0.01628103107213974\n",
            "Epoch 17196: train loss: 0.01627863384783268\n",
            "Epoch 17197: train loss: 0.016276242211461067\n",
            "Epoch 17198: train loss: 0.016273848712444305\n",
            "Epoch 17199: train loss: 0.016271455213427544\n",
            "Epoch 17200: train loss: 0.01626906357705593\n",
            "Epoch 17201: train loss: 0.01626667194068432\n",
            "Epoch 17202: train loss: 0.016264276579022408\n",
            "Epoch 17203: train loss: 0.016261888667941093\n",
            "Epoch 17204: train loss: 0.01625949703156948\n",
            "Epoch 17205: train loss: 0.01625710539519787\n",
            "Epoch 17206: train loss: 0.016254723072052002\n",
            "Epoch 17207: train loss: 0.016252338886260986\n",
            "Epoch 17208: train loss: 0.016249950975179672\n",
            "Epoch 17209: train loss: 0.016247564926743507\n",
            "Epoch 17210: train loss: 0.016245180740952492\n",
            "Epoch 17211: train loss: 0.01624280773103237\n",
            "Epoch 17212: train loss: 0.016240419819951057\n",
            "Epoch 17213: train loss: 0.01623803935945034\n",
            "Epoch 17214: train loss: 0.016235657036304474\n",
            "Epoch 17215: train loss: 0.016233274713158607\n",
            "Epoch 17216: train loss: 0.01623089611530304\n",
            "Epoch 17217: train loss: 0.01622852124273777\n",
            "Epoch 17218: train loss: 0.016226138919591904\n",
            "Epoch 17219: train loss: 0.016223762184381485\n",
            "Epoch 17220: train loss: 0.016221383586525917\n",
            "Epoch 17221: train loss: 0.016219012439250946\n",
            "Epoch 17222: train loss: 0.01621662825345993\n",
            "Epoch 17223: train loss: 0.01621425710618496\n",
            "Epoch 17224: train loss: 0.01621188595890999\n",
            "Epoch 17225: train loss: 0.016209503635764122\n",
            "Epoch 17226: train loss: 0.016207130625844002\n",
            "Epoch 17227: train loss: 0.01620475761592388\n",
            "Epoch 17228: train loss: 0.016202379018068314\n",
            "Epoch 17229: train loss: 0.016200009733438492\n",
            "Epoch 17230: train loss: 0.016197634860873222\n",
            "Epoch 17231: train loss: 0.01619526743888855\n",
            "Epoch 17232: train loss: 0.016192898154258728\n",
            "Epoch 17233: train loss: 0.016190532594919205\n",
            "Epoch 17234: train loss: 0.016188165172934532\n",
            "Epoch 17235: train loss: 0.01618579402565956\n",
            "Epoch 17236: train loss: 0.016183434054255486\n",
            "Epoch 17237: train loss: 0.01618107035756111\n",
            "Epoch 17238: train loss: 0.016178704798221588\n",
            "Epoch 17239: train loss: 0.016176346689462662\n",
            "Epoch 17240: train loss: 0.01617397926747799\n",
            "Epoch 17241: train loss: 0.016171615570783615\n",
            "Epoch 17242: train loss: 0.016169248148798943\n",
            "Epoch 17243: train loss: 0.016166888177394867\n",
            "Epoch 17244: train loss: 0.01616452820599079\n",
            "Epoch 17245: train loss: 0.016162164509296417\n",
            "Epoch 17246: train loss: 0.01615980453789234\n",
            "Epoch 17247: train loss: 0.016157446429133415\n",
            "Epoch 17248: train loss: 0.01615508273243904\n",
            "Epoch 17249: train loss: 0.016152720898389816\n",
            "Epoch 17250: train loss: 0.01615036651492119\n",
            "Epoch 17251: train loss: 0.016148004680871964\n",
            "Epoch 17252: train loss: 0.016145646572113037\n",
            "Epoch 17253: train loss: 0.01614329032599926\n",
            "Epoch 17254: train loss: 0.01614093966782093\n",
            "Epoch 17255: train loss: 0.016138575971126556\n",
            "Epoch 17256: train loss: 0.016136232763528824\n",
            "Epoch 17257: train loss: 0.016133878380060196\n",
            "Epoch 17258: train loss: 0.016131529584527016\n",
            "Epoch 17259: train loss: 0.016129175201058388\n",
            "Epoch 17260: train loss: 0.016126831993460655\n",
            "Epoch 17261: train loss: 0.016124477609992027\n",
            "Epoch 17262: train loss: 0.016122130677103996\n",
            "Epoch 17263: train loss: 0.016119781881570816\n",
            "Epoch 17264: train loss: 0.016117434948682785\n",
            "Epoch 17265: train loss: 0.016115089878439903\n",
            "Epoch 17266: train loss: 0.016112742945551872\n",
            "Epoch 17267: train loss: 0.01611039973795414\n",
            "Epoch 17268: train loss: 0.01610805094242096\n",
            "Epoch 17269: train loss: 0.016105711460113525\n",
            "Epoch 17270: train loss: 0.016103366389870644\n",
            "Epoch 17271: train loss: 0.016101015731692314\n",
            "Epoch 17272: train loss: 0.01609867252409458\n",
            "Epoch 17273: train loss: 0.0160963274538517\n",
            "Epoch 17274: train loss: 0.016093987971544266\n",
            "Epoch 17275: train loss: 0.016091644763946533\n",
            "Epoch 17276: train loss: 0.01608929969370365\n",
            "Epoch 17277: train loss: 0.016086962074041367\n",
            "Epoch 17278: train loss: 0.016084618866443634\n",
            "Epoch 17279: train loss: 0.016082286834716797\n",
            "Epoch 17280: train loss: 0.016079943627119064\n",
            "Epoch 17281: train loss: 0.01607760414481163\n",
            "Epoch 17282: train loss: 0.016075273975729942\n",
            "Epoch 17283: train loss: 0.016072943806648254\n",
            "Epoch 17284: train loss: 0.016070609912276268\n",
            "Epoch 17285: train loss: 0.016068268567323685\n",
            "Epoch 17286: train loss: 0.016065942123532295\n",
            "Epoch 17287: train loss: 0.016063613817095757\n",
            "Epoch 17288: train loss: 0.01606127992272377\n",
            "Epoch 17289: train loss: 0.016058949753642082\n",
            "Epoch 17290: train loss: 0.016056615859270096\n",
            "Epoch 17291: train loss: 0.016054285690188408\n",
            "Epoch 17292: train loss: 0.01605195738375187\n",
            "Epoch 17293: train loss: 0.016049625352025032\n",
            "Epoch 17294: train loss: 0.016047298908233643\n",
            "Epoch 17295: train loss: 0.016044968739151955\n",
            "Epoch 17296: train loss: 0.016042642295360565\n",
            "Epoch 17297: train loss: 0.01604030840098858\n",
            "Epoch 17298: train loss: 0.016037985682487488\n",
            "Epoch 17299: train loss: 0.0160356592386961\n",
            "Epoch 17300: train loss: 0.01603333279490471\n",
            "Epoch 17301: train loss: 0.016031011939048767\n",
            "Epoch 17302: train loss: 0.016028689220547676\n",
            "Epoch 17303: train loss: 0.016026364639401436\n",
            "Epoch 17304: train loss: 0.016024034470319748\n",
            "Epoch 17305: train loss: 0.016021721065044403\n",
            "Epoch 17306: train loss: 0.01601940207183361\n",
            "Epoch 17307: train loss: 0.01601707935333252\n",
            "Epoch 17308: train loss: 0.016014762222766876\n",
            "Epoch 17309: train loss: 0.01601245068013668\n",
            "Epoch 17310: train loss: 0.016010135412216187\n",
            "Epoch 17311: train loss: 0.016007810831069946\n",
            "Epoch 17312: train loss: 0.01600549928843975\n",
            "Epoch 17313: train loss: 0.016003187745809555\n",
            "Epoch 17314: train loss: 0.016000866889953613\n",
            "Epoch 17315: train loss: 0.015998559072613716\n",
            "Epoch 17316: train loss: 0.015996241942048073\n",
            "Epoch 17317: train loss: 0.01599392481148243\n",
            "Epoch 17318: train loss: 0.015991611406207085\n",
            "Epoch 17319: train loss: 0.015989303588867188\n",
            "Epoch 17320: train loss: 0.015986988320946693\n",
            "Epoch 17321: train loss: 0.015984676778316498\n",
            "Epoch 17322: train loss: 0.015982365235686302\n",
            "Epoch 17323: train loss: 0.015980055555701256\n",
            "Epoch 17324: train loss: 0.01597774215042591\n",
            "Epoch 17325: train loss: 0.015975438058376312\n",
            "Epoch 17326: train loss: 0.015973126515746117\n",
            "Epoch 17327: train loss: 0.01597082056105137\n",
            "Epoch 17328: train loss: 0.015968509018421173\n",
            "Epoch 17329: train loss: 0.015966197475790977\n",
            "Epoch 17330: train loss: 0.015963898971676826\n",
            "Epoch 17331: train loss: 0.01596159115433693\n",
            "Epoch 17332: train loss: 0.01595928892493248\n",
            "Epoch 17333: train loss: 0.01595698855817318\n",
            "Epoch 17334: train loss: 0.01595468819141388\n",
            "Epoch 17335: train loss: 0.01595238968729973\n",
            "Epoch 17336: train loss: 0.015950089320540428\n",
            "Epoch 17337: train loss: 0.015947792679071426\n",
            "Epoch 17338: train loss: 0.015945494174957275\n",
            "Epoch 17339: train loss: 0.015943191945552826\n",
            "Epoch 17340: train loss: 0.015940897166728973\n",
            "Epoch 17341: train loss: 0.015938596799969673\n",
            "Epoch 17342: train loss: 0.01593630388379097\n",
            "Epoch 17343: train loss: 0.01593400165438652\n",
            "Epoch 17344: train loss: 0.01593170315027237\n",
            "Epoch 17345: train loss: 0.015929406508803368\n",
            "Epoch 17346: train loss: 0.015927115455269814\n",
            "Epoch 17347: train loss: 0.01592482253909111\n",
            "Epoch 17348: train loss: 0.01592252589762211\n",
            "Epoch 17349: train loss: 0.015920231118798256\n",
            "Epoch 17350: train loss: 0.015917940065264702\n",
            "Epoch 17351: train loss: 0.015915637835860252\n",
            "Epoch 17352: train loss: 0.015913350507616997\n",
            "Epoch 17353: train loss: 0.015911059454083443\n",
            "Epoch 17354: train loss: 0.01590876467525959\n",
            "Epoch 17355: train loss: 0.015906475484371185\n",
            "Epoch 17356: train loss: 0.015904191881418228\n",
            "Epoch 17357: train loss: 0.015901902690529823\n",
            "Epoch 17358: train loss: 0.015899617224931717\n",
            "Epoch 17359: train loss: 0.01589733362197876\n",
            "Epoch 17360: train loss: 0.015895048156380653\n",
            "Epoch 17361: train loss: 0.015892766416072845\n",
            "Epoch 17362: train loss: 0.01589048281311989\n",
            "Epoch 17363: train loss: 0.01588819921016693\n",
            "Epoch 17364: train loss: 0.015885917469859123\n",
            "Epoch 17365: train loss: 0.015883630141615868\n",
            "Epoch 17366: train loss: 0.015881352126598358\n",
            "Epoch 17367: train loss: 0.0158790722489357\n",
            "Epoch 17368: train loss: 0.015876784920692444\n",
            "Epoch 17369: train loss: 0.015874505043029785\n",
            "Epoch 17370: train loss: 0.015872230753302574\n",
            "Epoch 17371: train loss: 0.015869950875639915\n",
            "Epoch 17372: train loss: 0.01586766727268696\n",
            "Epoch 17373: train loss: 0.01586538925766945\n",
            "Epoch 17374: train loss: 0.015863116830587387\n",
            "Epoch 17375: train loss: 0.01586083509027958\n",
            "Epoch 17376: train loss: 0.01585855893790722\n",
            "Epoch 17377: train loss: 0.01585628092288971\n",
            "Epoch 17378: train loss: 0.01585400477051735\n",
            "Epoch 17379: train loss: 0.015851732343435287\n",
            "Epoch 17380: train loss: 0.015849463641643524\n",
            "Epoch 17381: train loss: 0.015847185626626015\n",
            "Epoch 17382: train loss: 0.015844913199543953\n",
            "Epoch 17383: train loss: 0.01584264449775219\n",
            "Epoch 17384: train loss: 0.015840375795960426\n",
            "Epoch 17385: train loss: 0.015838103368878365\n",
            "Epoch 17386: train loss: 0.01583584025502205\n",
            "Epoch 17387: train loss: 0.015833569690585136\n",
            "Epoch 17388: train loss: 0.01583130843937397\n",
            "Epoch 17389: train loss: 0.015829037874937057\n",
            "Epoch 17390: train loss: 0.015826772898435593\n",
            "Epoch 17391: train loss: 0.01582450605928898\n",
            "Epoch 17392: train loss: 0.015822239220142365\n",
            "Epoch 17393: train loss: 0.01581997238099575\n",
            "Epoch 17394: train loss: 0.015817709267139435\n",
            "Epoch 17395: train loss: 0.01581544801592827\n",
            "Epoch 17396: train loss: 0.015813181176781654\n",
            "Epoch 17397: train loss: 0.01581091620028019\n",
            "Epoch 17398: train loss: 0.015808649361133575\n",
            "Epoch 17399: train loss: 0.015806395560503006\n",
            "Epoch 17400: train loss: 0.015804139897227287\n",
            "Epoch 17401: train loss: 0.015801873058080673\n",
            "Epoch 17402: train loss: 0.015799613669514656\n",
            "Epoch 17403: train loss: 0.01579735241830349\n",
            "Epoch 17404: train loss: 0.015795093029737473\n",
            "Epoch 17405: train loss: 0.015792829915881157\n",
            "Epoch 17406: train loss: 0.015790579840540886\n",
            "Epoch 17407: train loss: 0.015788322314620018\n",
            "Epoch 17408: train loss: 0.01578606851398945\n",
            "Epoch 17409: train loss: 0.01578381471335888\n",
            "Epoch 17410: train loss: 0.015781564638018608\n",
            "Epoch 17411: train loss: 0.01577930338680744\n",
            "Epoch 17412: train loss: 0.01577705703675747\n",
            "Epoch 17413: train loss: 0.015774806961417198\n",
            "Epoch 17414: train loss: 0.015772556886076927\n",
            "Epoch 17415: train loss: 0.015770303085446358\n",
            "Epoch 17416: train loss: 0.015768058598041534\n",
            "Epoch 17417: train loss: 0.015765801072120667\n",
            "Epoch 17418: train loss: 0.015763556584715843\n",
            "Epoch 17419: train loss: 0.015761300921440125\n",
            "Epoch 17420: train loss: 0.015759050846099854\n",
            "Epoch 17421: train loss: 0.01575680822134018\n",
            "Epoch 17422: train loss: 0.015754560008645058\n",
            "Epoch 17423: train loss: 0.015752313658595085\n",
            "Epoch 17424: train loss: 0.015750063583254814\n",
            "Epoch 17425: train loss: 0.01574781723320484\n",
            "Epoch 17426: train loss: 0.015745576471090317\n",
            "Epoch 17427: train loss: 0.015743326395750046\n",
            "Epoch 17428: train loss: 0.01574108749628067\n",
            "Epoch 17429: train loss: 0.01573883928358555\n",
            "Epoch 17430: train loss: 0.015736598521471024\n",
            "Epoch 17431: train loss: 0.015734359622001648\n",
            "Epoch 17432: train loss: 0.015732116997241974\n",
            "Epoch 17433: train loss: 0.01572987250983715\n",
            "Epoch 17434: train loss: 0.015727637335658073\n",
            "Epoch 17435: train loss: 0.015725398436188698\n",
            "Epoch 17436: train loss: 0.015723157674074173\n",
            "Epoch 17437: train loss: 0.015720924362540245\n",
            "Epoch 17438: train loss: 0.015718691051006317\n",
            "Epoch 17439: train loss: 0.01571645401418209\n",
            "Epoch 17440: train loss: 0.015714222565293312\n",
            "Epoch 17441: train loss: 0.015711979940533638\n",
            "Epoch 17442: train loss: 0.01570975035429001\n",
            "Epoch 17443: train loss: 0.01570751704275608\n",
            "Epoch 17444: train loss: 0.015705281868577003\n",
            "Epoch 17445: train loss: 0.015703050419688225\n",
            "Epoch 17446: train loss: 0.015700817108154297\n",
            "Epoch 17447: train loss: 0.015698585659265518\n",
            "Epoch 17448: train loss: 0.01569635607302189\n",
            "Epoch 17449: train loss: 0.01569412462413311\n",
            "Epoch 17450: train loss: 0.01569189503788948\n",
            "Epoch 17451: train loss: 0.01568966917693615\n",
            "Epoch 17452: train loss: 0.015687430277466774\n",
            "Epoch 17453: train loss: 0.015685204416513443\n",
            "Epoch 17454: train loss: 0.015682972967624664\n",
            "Epoch 17455: train loss: 0.015680750831961632\n",
            "Epoch 17456: train loss: 0.015678521245718002\n",
            "Epoch 17457: train loss: 0.01567629724740982\n",
            "Epoch 17458: train loss: 0.01567407138645649\n",
            "Epoch 17459: train loss: 0.015671847388148308\n",
            "Epoch 17460: train loss: 0.015669627115130424\n",
            "Epoch 17461: train loss: 0.015667397528886795\n",
            "Epoch 17462: train loss: 0.01566517911851406\n",
            "Epoch 17463: train loss: 0.015662958845496178\n",
            "Epoch 17464: train loss: 0.015660736709833145\n",
            "Epoch 17465: train loss: 0.01565852016210556\n",
            "Epoch 17466: train loss: 0.015656299889087677\n",
            "Epoch 17467: train loss: 0.015654077753424644\n",
            "Epoch 17468: train loss: 0.01565186306834221\n",
            "Epoch 17469: train loss: 0.015649646520614624\n",
            "Epoch 17470: train loss: 0.01564742811024189\n",
            "Epoch 17471: train loss: 0.015645213425159454\n",
            "Epoch 17472: train loss: 0.01564299687743187\n",
            "Epoch 17473: train loss: 0.015640778467059135\n",
            "Epoch 17474: train loss: 0.01563856564462185\n",
            "Epoch 17475: train loss: 0.015636352822184563\n",
            "Epoch 17476: train loss: 0.015634141862392426\n",
            "Epoch 17477: train loss: 0.01563192345201969\n",
            "Epoch 17478: train loss: 0.015629706904292107\n",
            "Epoch 17479: train loss: 0.015627490356564522\n",
            "Epoch 17480: train loss: 0.015625284984707832\n",
            "Epoch 17481: train loss: 0.01562307309359312\n",
            "Epoch 17482: train loss: 0.015620863065123558\n",
            "Epoch 17483: train loss: 0.015618648380041122\n",
            "Epoch 17484: train loss: 0.015616446733474731\n",
            "Epoch 17485: train loss: 0.015614239498972893\n",
            "Epoch 17486: train loss: 0.01561202947050333\n",
            "Epoch 17487: train loss: 0.015609819442033768\n",
            "Epoch 17488: train loss: 0.015607615001499653\n",
            "Epoch 17489: train loss: 0.015605414286255836\n",
            "Epoch 17490: train loss: 0.015603206120431423\n",
            "Epoch 17491: train loss: 0.015601001679897308\n",
            "Epoch 17492: train loss: 0.015598799102008343\n",
            "Epoch 17493: train loss: 0.015596592798829079\n",
            "Epoch 17494: train loss: 0.015594393014907837\n",
            "Epoch 17495: train loss: 0.015592187643051147\n",
            "Epoch 17496: train loss: 0.015589991584420204\n",
            "Epoch 17497: train loss: 0.015587789006531239\n",
            "Epoch 17498: train loss: 0.015585584565997124\n",
            "Epoch 17499: train loss: 0.015583383850753307\n",
            "Epoch 17500: train loss: 0.015581180341541767\n",
            "Epoch 17501: train loss: 0.01557898335158825\n",
            "Epoch 17502: train loss: 0.01557679008692503\n",
            "Epoch 17503: train loss: 0.015574590303003788\n",
            "Epoch 17504: train loss: 0.01557239331305027\n",
            "Epoch 17505: train loss: 0.015570192597806454\n",
            "Epoch 17506: train loss: 0.015567994676530361\n",
            "Epoch 17507: train loss: 0.015565807931125164\n",
            "Epoch 17508: train loss: 0.015563610941171646\n",
            "Epoch 17509: train loss: 0.015561413019895554\n",
            "Epoch 17510: train loss: 0.015559225343167782\n",
            "Epoch 17511: train loss: 0.015557029284536839\n",
            "Epoch 17512: train loss: 0.015554840676486492\n",
            "Epoch 17513: train loss: 0.015552647411823273\n",
            "Epoch 17514: train loss: 0.015550455078482628\n",
            "Epoch 17515: train loss: 0.015548266470432281\n",
            "Epoch 17516: train loss: 0.015546075999736786\n",
            "Epoch 17517: train loss: 0.015543889254331589\n",
            "Epoch 17518: train loss: 0.015541701577603817\n",
            "Epoch 17519: train loss: 0.015539511106908321\n",
            "Epoch 17520: train loss: 0.015537318773567677\n",
            "Epoch 17521: train loss: 0.015535139478743076\n",
            "Epoch 17522: train loss: 0.015532956458628178\n",
            "Epoch 17523: train loss: 0.015530767850577831\n",
            "Epoch 17524: train loss: 0.015528581105172634\n",
            "Epoch 17525: train loss: 0.015526396222412586\n",
            "Epoch 17526: train loss: 0.015524215064942837\n",
            "Epoch 17527: train loss: 0.015522032044827938\n",
            "Epoch 17528: train loss: 0.015519845299422741\n",
            "Epoch 17529: train loss: 0.015517663210630417\n",
            "Epoch 17530: train loss: 0.015515475533902645\n",
            "Epoch 17531: train loss: 0.015513291582465172\n",
            "Epoch 17532: train loss: 0.015511121600866318\n",
            "Epoch 17533: train loss: 0.01550893671810627\n",
            "Epoch 17534: train loss: 0.015506763942539692\n",
            "Epoch 17535: train loss: 0.015504579059779644\n",
            "Epoch 17536: train loss: 0.015502399764955044\n",
            "Epoch 17537: train loss: 0.015500232577323914\n",
            "Epoch 17538: train loss: 0.015498051419854164\n",
            "Epoch 17539: train loss: 0.015495876781642437\n",
            "Epoch 17540: train loss: 0.015493695624172688\n",
            "Epoch 17541: train loss: 0.015491519123315811\n",
            "Epoch 17542: train loss: 0.015489350073039532\n",
            "Epoch 17543: train loss: 0.015487180091440678\n",
            "Epoch 17544: train loss: 0.01548500545322895\n",
            "Epoch 17545: train loss: 0.015482831746339798\n",
            "Epoch 17546: train loss: 0.01548066083341837\n",
            "Epoch 17547: train loss: 0.01547848992049694\n",
            "Epoch 17548: train loss: 0.01547632273286581\n",
            "Epoch 17549: train loss: 0.015474151819944382\n",
            "Epoch 17550: train loss: 0.015471981838345528\n",
            "Epoch 17551: train loss: 0.015469816513359547\n",
            "Epoch 17552: train loss: 0.015467643737792969\n",
            "Epoch 17553: train loss: 0.015465476550161839\n",
            "Epoch 17554: train loss: 0.01546330563724041\n",
            "Epoch 17555: train loss: 0.01546113658696413\n",
            "Epoch 17556: train loss: 0.01545897126197815\n",
            "Epoch 17557: train loss: 0.015456805005669594\n",
            "Epoch 17558: train loss: 0.015454639680683613\n",
            "Epoch 17559: train loss: 0.015452477149665356\n",
            "Epoch 17560: train loss: 0.015450313687324524\n",
            "Epoch 17561: train loss: 0.015448146499693394\n",
            "Epoch 17562: train loss: 0.015445984899997711\n",
            "Epoch 17563: train loss: 0.015443825162947178\n",
            "Epoch 17564: train loss: 0.015441663563251495\n",
            "Epoch 17565: train loss: 0.015439496375620365\n",
            "Epoch 17566: train loss: 0.015437339432537556\n",
            "Epoch 17567: train loss: 0.015435175970196724\n",
            "Epoch 17568: train loss: 0.015433021821081638\n",
            "Epoch 17569: train loss: 0.01543086301535368\n",
            "Epoch 17570: train loss: 0.015428707003593445\n",
            "Epoch 17571: train loss: 0.015426548197865486\n",
            "Epoch 17572: train loss: 0.015424397774040699\n",
            "Epoch 17573: train loss: 0.015422235243022442\n",
            "Epoch 17574: train loss: 0.015420082956552505\n",
            "Epoch 17575: train loss: 0.015417929738759995\n",
            "Epoch 17576: train loss: 0.01541577372699976\n",
            "Epoch 17577: train loss: 0.015413626097142696\n",
            "Epoch 17578: train loss: 0.015411472879350185\n",
            "Epoch 17579: train loss: 0.01540931686758995\n",
            "Epoch 17580: train loss: 0.01540716364979744\n",
            "Epoch 17581: train loss: 0.015405014157295227\n",
            "Epoch 17582: train loss: 0.01540286187082529\n",
            "Epoch 17583: train loss: 0.01540070679038763\n",
            "Epoch 17584: train loss: 0.015398562885820866\n",
            "Epoch 17585: train loss: 0.015396409668028355\n",
            "Epoch 17586: train loss: 0.015394271351397038\n",
            "Epoch 17587: train loss: 0.015392118133604527\n",
            "Epoch 17588: train loss: 0.015389972366392612\n",
            "Epoch 17589: train loss: 0.015387821942567825\n",
            "Epoch 17590: train loss: 0.015385677106678486\n",
            "Epoch 17591: train loss: 0.01538353506475687\n",
            "Epoch 17592: train loss: 0.015381384640932083\n",
            "Epoch 17593: train loss: 0.015379238873720169\n",
            "Epoch 17594: train loss: 0.015377091243863106\n",
            "Epoch 17595: train loss: 0.015374948270618916\n",
            "Epoch 17596: train loss: 0.0153728062286973\n",
            "Epoch 17597: train loss: 0.015370658598840237\n",
            "Epoch 17598: train loss: 0.015368514694273472\n",
            "Epoch 17599: train loss: 0.015366383828222752\n",
            "Epoch 17600: train loss: 0.015364246442914009\n",
            "Epoch 17601: train loss: 0.015362106263637543\n",
            "Epoch 17602: train loss: 0.015359964221715927\n",
            "Epoch 17603: train loss: 0.015357826836407185\n",
            "Epoch 17604: train loss: 0.015355686657130718\n",
            "Epoch 17605: train loss: 0.01535355020314455\n",
            "Epoch 17606: train loss: 0.01535140909254551\n",
            "Epoch 17607: train loss: 0.01534927636384964\n",
            "Epoch 17608: train loss: 0.015347142703831196\n",
            "Epoch 17609: train loss: 0.015345006249845028\n",
            "Epoch 17610: train loss: 0.015342872589826584\n",
            "Epoch 17611: train loss: 0.01534073892980814\n",
            "Epoch 17612: train loss: 0.015338604338467121\n",
            "Epoch 17613: train loss: 0.015336470678448677\n",
            "Epoch 17614: train loss: 0.015334337949752808\n",
            "Epoch 17615: train loss: 0.01533221174031496\n",
            "Epoch 17616: train loss: 0.015330079942941666\n",
            "Epoch 17617: train loss: 0.01532794814556837\n",
            "Epoch 17618: train loss: 0.015325818210840225\n",
            "Epoch 17619: train loss: 0.015323685482144356\n",
            "Epoch 17620: train loss: 0.015321553684771061\n",
            "Epoch 17621: train loss: 0.015319423750042915\n",
            "Epoch 17622: train loss: 0.015317297540605068\n",
            "Epoch 17623: train loss: 0.01531517319381237\n",
            "Epoch 17624: train loss: 0.015313041396439075\n",
            "Epoch 17625: train loss: 0.015310913324356079\n",
            "Epoch 17626: train loss: 0.015308786183595657\n",
            "Epoch 17627: train loss: 0.015306664630770683\n",
            "Epoch 17628: train loss: 0.015304542146623135\n",
            "Epoch 17629: train loss: 0.015302425250411034\n",
            "Epoch 17630: train loss: 0.015300293453037739\n",
            "Epoch 17631: train loss: 0.015298174694180489\n",
            "Epoch 17632: train loss: 0.015296053141355515\n",
            "Epoch 17633: train loss: 0.015293930657207966\n",
            "Epoch 17634: train loss: 0.015291810035705566\n",
            "Epoch 17635: train loss: 0.015289689414203167\n",
            "Epoch 17636: train loss: 0.015287566930055618\n",
            "Epoch 17637: train loss: 0.015285447239875793\n",
            "Epoch 17638: train loss: 0.01528333593159914\n",
            "Epoch 17639: train loss: 0.015281214378774166\n",
            "Epoch 17640: train loss: 0.015279097482562065\n",
            "Epoch 17641: train loss: 0.015276987105607986\n",
            "Epoch 17642: train loss: 0.015274866484105587\n",
            "Epoch 17643: train loss: 0.015272745862603188\n",
            "Epoch 17644: train loss: 0.01527063176035881\n",
            "Epoch 17645: train loss: 0.015268517658114433\n",
            "Epoch 17646: train loss: 0.015266402624547482\n",
            "Epoch 17647: train loss: 0.015264290384948254\n",
            "Epoch 17648: train loss: 0.015262178145349026\n",
            "Epoch 17649: train loss: 0.015260066837072372\n",
            "Epoch 17650: train loss: 0.015257944352924824\n",
            "Epoch 17651: train loss: 0.015255832113325596\n",
            "Epoch 17652: train loss: 0.015253723599016666\n",
            "Epoch 17653: train loss: 0.015251613222062588\n",
            "Epoch 17654: train loss: 0.015249512158334255\n",
            "Epoch 17655: train loss: 0.01524739433079958\n",
            "Epoch 17656: train loss: 0.015245289541780949\n",
            "Epoch 17657: train loss: 0.015243181958794594\n",
            "Epoch 17658: train loss: 0.015241071581840515\n",
            "Epoch 17659: train loss: 0.015238970518112183\n",
            "Epoch 17660: train loss: 0.015236862003803253\n",
            "Epoch 17661: train loss: 0.015234755352139473\n",
            "Epoch 17662: train loss: 0.015232652425765991\n",
            "Epoch 17663: train loss: 0.015230543911457062\n",
            "Epoch 17664: train loss: 0.015228441916406155\n",
            "Epoch 17665: train loss: 0.015226338990032673\n",
            "Epoch 17666: train loss: 0.01522423792630434\n",
            "Epoch 17667: train loss: 0.015222138725221157\n",
            "Epoch 17668: train loss: 0.01522003673017025\n",
            "Epoch 17669: train loss: 0.015217937529087067\n",
            "Epoch 17670: train loss: 0.015215834602713585\n",
            "Epoch 17671: train loss: 0.015213733538985252\n",
            "Epoch 17672: train loss: 0.015211638994514942\n",
            "Epoch 17673: train loss: 0.015209532342851162\n",
            "Epoch 17674: train loss: 0.015207433141767979\n",
            "Epoch 17675: train loss: 0.015205338597297668\n",
            "Epoch 17676: train loss: 0.015203239396214485\n",
            "Epoch 17677: train loss: 0.015201138332486153\n",
            "Epoch 17678: train loss: 0.015199040062725544\n",
            "Epoch 17679: train loss: 0.01519694458693266\n",
            "Epoch 17680: train loss: 0.015194851905107498\n",
            "Epoch 17681: train loss: 0.015192754566669464\n",
            "Epoch 17682: train loss: 0.01519067119807005\n",
            "Epoch 17683: train loss: 0.01518857292830944\n",
            "Epoch 17684: train loss: 0.015186482109129429\n",
            "Epoch 17685: train loss: 0.015184390358626842\n",
            "Epoch 17686: train loss: 0.015182297676801682\n",
            "Epoch 17687: train loss: 0.015180203132331371\n",
            "Epoch 17688: train loss: 0.015178119763731956\n",
            "Epoch 17689: train loss: 0.015176024287939072\n",
            "Epoch 17690: train loss: 0.01517393346875906\n",
            "Epoch 17691: train loss: 0.015171844512224197\n",
            "Epoch 17692: train loss: 0.015169753693044186\n",
            "Epoch 17693: train loss: 0.01516767032444477\n",
            "Epoch 17694: train loss: 0.015165579505264759\n",
            "Epoch 17695: train loss: 0.015163497999310493\n",
            "Epoch 17696: train loss: 0.015161409042775631\n",
            "Epoch 17697: train loss: 0.015159325674176216\n",
            "Epoch 17698: train loss: 0.015157240442931652\n",
            "Epoch 17699: train loss: 0.015155160799622536\n",
            "Epoch 17700: train loss: 0.015153071843087673\n",
            "Epoch 17701: train loss: 0.015150988474488258\n",
            "Epoch 17702: train loss: 0.015148909762501717\n",
            "Epoch 17703: train loss: 0.015146824531257153\n",
            "Epoch 17704: train loss: 0.01514473557472229\n",
            "Epoch 17705: train loss: 0.01514266338199377\n",
            "Epoch 17706: train loss: 0.015140576288104057\n",
            "Epoch 17707: train loss: 0.015138496644794941\n",
            "Epoch 17708: train loss: 0.01513641607016325\n",
            "Epoch 17709: train loss: 0.015134342014789581\n",
            "Epoch 17710: train loss: 0.01513226330280304\n",
            "Epoch 17711: train loss: 0.015130183659493923\n",
            "Epoch 17712: train loss: 0.01512810681015253\n",
            "Epoch 17713: train loss: 0.015126030892133713\n",
            "Epoch 17714: train loss: 0.015123951248824596\n",
            "Epoch 17715: train loss: 0.015121878124773502\n",
            "Epoch 17716: train loss: 0.015119806863367558\n",
            "Epoch 17717: train loss: 0.015117723494768143\n",
            "Epoch 17718: train loss: 0.015115654096007347\n",
            "Epoch 17719: train loss: 0.015113581903278828\n",
            "Epoch 17720: train loss: 0.015111503191292286\n",
            "Epoch 17721: train loss: 0.015109441243112087\n",
            "Epoch 17722: train loss: 0.015107364393770695\n",
            "Epoch 17723: train loss: 0.015105294063687325\n",
            "Epoch 17724: train loss: 0.01510322280228138\n",
            "Epoch 17725: train loss: 0.015101155266165733\n",
            "Epoch 17726: train loss: 0.015099084004759789\n",
            "Epoch 17727: train loss: 0.015097012743353844\n",
            "Epoch 17728: train loss: 0.015094948932528496\n",
            "Epoch 17729: train loss: 0.015092874877154827\n",
            "Epoch 17730: train loss: 0.01509080920368433\n",
            "Epoch 17731: train loss: 0.015088741667568684\n",
            "Epoch 17732: train loss: 0.015086675994098186\n",
            "Epoch 17733: train loss: 0.01508460845798254\n",
            "Epoch 17734: train loss: 0.015082540921866894\n",
            "Epoch 17735: train loss: 0.015080475248396397\n",
            "Epoch 17736: train loss: 0.015078413300216198\n",
            "Epoch 17737: train loss: 0.0150763476267457\n",
            "Epoch 17738: train loss: 0.015074288472533226\n",
            "Epoch 17739: train loss: 0.015072226524353027\n",
            "Epoch 17740: train loss: 0.01507016271352768\n",
            "Epoch 17741: train loss: 0.015068107284605503\n",
            "Epoch 17742: train loss: 0.015066049061715603\n",
            "Epoch 17743: train loss: 0.015063987113535404\n",
            "Epoch 17744: train loss: 0.015061921440064907\n",
            "Epoch 17745: train loss: 0.015059864148497581\n",
            "Epoch 17746: train loss: 0.01505780778825283\n",
            "Epoch 17747: train loss: 0.015055752359330654\n",
            "Epoch 17748: train loss: 0.015053694136440754\n",
            "Epoch 17749: train loss: 0.015051636844873428\n",
            "Epoch 17750: train loss: 0.015049582347273827\n",
            "Epoch 17751: train loss: 0.015047525055706501\n",
            "Epoch 17752: train loss: 0.01504546869546175\n",
            "Epoch 17753: train loss: 0.015043415129184723\n",
            "Epoch 17754: train loss: 0.015041358768939972\n",
            "Epoch 17755: train loss: 0.015039305202662945\n",
            "Epoch 17756: train loss: 0.015037255361676216\n",
            "Epoch 17757: train loss: 0.015035201795399189\n",
            "Epoch 17758: train loss: 0.01503314170986414\n",
            "Epoch 17759: train loss: 0.015031091868877411\n",
            "Epoch 17760: train loss: 0.015029039233922958\n",
            "Epoch 17761: train loss: 0.01502698939293623\n",
            "Epoch 17762: train loss: 0.015024939551949501\n",
            "Epoch 17763: train loss: 0.015022884123027325\n",
            "Epoch 17764: train loss: 0.015020843595266342\n",
            "Epoch 17765: train loss: 0.01501879096031189\n",
            "Epoch 17766: train loss: 0.01501674484461546\n",
            "Epoch 17767: train loss: 0.015014701522886753\n",
            "Epoch 17768: train loss: 0.015012655407190323\n",
            "Epoch 17769: train loss: 0.015010611154139042\n",
            "Epoch 17770: train loss: 0.015008559450507164\n",
            "Epoch 17771: train loss: 0.015006517060101032\n",
            "Epoch 17772: train loss: 0.015004469081759453\n",
            "Epoch 17773: train loss: 0.015002434141933918\n",
            "Epoch 17774: train loss: 0.015000386163592339\n",
            "Epoch 17775: train loss: 0.014998343773186207\n",
            "Epoch 17776: train loss: 0.014996306039392948\n",
            "Epoch 17777: train loss: 0.014994263648986816\n",
            "Epoch 17778: train loss: 0.014992223121225834\n",
            "Epoch 17779: train loss: 0.014990187250077724\n",
            "Epoch 17780: train loss: 0.014988141134381294\n",
            "Epoch 17781: train loss: 0.014986102469265461\n",
            "Epoch 17782: train loss: 0.014984064735472202\n",
            "Epoch 17783: train loss: 0.014982027933001518\n",
            "Epoch 17784: train loss: 0.014979985542595387\n",
            "Epoch 17785: train loss: 0.014977945946156979\n",
            "Epoch 17786: train loss: 0.014975911006331444\n",
            "Epoch 17787: train loss: 0.014973871409893036\n",
            "Epoch 17788: train loss: 0.014971830882132053\n",
            "Epoch 17789: train loss: 0.01496980432420969\n",
            "Epoch 17790: train loss: 0.014967761933803558\n",
            "Epoch 17791: train loss: 0.014965723268687725\n",
            "Epoch 17792: train loss: 0.014963689260184765\n",
            "Epoch 17793: train loss: 0.014961658976972103\n",
            "Epoch 17794: train loss: 0.014959626831114292\n",
            "Epoch 17795: train loss: 0.014957590028643608\n",
            "Epoch 17796: train loss: 0.014955559745430946\n",
            "Epoch 17797: train loss: 0.014953527599573135\n",
            "Epoch 17798: train loss: 0.014951497316360474\n",
            "Epoch 17799: train loss: 0.01494947075843811\n",
            "Epoch 17800: train loss: 0.014947446063160896\n",
            "Epoch 17801: train loss: 0.014945417642593384\n",
            "Epoch 17802: train loss: 0.014943390153348446\n",
            "Epoch 17803: train loss: 0.014941358007490635\n",
            "Epoch 17804: train loss: 0.014939339831471443\n",
            "Epoch 17805: train loss: 0.014937310479581356\n",
            "Epoch 17806: train loss: 0.014935288578271866\n",
            "Epoch 17807: train loss: 0.014933260157704353\n",
            "Epoch 17808: train loss: 0.01493123546242714\n",
            "Epoch 17809: train loss: 0.014929212629795074\n",
            "Epoch 17810: train loss: 0.01492718793451786\n",
            "Epoch 17811: train loss: 0.014925166964530945\n",
            "Epoch 17812: train loss: 0.014923140406608582\n",
            "Epoch 17813: train loss: 0.014921113848686218\n",
            "Epoch 17814: train loss: 0.01491909846663475\n",
            "Epoch 17815: train loss: 0.014917073771357536\n",
            "Epoch 17816: train loss: 0.014915051870048046\n",
            "Epoch 17817: train loss: 0.014913029968738556\n",
            "Epoch 17818: train loss: 0.014911007136106491\n",
            "Epoch 17819: train loss: 0.014908986166119576\n",
            "Epoch 17820: train loss: 0.01490696705877781\n",
            "Epoch 17821: train loss: 0.014904944226145744\n",
            "Epoch 17822: train loss: 0.014902926050126553\n",
            "Epoch 17823: train loss: 0.014900915324687958\n",
            "Epoch 17824: train loss: 0.014898899011313915\n",
            "Epoch 17825: train loss: 0.014896883629262447\n",
            "Epoch 17826: train loss: 0.014894861727952957\n",
            "Epoch 17827: train loss: 0.014892843551933765\n",
            "Epoch 17828: train loss: 0.014890831895172596\n",
            "Epoch 17829: train loss: 0.014888825826346874\n",
            "Epoch 17830: train loss: 0.014886811375617981\n",
            "Epoch 17831: train loss: 0.014884799718856812\n",
            "Epoch 17832: train loss: 0.014882788993418217\n",
            "Epoch 17833: train loss: 0.014880772680044174\n",
            "Epoch 17834: train loss: 0.014878765679895878\n",
            "Epoch 17835: train loss: 0.014876753091812134\n",
            "Epoch 17836: train loss: 0.014874746091663837\n",
            "Epoch 17837: train loss: 0.014872732572257519\n",
            "Epoch 17838: train loss: 0.014870721846818924\n",
            "Epoch 17839: train loss: 0.014868718571960926\n",
            "Epoch 17840: train loss: 0.014866707846522331\n",
            "Epoch 17841: train loss: 0.014864704571664333\n",
            "Epoch 17842: train loss: 0.014862692914903164\n",
            "Epoch 17843: train loss: 0.014860681258141994\n",
            "Epoch 17844: train loss: 0.014858673326671124\n",
            "Epoch 17845: train loss: 0.014856670051813126\n",
            "Epoch 17846: train loss: 0.014854665845632553\n",
            "Epoch 17847: train loss: 0.014852658845484257\n",
            "Epoch 17848: train loss: 0.014850658364593983\n",
            "Epoch 17849: train loss: 0.014848651364445686\n",
            "Epoch 17850: train loss: 0.014846648089587688\n",
            "Epoch 17851: train loss: 0.014844643883407116\n",
            "Epoch 17852: train loss: 0.014842640608549118\n",
            "Epoch 17853: train loss: 0.014840643852949142\n",
            "Epoch 17854: train loss: 0.014838644303381443\n",
            "Epoch 17855: train loss: 0.014836641028523445\n",
            "Epoch 17856: train loss: 0.014834643341600895\n",
            "Epoch 17857: train loss: 0.014832643792033195\n",
            "Epoch 17858: train loss: 0.014830644242465496\n",
            "Epoch 17859: train loss: 0.014828648418188095\n",
            "Epoch 17860: train loss: 0.014826658181846142\n",
            "Epoch 17861: train loss: 0.01482465397566557\n",
            "Epoch 17862: train loss: 0.014822655357420444\n",
            "Epoch 17863: train loss: 0.01482066698372364\n",
            "Epoch 17864: train loss: 0.014818664640188217\n",
            "Epoch 17865: train loss: 0.014816670678555965\n",
            "Epoch 17866: train loss: 0.014814678579568863\n",
            "Epoch 17867: train loss: 0.014812680892646313\n",
            "Epoch 17868: train loss: 0.01481069065630436\n",
            "Epoch 17869: train loss: 0.014808695763349533\n",
            "Epoch 17870: train loss: 0.014806699007749557\n",
            "Epoch 17871: train loss: 0.014804715290665627\n",
            "Epoch 17872: train loss: 0.014802713878452778\n",
            "Epoch 17873: train loss: 0.0148007245734334\n",
            "Epoch 17874: train loss: 0.014798733405768871\n",
            "Epoch 17875: train loss: 0.014796745963394642\n",
            "Epoch 17876: train loss: 0.014794749207794666\n",
            "Epoch 17877: train loss: 0.014792761765420437\n",
            "Epoch 17878: train loss: 0.014790775254368782\n",
            "Epoch 17879: train loss: 0.014788780361413956\n",
            "Epoch 17880: train loss: 0.014786796644330025\n",
            "Epoch 17881: train loss: 0.014784809201955795\n",
            "Epoch 17882: train loss: 0.014782823622226715\n",
            "Epoch 17883: train loss: 0.014780840836465359\n",
            "Epoch 17884: train loss: 0.014778854325413704\n",
            "Epoch 17885: train loss: 0.014776872470974922\n",
            "Epoch 17886: train loss: 0.014774888753890991\n",
            "Epoch 17887: train loss: 0.014772904105484486\n",
            "Epoch 17888: train loss: 0.014770925976336002\n",
            "Epoch 17889: train loss: 0.014768943190574646\n",
            "Epoch 17890: train loss: 0.014766957610845566\n",
            "Epoch 17891: train loss: 0.014764981344342232\n",
            "Epoch 17892: train loss: 0.014762993901968002\n",
            "Epoch 17893: train loss: 0.014761018566787243\n",
            "Epoch 17894: train loss: 0.014759035781025887\n",
            "Epoch 17895: train loss: 0.014757056720554829\n",
            "Epoch 17896: train loss: 0.014755074866116047\n",
            "Epoch 17897: train loss: 0.014753095805644989\n",
            "Epoch 17898: train loss: 0.014751117676496506\n",
            "Epoch 17899: train loss: 0.014749135822057724\n",
            "Epoch 17900: train loss: 0.014747166074812412\n",
            "Epoch 17901: train loss: 0.01474518608301878\n",
            "Epoch 17902: train loss: 0.01474321074783802\n",
            "Epoch 17903: train loss: 0.014741233550012112\n",
            "Epoch 17904: train loss: 0.014739256352186203\n",
            "Epoch 17905: train loss: 0.014737283810973167\n",
            "Epoch 17906: train loss: 0.01473530475050211\n",
            "Epoch 17907: train loss: 0.014733328483998775\n",
            "Epoch 17908: train loss: 0.014731357805430889\n",
            "Epoch 17909: train loss: 0.014729387126863003\n",
            "Epoch 17910: train loss: 0.014727411791682243\n",
            "Epoch 17911: train loss: 0.014725442044436932\n",
            "Epoch 17912: train loss: 0.014723468571901321\n",
            "Epoch 17913: train loss: 0.014721494168043137\n",
            "Epoch 17914: train loss: 0.014719532802700996\n",
            "Epoch 17915: train loss: 0.014717564918100834\n",
            "Epoch 17916: train loss: 0.014715593308210373\n",
            "Epoch 17917: train loss: 0.014713631011545658\n",
            "Epoch 17918: train loss: 0.014711661264300346\n",
            "Epoch 17919: train loss: 0.014709697104990482\n",
            "Epoch 17920: train loss: 0.014707732014358044\n",
            "Epoch 17921: train loss: 0.014705764129757881\n",
            "Epoch 17922: train loss: 0.014703799970448017\n",
            "Epoch 17923: train loss: 0.01470183301717043\n",
            "Epoch 17924: train loss: 0.01469986792653799\n",
            "Epoch 17925: train loss: 0.01469790656119585\n",
            "Epoch 17926: train loss: 0.014695940539240837\n",
            "Epoch 17927: train loss: 0.014693976379930973\n",
            "Epoch 17928: train loss: 0.014692013151943684\n",
            "Epoch 17929: train loss: 0.01469004712998867\n",
            "Epoch 17930: train loss: 0.01468808762729168\n",
            "Epoch 17931: train loss: 0.01468612439930439\n",
            "Epoch 17932: train loss: 0.014684166759252548\n",
            "Epoch 17933: train loss: 0.014682198874652386\n",
            "Epoch 17934: train loss: 0.014680244028568268\n",
            "Epoch 17935: train loss: 0.014678280800580978\n",
            "Epoch 17936: train loss: 0.014676323160529137\n",
            "Epoch 17937: train loss: 0.014674357138574123\n",
            "Epoch 17938: train loss: 0.014672400429844856\n",
            "Epoch 17939: train loss: 0.014670442789793015\n",
            "Epoch 17940: train loss: 0.014668489806354046\n",
            "Epoch 17941: train loss: 0.014666528441011906\n",
            "Epoch 17942: train loss: 0.014664582908153534\n",
            "Epoch 17943: train loss: 0.014662623405456543\n",
            "Epoch 17944: train loss: 0.014660668559372425\n",
            "Epoch 17945: train loss: 0.014658716507256031\n",
            "Epoch 17946: train loss: 0.014656771905720234\n",
            "Epoch 17947: train loss: 0.014654818922281265\n",
            "Epoch 17948: train loss: 0.014652860350906849\n",
            "Epoch 17949: train loss: 0.01465090923011303\n",
            "Epoch 17950: train loss: 0.014648962765932083\n",
            "Epoch 17951: train loss: 0.014647012576460838\n",
            "Epoch 17952: train loss: 0.014645058661699295\n",
            "Epoch 17953: train loss: 0.014643103815615177\n",
            "Epoch 17954: train loss: 0.01464115921407938\n",
            "Epoch 17955: train loss: 0.014639207161962986\n",
            "Epoch 17956: train loss: 0.014637261629104614\n",
            "Epoch 17957: train loss: 0.014635314233601093\n",
            "Epoch 17958: train loss: 0.01463336031883955\n",
            "Epoch 17959: train loss: 0.014631417579948902\n",
            "Epoch 17960: train loss: 0.014629472978413105\n",
            "Epoch 17961: train loss: 0.01462752465158701\n",
            "Epoch 17962: train loss: 0.014625579118728638\n",
            "Epoch 17963: train loss: 0.014623627997934818\n",
            "Epoch 17964: train loss: 0.01462168712168932\n",
            "Epoch 17965: train loss: 0.014619739726185799\n",
            "Epoch 17966: train loss: 0.014617790468037128\n",
            "Epoch 17967: train loss: 0.014615846797823906\n",
            "Epoch 17968: train loss: 0.014613904058933258\n",
            "Epoch 17969: train loss: 0.014611959457397461\n",
            "Epoch 17970: train loss: 0.014610019512474537\n",
            "Epoch 17971: train loss: 0.014608082361519337\n",
            "Epoch 17972: train loss: 0.014606138691306114\n",
            "Epoch 17973: train loss: 0.014604205265641212\n",
            "Epoch 17974: train loss: 0.014602262526750565\n",
            "Epoch 17975: train loss: 0.014600325375795364\n",
            "Epoch 17976: train loss: 0.014598389156162739\n",
            "Epoch 17977: train loss: 0.014596452005207539\n",
            "Epoch 17978: train loss: 0.014594513922929764\n",
            "Epoch 17979: train loss: 0.014592579565942287\n",
            "Epoch 17980: train loss: 0.014590648002922535\n",
            "Epoch 17981: train loss: 0.01458871178328991\n",
            "Epoch 17982: train loss: 0.01458677090704441\n",
            "Epoch 17983: train loss: 0.014584840275347233\n",
            "Epoch 17984: train loss: 0.014582901261746883\n",
            "Epoch 17985: train loss: 0.014580970630049706\n",
            "Epoch 17986: train loss: 0.01457903254777193\n",
            "Epoch 17987: train loss: 0.014577102847397327\n",
            "Epoch 17988: train loss: 0.014575166627764702\n",
            "Epoch 17989: train loss: 0.014573231339454651\n",
            "Epoch 17990: train loss: 0.014571299776434898\n",
            "Epoch 17991: train loss: 0.014569372870028019\n",
            "Epoch 17992: train loss: 0.014567437581717968\n",
            "Epoch 17993: train loss: 0.01456550695002079\n",
            "Epoch 17994: train loss: 0.014563576318323612\n",
            "Epoch 17995: train loss: 0.014561643823981285\n",
            "Epoch 17996: train loss: 0.014559712260961533\n",
            "Epoch 17997: train loss: 0.01455778256058693\n",
            "Epoch 17998: train loss: 0.014555854722857475\n",
            "Epoch 17999: train loss: 0.014553927816450596\n",
            "Epoch 18000: train loss: 0.014551999047398567\n",
            "Epoch 18001: train loss: 0.014550075866281986\n",
            "Epoch 18002: train loss: 0.014548150822520256\n",
            "Epoch 18003: train loss: 0.0145462267100811\n",
            "Epoch 18004: train loss: 0.01454430352896452\n",
            "Epoch 18005: train loss: 0.01454237848520279\n",
            "Epoch 18006: train loss: 0.014540459029376507\n",
            "Epoch 18007: train loss: 0.0145385367795825\n",
            "Epoch 18008: train loss: 0.014536615461111069\n",
            "Epoch 18009: train loss: 0.014534693211317062\n",
            "Epoch 18010: train loss: 0.014532769098877907\n",
            "Epoch 18011: train loss: 0.014530853368341923\n",
            "Epoch 18012: train loss: 0.014528929255902767\n",
            "Epoch 18013: train loss: 0.01452701073139906\n",
            "Epoch 18014: train loss: 0.014525092206895351\n",
            "Epoch 18015: train loss: 0.014523171819746494\n",
            "Epoch 18016: train loss: 0.014521253295242786\n",
            "Epoch 18017: train loss: 0.01451933104544878\n",
            "Epoch 18018: train loss: 0.014517415314912796\n",
            "Epoch 18019: train loss: 0.014515497721731663\n",
            "Epoch 18020: train loss: 0.014513584785163403\n",
            "Epoch 18021: train loss: 0.01451166719198227\n",
            "Epoch 18022: train loss: 0.014509748667478561\n",
            "Epoch 18023: train loss: 0.01450782548636198\n",
            "Epoch 18024: train loss: 0.01450591254979372\n",
            "Epoch 18025: train loss: 0.01450399961322546\n",
            "Epoch 18026: train loss: 0.014502083882689476\n",
            "Epoch 18027: train loss: 0.014500166289508343\n",
            "Epoch 18028: train loss: 0.014498252421617508\n",
            "Epoch 18029: train loss: 0.014496342279016972\n",
            "Epoch 18030: train loss: 0.014494429342448711\n",
            "Epoch 18031: train loss: 0.014492521062493324\n",
            "Epoch 18032: train loss: 0.014490610919892788\n",
            "Epoch 18033: train loss: 0.014488695189356804\n",
            "Epoch 18034: train loss: 0.014486790634691715\n",
            "Epoch 18035: train loss: 0.014484884217381477\n",
            "Epoch 18036: train loss: 0.01448297780007124\n",
            "Epoch 18037: train loss: 0.014481068588793278\n",
            "Epoch 18038: train loss: 0.01447916030883789\n",
            "Epoch 18039: train loss: 0.014477250166237354\n",
            "Epoch 18040: train loss: 0.01447534654289484\n",
            "Epoch 18041: train loss: 0.014473436400294304\n",
            "Epoch 18042: train loss: 0.01447153091430664\n",
            "Epoch 18043: train loss: 0.014469625428318977\n",
            "Epoch 18044: train loss: 0.014467722736299038\n",
            "Epoch 18045: train loss: 0.014465813525021076\n",
            "Epoch 18046: train loss: 0.014463914558291435\n",
            "Epoch 18047: train loss: 0.014462011866271496\n",
            "Epoch 18048: train loss: 0.014460102654993534\n",
            "Epoch 18049: train loss: 0.014458205550909042\n",
            "Epoch 18050: train loss: 0.014456300996243954\n",
            "Epoch 18051: train loss: 0.014454398304224014\n",
            "Epoch 18052: train loss: 0.014452489092946053\n",
            "Epoch 18053: train loss: 0.01445059571415186\n",
            "Epoch 18054: train loss: 0.01444869302213192\n",
            "Epoch 18055: train loss: 0.014446786604821682\n",
            "Epoch 18056: train loss: 0.014444884844124317\n",
            "Epoch 18057: train loss: 0.0144429886713624\n",
            "Epoch 18058: train loss: 0.01444108784198761\n",
            "Epoch 18059: train loss: 0.014439193531870842\n",
            "Epoch 18060: train loss: 0.014437295496463776\n",
            "Epoch 18061: train loss: 0.014435403048992157\n",
            "Epoch 18062: train loss: 0.014433508738875389\n",
            "Epoch 18063: train loss: 0.014431611634790897\n",
            "Epoch 18064: train loss: 0.014429718255996704\n",
            "Epoch 18065: train loss: 0.014427822083234787\n",
            "Epoch 18066: train loss: 0.014425932429730892\n",
            "Epoch 18067: train loss: 0.014424038119614124\n",
            "Epoch 18068: train loss: 0.014422142878174782\n",
            "Epoch 18069: train loss: 0.014420252293348312\n",
            "Epoch 18070: train loss: 0.014418359845876694\n",
            "Epoch 18071: train loss: 0.014416465535759926\n",
            "Epoch 18072: train loss: 0.014414574950933456\n",
            "Epoch 18073: train loss: 0.014412683434784412\n",
            "Epoch 18074: train loss: 0.014410790987312794\n",
            "Epoch 18075: train loss: 0.014408905990421772\n",
            "Epoch 18076: train loss: 0.014407011680305004\n",
            "Epoch 18077: train loss: 0.01440512016415596\n",
            "Epoch 18078: train loss: 0.014403230510652065\n",
            "Epoch 18079: train loss: 0.014401346445083618\n",
            "Epoch 18080: train loss: 0.014399459585547447\n",
            "Epoch 18081: train loss: 0.014397569932043552\n",
            "Epoch 18082: train loss: 0.014395681209862232\n",
            "Epoch 18083: train loss: 0.014393791556358337\n",
            "Epoch 18084: train loss: 0.014391906559467316\n",
            "Epoch 18085: train loss: 0.01439002063125372\n",
            "Epoch 18086: train loss: 0.014388127252459526\n",
            "Epoch 18087: train loss: 0.014386246912181377\n",
            "Epoch 18088: train loss: 0.014384354464709759\n",
            "Epoch 18089: train loss: 0.01438247598707676\n",
            "Epoch 18090: train loss: 0.01438059750944376\n",
            "Epoch 18091: train loss: 0.014378713443875313\n",
            "Epoch 18092: train loss: 0.01437683217227459\n",
            "Epoch 18093: train loss: 0.01437495369464159\n",
            "Epoch 18094: train loss: 0.014373071491718292\n",
            "Epoch 18095: train loss: 0.014371195808053017\n",
            "Epoch 18096: train loss: 0.014369317330420017\n",
            "Epoch 18097: train loss: 0.014367436990141869\n",
            "Epoch 18098: train loss: 0.014365557581186295\n",
            "Epoch 18099: train loss: 0.014363675378262997\n",
            "Epoch 18100: train loss: 0.01436180341988802\n",
            "Epoch 18101: train loss: 0.014359928667545319\n",
            "Epoch 18102: train loss: 0.01435805018991232\n",
            "Epoch 18103: train loss: 0.014356175437569618\n",
            "Epoch 18104: train loss: 0.01435429509729147\n",
            "Epoch 18105: train loss: 0.014352415688335896\n",
            "Epoch 18106: train loss: 0.014350544661283493\n",
            "Epoch 18107: train loss: 0.014348668977618217\n",
            "Epoch 18108: train loss: 0.014346791431307793\n",
            "Epoch 18109: train loss: 0.014344916678965092\n",
            "Epoch 18110: train loss: 0.014343046583235264\n",
            "Epoch 18111: train loss: 0.014341170899569988\n",
            "Epoch 18112: train loss: 0.014339298009872437\n",
            "Epoch 18113: train loss: 0.014337429776787758\n",
            "Epoch 18114: train loss: 0.014335551299154758\n",
            "Epoch 18115: train loss: 0.01433368120342493\n",
            "Epoch 18116: train loss: 0.01433180645108223\n",
            "Epoch 18117: train loss: 0.014329932630062103\n",
            "Epoch 18118: train loss: 0.0143280616030097\n",
            "Epoch 18119: train loss: 0.014326194301247597\n",
            "Epoch 18120: train loss: 0.014324326999485493\n",
            "Epoch 18121: train loss: 0.014322456903755665\n",
            "Epoch 18122: train loss: 0.014320592395961285\n",
            "Epoch 18123: train loss: 0.01431872695684433\n",
            "Epoch 18124: train loss: 0.014316855929791927\n",
            "Epoch 18125: train loss: 0.014314992353320122\n",
            "Epoch 18126: train loss: 0.01431312970817089\n",
            "Epoch 18127: train loss: 0.01431126520037651\n",
            "Epoch 18128: train loss: 0.014309403486549854\n",
            "Epoch 18129: train loss: 0.0143075380474329\n",
            "Epoch 18130: train loss: 0.014305676333606243\n",
            "Epoch 18131: train loss: 0.014303813688457012\n",
            "Epoch 18132: train loss: 0.014301945455372334\n",
            "Epoch 18133: train loss: 0.01430008839815855\n",
            "Epoch 18134: train loss: 0.01429822575300932\n",
            "Epoch 18135: train loss: 0.014296364039182663\n",
            "Epoch 18136: train loss: 0.014294499531388283\n",
            "Epoch 18137: train loss: 0.014292644336819649\n",
            "Epoch 18138: train loss: 0.014290782622992992\n",
            "Epoch 18139: train loss: 0.01428892184048891\n",
            "Epoch 18140: train loss: 0.014287061057984829\n",
            "Epoch 18141: train loss: 0.01428520493209362\n",
            "Epoch 18142: train loss: 0.014283344149589539\n",
            "Epoch 18143: train loss: 0.014281482435762882\n",
            "Epoch 18144: train loss: 0.014279618859291077\n",
            "Epoch 18145: train loss: 0.014277768321335316\n",
            "Epoch 18146: train loss: 0.014275912195444107\n",
            "Epoch 18147: train loss: 0.014274048618972301\n",
            "Epoch 18148: train loss: 0.014272193424403667\n",
            "Epoch 18149: train loss: 0.01427033357322216\n",
            "Epoch 18150: train loss: 0.014268482103943825\n",
            "Epoch 18151: train loss: 0.01426663063466549\n",
            "Epoch 18152: train loss: 0.014264779165387154\n",
            "Epoch 18153: train loss: 0.01426292210817337\n",
            "Epoch 18154: train loss: 0.014261065050959587\n",
            "Epoch 18155: train loss: 0.0142592191696167\n",
            "Epoch 18156: train loss: 0.014257368631660938\n",
            "Epoch 18157: train loss: 0.014255514368414879\n",
            "Epoch 18158: train loss: 0.014253667555749416\n",
            "Epoch 18159: train loss: 0.01425181981176138\n",
            "Epoch 18160: train loss: 0.014249964617192745\n",
            "Epoch 18161: train loss: 0.014248119667172432\n",
            "Epoch 18162: train loss: 0.014246270060539246\n",
            "Epoch 18163: train loss: 0.014244426973164082\n",
            "Epoch 18164: train loss: 0.014242573641240597\n",
            "Epoch 18165: train loss: 0.01424072403460741\n",
            "Epoch 18166: train loss: 0.014238880947232246\n",
            "Epoch 18167: train loss: 0.014237028546631336\n",
            "Epoch 18168: train loss: 0.014235190115869045\n",
            "Epoch 18169: train loss: 0.014233341440558434\n",
            "Epoch 18170: train loss: 0.014231493696570396\n",
            "Epoch 18171: train loss: 0.014229651540517807\n",
            "Epoch 18172: train loss: 0.014227802865207195\n",
            "Epoch 18173: train loss: 0.014225957915186882\n",
            "Epoch 18174: train loss: 0.014224119484424591\n",
            "Epoch 18175: train loss: 0.014222274534404278\n",
            "Epoch 18176: train loss: 0.014220431447029114\n",
            "Epoch 18177: train loss: 0.0142185864970088\n",
            "Epoch 18178: train loss: 0.014216739684343338\n",
            "Epoch 18179: train loss: 0.014214901253581047\n",
            "Epoch 18180: train loss: 0.014213060028851032\n",
            "Epoch 18181: train loss: 0.014211218804121017\n",
            "Epoch 18182: train loss: 0.014209382236003876\n",
            "Epoch 18183: train loss: 0.014207540079951286\n",
            "Epoch 18184: train loss: 0.01420570444315672\n",
            "Epoch 18185: train loss: 0.014203861355781555\n",
            "Epoch 18186: train loss: 0.014202028512954712\n",
            "Epoch 18187: train loss: 0.014200192876160145\n",
            "Epoch 18188: train loss: 0.014198356308043003\n",
            "Epoch 18189: train loss: 0.01419652160257101\n",
            "Epoch 18190: train loss: 0.014194685034453869\n",
            "Epoch 18191: train loss: 0.014192848466336727\n",
            "Epoch 18192: train loss: 0.01419101282954216\n",
            "Epoch 18193: train loss: 0.01418918464332819\n",
            "Epoch 18194: train loss: 0.014187345281243324\n",
            "Epoch 18195: train loss: 0.01418551616370678\n",
            "Epoch 18196: train loss: 0.014183683320879936\n",
            "Epoch 18197: train loss: 0.014181844890117645\n",
            "Epoch 18198: train loss: 0.014180019497871399\n",
            "Epoch 18199: train loss: 0.014178181067109108\n",
            "Epoch 18200: train loss: 0.01417634729295969\n",
            "Epoch 18201: train loss: 0.014174516312777996\n",
            "Epoch 18202: train loss: 0.014172692783176899\n",
            "Epoch 18203: train loss: 0.014170859940350056\n",
            "Epoch 18204: train loss: 0.01416902244091034\n",
            "Epoch 18205: train loss: 0.014167197048664093\n",
            "Epoch 18206: train loss: 0.014165366068482399\n",
            "Epoch 18207: train loss: 0.014163538813591003\n",
            "Epoch 18208: train loss: 0.014161713421344757\n",
            "Epoch 18209: train loss: 0.014159874059259892\n",
            "Epoch 18210: train loss: 0.014158052392303944\n",
            "Epoch 18211: train loss: 0.014156227000057697\n",
            "Epoch 18212: train loss: 0.014154397882521152\n",
            "Epoch 18213: train loss: 0.014152578078210354\n",
            "Epoch 18214: train loss: 0.014150748960673809\n",
            "Epoch 18215: train loss: 0.014148923568427563\n",
            "Epoch 18216: train loss: 0.014147094450891018\n",
            "Epoch 18217: train loss: 0.014145276509225368\n",
            "Epoch 18218: train loss: 0.01414345670491457\n",
            "Epoch 18219: train loss: 0.014141635037958622\n",
            "Epoch 18220: train loss: 0.014139813371002674\n",
            "Epoch 18221: train loss: 0.014137989841401577\n",
            "Epoch 18222: train loss: 0.014136168174445629\n",
            "Epoch 18223: train loss: 0.014134354889392853\n",
            "Epoch 18224: train loss: 0.014132536016404629\n",
            "Epoch 18225: train loss: 0.014130714349448681\n",
            "Epoch 18226: train loss: 0.014128894545137882\n",
            "Epoch 18227: train loss: 0.014127077534794807\n",
            "Epoch 18228: train loss: 0.014125254936516285\n",
            "Epoch 18229: train loss: 0.014123434200882912\n",
            "Epoch 18230: train loss: 0.01412161998450756\n",
            "Epoch 18231: train loss: 0.01411980390548706\n",
            "Epoch 18232: train loss: 0.014117985963821411\n",
            "Epoch 18233: train loss: 0.014116167090833187\n",
            "Epoch 18234: train loss: 0.014114351943135262\n",
            "Epoch 18235: train loss: 0.014112534001469612\n",
            "Epoch 18236: train loss: 0.014110720716416836\n",
            "Epoch 18237: train loss: 0.01410890556871891\n",
            "Epoch 18238: train loss: 0.014107086695730686\n",
            "Epoch 18239: train loss: 0.014105268754065037\n",
            "Epoch 18240: train loss: 0.01410345546901226\n",
            "Epoch 18241: train loss: 0.014101644046604633\n",
            "Epoch 18242: train loss: 0.014099830761551857\n",
            "Epoch 18243: train loss: 0.01409801933914423\n",
            "Epoch 18244: train loss: 0.014096209779381752\n",
            "Epoch 18245: train loss: 0.01409439742565155\n",
            "Epoch 18246: train loss: 0.014092585071921349\n",
            "Epoch 18247: train loss: 0.014090776443481445\n",
            "Epoch 18248: train loss: 0.014088965021073818\n",
            "Epoch 18249: train loss: 0.014087152667343616\n",
            "Epoch 18250: train loss: 0.014085349626839161\n",
            "Epoch 18251: train loss: 0.014083540067076683\n",
            "Epoch 18252: train loss: 0.014081734232604504\n",
            "Epoch 18253: train loss: 0.014079929329454899\n",
            "Epoch 18254: train loss: 0.014078120701014996\n",
            "Epoch 18255: train loss: 0.014076316729187965\n",
            "Epoch 18256: train loss: 0.014074508100748062\n",
            "Epoch 18257: train loss: 0.01407270785421133\n",
            "Epoch 18258: train loss: 0.014070898294448853\n",
            "Epoch 18259: train loss: 0.014069096185266972\n",
            "Epoch 18260: train loss: 0.014067295007407665\n",
            "Epoch 18261: train loss: 0.014065493829548359\n",
            "Epoch 18262: train loss: 0.01406368613243103\n",
            "Epoch 18263: train loss: 0.014061888679862022\n",
            "Epoch 18264: train loss: 0.01406007818877697\n",
            "Epoch 18265: train loss: 0.014058280736207962\n",
            "Epoch 18266: train loss: 0.014056476764380932\n",
            "Epoch 18267: train loss: 0.014054673723876476\n",
            "Epoch 18268: train loss: 0.014052867889404297\n",
            "Epoch 18269: train loss: 0.014051069505512714\n",
            "Epoch 18270: train loss: 0.014049268327653408\n",
            "Epoch 18271: train loss: 0.014047469943761826\n",
            "Epoch 18272: train loss: 0.014045672491192818\n",
            "Epoch 18273: train loss: 0.014043869450688362\n",
            "Epoch 18274: train loss: 0.01404207106679678\n",
            "Epoch 18275: train loss: 0.014040278270840645\n",
            "Epoch 18276: train loss: 0.014038474299013615\n",
            "Epoch 18277: train loss: 0.014036678709089756\n",
            "Epoch 18278: train loss: 0.01403487753123045\n",
            "Epoch 18279: train loss: 0.014033091254532337\n",
            "Epoch 18280: train loss: 0.014031292870640755\n",
            "Epoch 18281: train loss: 0.014029499143362045\n",
            "Epoch 18282: train loss: 0.014027705416083336\n",
            "Epoch 18283: train loss: 0.014025908894836903\n",
            "Epoch 18284: train loss: 0.014024117961525917\n",
            "Epoch 18285: train loss: 0.014022319577634335\n",
            "Epoch 18286: train loss: 0.014020534232258797\n",
            "Epoch 18287: train loss: 0.014018742367625237\n",
            "Epoch 18288: train loss: 0.014016952365636826\n",
            "Epoch 18289: train loss: 0.014015164226293564\n",
            "Epoch 18290: train loss: 0.014013371430337429\n",
            "Epoch 18291: train loss: 0.01401158794760704\n",
            "Epoch 18292: train loss: 0.014009786769747734\n",
            "Epoch 18293: train loss: 0.014007997699081898\n",
            "Epoch 18294: train loss: 0.014006209559738636\n",
            "Epoch 18295: train loss: 0.014004421420395374\n",
            "Epoch 18296: train loss: 0.014002634212374687\n",
            "Epoch 18297: train loss: 0.014000840485095978\n",
            "Epoch 18298: train loss: 0.01399905700236559\n",
            "Epoch 18299: train loss: 0.013997274450957775\n",
            "Epoch 18300: train loss: 0.013995478861033916\n",
            "Epoch 18301: train loss: 0.01399369165301323\n",
            "Epoch 18302: train loss: 0.013991909101605415\n",
            "Epoch 18303: train loss: 0.013990121893584728\n",
            "Epoch 18304: train loss: 0.013988343067467213\n",
            "Epoch 18305: train loss: 0.013986550271511078\n",
            "Epoch 18306: train loss: 0.013984771445393562\n",
            "Epoch 18307: train loss: 0.013982987031340599\n",
            "Epoch 18308: train loss: 0.013981210999190807\n",
            "Epoch 18309: train loss: 0.013979422859847546\n",
            "Epoch 18310: train loss: 0.013977643102407455\n",
            "Epoch 18311: train loss: 0.013975853100419044\n",
            "Epoch 18312: train loss: 0.013974074274301529\n",
            "Epoch 18313: train loss: 0.013972293585538864\n",
            "Epoch 18314: train loss: 0.013970519416034222\n",
            "Epoch 18315: train loss: 0.013968737795948982\n",
            "Epoch 18316: train loss: 0.013966964557766914\n",
            "Epoch 18317: train loss: 0.013965181075036526\n",
            "Epoch 18318: train loss: 0.013963405042886734\n",
            "Epoch 18319: train loss: 0.013961630873382092\n",
            "Epoch 18320: train loss: 0.013959850184619427\n",
            "Epoch 18321: train loss: 0.013958076015114784\n",
            "Epoch 18322: train loss: 0.013956296257674694\n",
            "Epoch 18323: train loss: 0.013954516500234604\n",
            "Epoch 18324: train loss: 0.013952747918665409\n",
            "Epoch 18325: train loss: 0.013950963504612446\n",
            "Epoch 18326: train loss: 0.013949193060398102\n",
            "Epoch 18327: train loss: 0.01394741702824831\n",
            "Epoch 18328: train loss: 0.01394564937800169\n",
            "Epoch 18329: train loss: 0.0139438696205616\n",
            "Epoch 18330: train loss: 0.013942097313702106\n",
            "Epoch 18331: train loss: 0.013940324075520039\n",
            "Epoch 18332: train loss: 0.013938543386757374\n",
            "Epoch 18333: train loss: 0.013936775736510754\n",
            "Epoch 18334: train loss: 0.01393500342965126\n",
            "Epoch 18335: train loss: 0.013933228328824043\n",
            "Epoch 18336: train loss: 0.013931459747254848\n",
            "Epoch 18337: train loss: 0.013929693028330803\n",
            "Epoch 18338: train loss: 0.01392792072147131\n",
            "Epoch 18339: train loss: 0.013926154002547264\n",
            "Epoch 18340: train loss: 0.01392438355833292\n",
            "Epoch 18341: train loss: 0.013922614976763725\n",
            "Epoch 18342: train loss: 0.01392084639519453\n",
            "Epoch 18343: train loss: 0.013919073157012463\n",
            "Epoch 18344: train loss: 0.013917309232056141\n",
            "Epoch 18345: train loss: 0.01391554158180952\n",
            "Epoch 18346: train loss: 0.01391377579420805\n",
            "Epoch 18347: train loss: 0.013912015594542027\n",
            "Epoch 18348: train loss: 0.01391025260090828\n",
            "Epoch 18349: train loss: 0.013908487744629383\n",
            "Epoch 18350: train loss: 0.013906721025705338\n",
            "Epoch 18351: train loss: 0.013904960826039314\n",
            "Epoch 18352: train loss: 0.013903200626373291\n",
            "Epoch 18353: train loss: 0.013901427388191223\n",
            "Epoch 18354: train loss: 0.013899672776460648\n",
            "Epoch 18355: train loss: 0.0138979097828269\n",
            "Epoch 18356: train loss: 0.01389614399522543\n",
            "Epoch 18357: train loss: 0.013894381932914257\n",
            "Epoch 18358: train loss: 0.013892621733248234\n",
            "Epoch 18359: train loss: 0.013890864327549934\n",
            "Epoch 18360: train loss: 0.013889102265238762\n",
            "Epoch 18361: train loss: 0.013887337408959866\n",
            "Epoch 18362: train loss: 0.01388558093458414\n",
            "Epoch 18363: train loss: 0.013883820734918118\n",
            "Epoch 18364: train loss: 0.013882062397897243\n",
            "Epoch 18365: train loss: 0.013880297541618347\n",
            "Epoch 18366: train loss: 0.013878542929887772\n",
            "Epoch 18367: train loss: 0.013876788318157196\n",
            "Epoch 18368: train loss: 0.013875034637749195\n",
            "Epoch 18369: train loss: 0.013873270712792873\n",
            "Epoch 18370: train loss: 0.013871519826352596\n",
            "Epoch 18371: train loss: 0.013869764283299446\n",
            "Epoch 18372: train loss: 0.013868003152310848\n",
            "Epoch 18373: train loss: 0.01386625599116087\n",
            "Epoch 18374: train loss: 0.013864494860172272\n",
            "Epoch 18375: train loss: 0.013862745836377144\n",
            "Epoch 18376: train loss: 0.013860992155969143\n",
            "Epoch 18377: train loss: 0.013859236612915993\n",
            "Epoch 18378: train loss: 0.013857490383088589\n",
            "Epoch 18379: train loss: 0.01385573297739029\n",
            "Epoch 18380: train loss: 0.01385398581624031\n",
            "Epoch 18381: train loss: 0.013852233998477459\n",
            "Epoch 18382: train loss: 0.013850482180714607\n",
            "Epoch 18383: train loss: 0.013848732225596905\n",
            "Epoch 18384: train loss: 0.01384698785841465\n",
            "Epoch 18385: train loss: 0.013845235109329224\n",
            "Epoch 18386: train loss: 0.01384348701685667\n",
            "Epoch 18387: train loss: 0.013841737993061543\n",
            "Epoch 18388: train loss: 0.013839988969266415\n",
            "Epoch 18389: train loss: 0.013838240876793861\n",
            "Epoch 18390: train loss: 0.01383649930357933\n",
            "Epoch 18391: train loss: 0.013834752142429352\n",
            "Epoch 18392: train loss: 0.013833005912601948\n",
            "Epoch 18393: train loss: 0.013831257820129395\n",
            "Epoch 18394: train loss: 0.013829510658979416\n",
            "Epoch 18395: train loss: 0.013827765360474586\n",
            "Epoch 18396: train loss: 0.013826016336679459\n",
            "Epoch 18397: train loss: 0.013824272900819778\n",
            "Epoch 18398: train loss: 0.013822528533637524\n",
            "Epoch 18399: train loss: 0.013820783235132694\n",
            "Epoch 18400: train loss: 0.013819042593240738\n",
            "Epoch 18401: train loss: 0.013817300088703632\n",
            "Epoch 18402: train loss: 0.013815552927553654\n",
            "Epoch 18403: train loss: 0.013813816010951996\n",
            "Epoch 18404: train loss: 0.013812070712447166\n",
            "Epoch 18405: train loss: 0.01381033193320036\n",
            "Epoch 18406: train loss: 0.01380858849734068\n",
            "Epoch 18407: train loss: 0.013806851580739021\n",
            "Epoch 18408: train loss: 0.01380511187016964\n",
            "Epoch 18409: train loss: 0.013803372159600258\n",
            "Epoch 18410: train loss: 0.013801628723740578\n",
            "Epoch 18411: train loss: 0.013799893669784069\n",
            "Epoch 18412: train loss: 0.013798155821859837\n",
            "Epoch 18413: train loss: 0.013796417973935604\n",
            "Epoch 18414: train loss: 0.013794684782624245\n",
            "Epoch 18415: train loss: 0.01379295065999031\n",
            "Epoch 18416: train loss: 0.013791210018098354\n",
            "Epoch 18417: train loss: 0.013789479620754719\n",
            "Epoch 18418: train loss: 0.01378774456679821\n",
            "Epoch 18419: train loss: 0.013786007650196552\n",
            "Epoch 18420: train loss: 0.013784279115498066\n",
            "Epoch 18421: train loss: 0.013782540336251259\n",
            "Epoch 18422: train loss: 0.0137808071449399\n",
            "Epoch 18423: train loss: 0.013779077678918839\n",
            "Epoch 18424: train loss: 0.013777341693639755\n",
            "Epoch 18425: train loss: 0.013775608502328396\n",
            "Epoch 18426: train loss: 0.013773875311017036\n",
            "Epoch 18427: train loss: 0.013772143982350826\n",
            "Epoch 18428: train loss: 0.013770411722362041\n",
            "Epoch 18429: train loss: 0.01376868225634098\n",
            "Epoch 18430: train loss: 0.013766951858997345\n",
            "Epoch 18431: train loss: 0.013765216805040836\n",
            "Epoch 18432: train loss: 0.0137634864076972\n",
            "Epoch 18433: train loss: 0.01376175694167614\n",
            "Epoch 18434: train loss: 0.013760033994913101\n",
            "Epoch 18435: train loss: 0.013758298940956593\n",
            "Epoch 18436: train loss: 0.01375657133758068\n",
            "Epoch 18437: train loss: 0.013754847459495068\n",
            "Epoch 18438: train loss: 0.013753114268183708\n",
            "Epoch 18439: train loss: 0.01375139132142067\n",
            "Epoch 18440: train loss: 0.013749669305980206\n",
            "Epoch 18441: train loss: 0.013747946359217167\n",
            "Epoch 18442: train loss: 0.013746215030550957\n",
            "Epoch 18443: train loss: 0.01374448649585247\n",
            "Epoch 18444: train loss: 0.013742763549089432\n",
            "Epoch 18445: train loss: 0.013741042464971542\n",
            "Epoch 18446: train loss: 0.013739315792918205\n",
            "Epoch 18447: train loss: 0.013737596571445465\n",
            "Epoch 18448: train loss: 0.013735874556005001\n",
            "Epoch 18449: train loss: 0.013734155334532261\n",
            "Epoch 18450: train loss: 0.013732429593801498\n",
            "Epoch 18451: train loss: 0.013730707578361034\n",
            "Epoch 18452: train loss: 0.013728990219533443\n",
            "Epoch 18453: train loss: 0.013727265410125256\n",
            "Epoch 18454: train loss: 0.01372554711997509\n",
            "Epoch 18455: train loss: 0.013723830692470074\n",
            "Epoch 18456: train loss: 0.01372210867702961\n",
            "Epoch 18457: train loss: 0.013720388524234295\n",
            "Epoch 18458: train loss: 0.013718672096729279\n",
            "Epoch 18459: train loss: 0.013716952875256538\n",
            "Epoch 18460: train loss: 0.013715236447751522\n",
            "Epoch 18461: train loss: 0.013713517226278782\n",
            "Epoch 18462: train loss: 0.013711799867451191\n",
            "Epoch 18463: train loss: 0.013710086233913898\n",
            "Epoch 18464: train loss: 0.01370836328715086\n",
            "Epoch 18465: train loss: 0.01370665617287159\n",
            "Epoch 18466: train loss: 0.0137049350887537\n",
            "Epoch 18467: train loss: 0.013703223317861557\n",
            "Epoch 18468: train loss: 0.01370151061564684\n",
            "Epoch 18469: train loss: 0.01369979977607727\n",
            "Epoch 18470: train loss: 0.013698081485927105\n",
            "Epoch 18471: train loss: 0.013696367852389812\n",
            "Epoch 18472: train loss: 0.013694655150175095\n",
            "Epoch 18473: train loss: 0.013692941516637802\n",
            "Epoch 18474: train loss: 0.013691232539713383\n",
            "Epoch 18475: train loss: 0.01368952076882124\n",
            "Epoch 18476: train loss: 0.01368781179189682\n",
            "Epoch 18477: train loss: 0.013686096295714378\n",
            "Epoch 18478: train loss: 0.01368438359349966\n",
            "Epoch 18479: train loss: 0.013682671822607517\n",
            "Epoch 18480: train loss: 0.013680963777005672\n",
            "Epoch 18481: train loss: 0.013679253868758678\n",
            "Epoch 18482: train loss: 0.013677547685801983\n",
            "Epoch 18483: train loss: 0.013675842434167862\n",
            "Epoch 18484: train loss: 0.013674130663275719\n",
            "Epoch 18485: train loss: 0.0136724216863513\n",
            "Epoch 18486: train loss: 0.013670719228684902\n",
            "Epoch 18487: train loss: 0.013669012114405632\n",
            "Epoch 18488: train loss: 0.013667304068803787\n",
            "Epoch 18489: train loss: 0.013665593229234219\n",
            "Epoch 18490: train loss: 0.01366389449685812\n",
            "Epoch 18491: train loss: 0.01366218738257885\n",
            "Epoch 18492: train loss: 0.013660483993589878\n",
            "Epoch 18493: train loss: 0.013658781535923481\n",
            "Epoch 18494: train loss: 0.013657072558999062\n",
            "Epoch 18495: train loss: 0.013655373826622963\n",
            "Epoch 18496: train loss: 0.013653669506311417\n",
            "Epoch 18497: train loss: 0.01365196704864502\n",
            "Epoch 18498: train loss: 0.013650268316268921\n",
            "Epoch 18499: train loss: 0.013648566789925098\n",
            "Epoch 18500: train loss: 0.01364686619490385\n",
            "Epoch 18501: train loss: 0.013645167462527752\n",
            "Epoch 18502: train loss: 0.01364346593618393\n",
            "Epoch 18503: train loss: 0.013641765341162682\n",
            "Epoch 18504: train loss: 0.013640066608786583\n",
            "Epoch 18505: train loss: 0.013638367876410484\n",
            "Epoch 18506: train loss: 0.013636675663292408\n",
            "Epoch 18507: train loss: 0.013634970411658287\n",
            "Epoch 18508: train loss: 0.013633272610604763\n",
            "Epoch 18509: train loss: 0.013631575740873814\n",
            "Epoch 18510: train loss: 0.013629878871142864\n",
            "Epoch 18511: train loss: 0.013628176413476467\n",
            "Epoch 18512: train loss: 0.01362648420035839\n",
            "Epoch 18513: train loss: 0.01362479105591774\n",
            "Epoch 18514: train loss: 0.013623091392219067\n",
            "Epoch 18515: train loss: 0.01362139917910099\n",
            "Epoch 18516: train loss: 0.013619706965982914\n",
            "Epoch 18517: train loss: 0.013618004508316517\n",
            "Epoch 18518: train loss: 0.013616313226521015\n",
            "Epoch 18519: train loss: 0.013614621013402939\n",
            "Epoch 18520: train loss: 0.013612932525575161\n",
            "Epoch 18521: train loss: 0.013611238449811935\n",
            "Epoch 18522: train loss: 0.01360954251140356\n",
            "Epoch 18523: train loss: 0.01360784750431776\n",
            "Epoch 18524: train loss: 0.013606159947812557\n",
            "Epoch 18525: train loss: 0.013604468666017056\n",
            "Epoch 18526: train loss: 0.013602773658931255\n",
            "Epoch 18527: train loss: 0.013601084239780903\n",
            "Epoch 18528: train loss: 0.013599391095340252\n",
            "Epoch 18529: train loss: 0.013597700744867325\n",
            "Epoch 18530: train loss: 0.013596020638942719\n",
            "Epoch 18531: train loss: 0.013594326563179493\n",
            "Epoch 18532: train loss: 0.013592645525932312\n",
            "Epoch 18533: train loss: 0.013590953312814236\n",
            "Epoch 18534: train loss: 0.013589268550276756\n",
            "Epoch 18535: train loss: 0.013587583787739277\n",
            "Epoch 18536: train loss: 0.013585895299911499\n",
            "Epoch 18537: train loss: 0.013584209606051445\n",
            "Epoch 18538: train loss: 0.013582526706159115\n",
            "Epoch 18539: train loss: 0.013580841943621635\n",
            "Epoch 18540: train loss: 0.013579153455793858\n",
            "Epoch 18541: train loss: 0.013577472418546677\n",
            "Epoch 18542: train loss: 0.013575786724686623\n",
            "Epoch 18543: train loss: 0.013574105687439442\n",
            "Epoch 18544: train loss: 0.013572419062256813\n",
            "Epoch 18545: train loss: 0.013570737093687057\n",
            "Epoch 18546: train loss: 0.013569051399827003\n",
            "Epoch 18547: train loss: 0.013567370362579823\n",
            "Epoch 18548: train loss: 0.013565693981945515\n",
            "Epoch 18549: train loss: 0.013564012944698334\n",
            "Epoch 18550: train loss: 0.013562330976128578\n",
            "Epoch 18551: train loss: 0.01356065459549427\n",
            "Epoch 18552: train loss: 0.013558981008827686\n",
            "Epoch 18553: train loss: 0.01355728879570961\n",
            "Epoch 18554: train loss: 0.013555609621107578\n",
            "Epoch 18555: train loss: 0.013553936965763569\n",
            "Epoch 18556: train loss: 0.013552249409258366\n",
            "Epoch 18557: train loss: 0.013550573959946632\n",
            "Epoch 18558: train loss: 0.013548900373280048\n",
            "Epoch 18559: train loss: 0.013547222130000591\n",
            "Epoch 18560: train loss: 0.013545540161430836\n",
            "Epoch 18561: train loss: 0.013543868437409401\n",
            "Epoch 18562: train loss: 0.01354218740016222\n",
            "Epoch 18563: train loss: 0.013540511019527912\n",
            "Epoch 18564: train loss: 0.013538841158151627\n",
            "Epoch 18565: train loss: 0.01353716105222702\n",
            "Epoch 18566: train loss: 0.01353549025952816\n",
            "Epoch 18567: train loss: 0.013533814810216427\n",
            "Epoch 18568: train loss: 0.013532138429582119\n",
            "Epoch 18569: train loss: 0.013530472293496132\n",
            "Epoch 18570: train loss: 0.013528792187571526\n",
            "Epoch 18571: train loss: 0.01352712232619524\n",
            "Epoch 18572: train loss: 0.013525450602173805\n",
            "Epoch 18573: train loss: 0.013523783534765244\n",
            "Epoch 18574: train loss: 0.01352210994809866\n",
            "Epoch 18575: train loss: 0.01352043729275465\n",
            "Epoch 18576: train loss: 0.01351876836270094\n",
            "Epoch 18577: train loss: 0.013517094776034355\n",
            "Epoch 18578: train loss: 0.013515428639948368\n",
            "Epoch 18579: train loss: 0.013513762503862381\n",
            "Epoch 18580: train loss: 0.013512087985873222\n",
            "Epoch 18581: train loss: 0.013510417193174362\n",
            "Epoch 18582: train loss: 0.013508753851056099\n",
            "Epoch 18583: train loss: 0.013507086783647537\n",
            "Epoch 18584: train loss: 0.013505418784916401\n",
            "Epoch 18585: train loss: 0.013503757305443287\n",
            "Epoch 18586: train loss: 0.013502092100679874\n",
            "Epoch 18587: train loss: 0.013500423170626163\n",
            "Epoch 18588: train loss: 0.013498757965862751\n",
            "Epoch 18589: train loss: 0.013497094623744488\n",
            "Epoch 18590: train loss: 0.013495431281626225\n",
            "Epoch 18591: train loss: 0.013493762351572514\n",
            "Epoch 18592: train loss: 0.013492097146809101\n",
            "Epoch 18593: train loss: 0.013490436598658562\n",
            "Epoch 18594: train loss: 0.013488765805959702\n",
            "Epoch 18595: train loss: 0.013487102463841438\n",
            "Epoch 18596: train loss: 0.013485446572303772\n",
            "Epoch 18597: train loss: 0.013483784161508083\n",
            "Epoch 18598: train loss: 0.01348212081938982\n",
            "Epoch 18599: train loss: 0.013480464927852154\n",
            "Epoch 18600: train loss: 0.013478804379701614\n",
            "Epoch 18601: train loss: 0.013477146625518799\n",
            "Epoch 18602: train loss: 0.013475487940013409\n",
            "Epoch 18603: train loss: 0.01347382739186287\n",
            "Epoch 18604: train loss: 0.013472161255776882\n",
            "Epoch 18605: train loss: 0.01347050629556179\n",
            "Epoch 18606: train loss: 0.013468846678733826\n",
            "Epoch 18607: train loss: 0.013467192649841309\n",
            "Epoch 18608: train loss: 0.013465533964335918\n",
            "Epoch 18609: train loss: 0.013463876210153103\n",
            "Epoch 18610: train loss: 0.013462216593325138\n",
            "Epoch 18611: train loss: 0.013460562564432621\n",
            "Epoch 18612: train loss: 0.013458899222314358\n",
            "Epoch 18613: train loss: 0.013457246124744415\n",
            "Epoch 18614: train loss: 0.0134555883705616\n",
            "Epoch 18615: train loss: 0.013453938998281956\n",
            "Epoch 18616: train loss: 0.013452275656163692\n",
            "Epoch 18617: train loss: 0.0134506244212389\n",
            "Epoch 18618: train loss: 0.013448971323668957\n",
            "Epoch 18619: train loss: 0.013447320088744164\n",
            "Epoch 18620: train loss: 0.013445664197206497\n",
            "Epoch 18621: train loss: 0.013444016687572002\n",
            "Epoch 18622: train loss: 0.01344236359000206\n",
            "Epoch 18623: train loss: 0.013440713286399841\n",
            "Epoch 18624: train loss: 0.013439060188829899\n",
            "Epoch 18625: train loss: 0.01343741174787283\n",
            "Epoch 18626: train loss: 0.013435755856335163\n",
            "Epoch 18627: train loss: 0.013434109278023243\n",
            "Epoch 18628: train loss: 0.013432458974421024\n",
            "Epoch 18629: train loss: 0.01343080960214138\n",
            "Epoch 18630: train loss: 0.013429158367216587\n",
            "Epoch 18631: train loss: 0.013427513651549816\n",
            "Epoch 18632: train loss: 0.013425868935883045\n",
            "Epoch 18633: train loss: 0.013424218632280827\n",
            "Epoch 18634: train loss: 0.01342257484793663\n",
            "Epoch 18635: train loss: 0.013420922681689262\n",
            "Epoch 18636: train loss: 0.01341928355395794\n",
            "Epoch 18637: train loss: 0.013417636975646019\n",
            "Epoch 18638: train loss: 0.01341599877923727\n",
            "Epoch 18639: train loss: 0.013414346612989902\n",
            "Epoch 18640: train loss: 0.013412704691290855\n",
            "Epoch 18641: train loss: 0.013411055319011211\n",
            "Epoch 18642: train loss: 0.013409409672021866\n",
            "Epoch 18643: train loss: 0.01340776588767767\n",
            "Epoch 18644: train loss: 0.013406125828623772\n",
            "Epoch 18645: train loss: 0.013404481112957\n",
            "Epoch 18646: train loss: 0.01340283453464508\n",
            "Epoch 18647: train loss: 0.013401197269558907\n",
            "Epoch 18648: train loss: 0.013399551622569561\n",
            "Epoch 18649: train loss: 0.01339790690690279\n",
            "Epoch 18650: train loss: 0.013396266847848892\n",
            "Epoch 18651: train loss: 0.013394618406891823\n",
            "Epoch 18652: train loss: 0.013392982073128223\n",
            "Epoch 18653: train loss: 0.013391340151429176\n",
            "Epoch 18654: train loss: 0.013389701955020428\n",
            "Epoch 18655: train loss: 0.013388066552579403\n",
            "Epoch 18656: train loss: 0.013386424630880356\n",
            "Epoch 18657: train loss: 0.013384788297116756\n",
            "Epoch 18658: train loss: 0.013383151963353157\n",
            "Epoch 18659: train loss: 0.01338151190429926\n",
            "Epoch 18660: train loss: 0.013379869051277637\n",
            "Epoch 18661: train loss: 0.013378237374126911\n",
            "Epoch 18662: train loss: 0.013376595452427864\n",
            "Epoch 18663: train loss: 0.013374959118664265\n",
            "Epoch 18664: train loss: 0.01337333582341671\n",
            "Epoch 18665: train loss: 0.013371695764362812\n",
            "Epoch 18666: train loss: 0.013370058499276638\n",
            "Epoch 18667: train loss: 0.013368424959480762\n",
            "Epoch 18668: train loss: 0.013366793282330036\n",
            "Epoch 18669: train loss: 0.01336516160517931\n",
            "Epoch 18670: train loss: 0.013363530859351158\n",
            "Epoch 18671: train loss: 0.013361898250877857\n",
            "Epoch 18672: train loss: 0.013360267505049706\n",
            "Epoch 18673: train loss: 0.01335863396525383\n",
            "Epoch 18674: train loss: 0.013357004150748253\n",
            "Epoch 18675: train loss: 0.013355371542274952\n",
            "Epoch 18676: train loss: 0.013353739865124226\n",
            "Epoch 18677: train loss: 0.013352112844586372\n",
            "Epoch 18678: train loss: 0.013350483030080795\n",
            "Epoch 18679: train loss: 0.013348852284252644\n",
            "Epoch 18680: train loss: 0.013347224332392216\n",
            "Epoch 18681: train loss: 0.01334559265524149\n",
            "Epoch 18682: train loss: 0.013343962840735912\n",
            "Epoch 18683: train loss: 0.01334233395755291\n",
            "Epoch 18684: train loss: 0.013340702280402184\n",
            "Epoch 18685: train loss: 0.013339072465896606\n",
            "Epoch 18686: train loss: 0.013337444514036179\n",
            "Epoch 18687: train loss: 0.013335815630853176\n",
            "Epoch 18688: train loss: 0.013334189541637897\n",
            "Epoch 18689: train loss: 0.013332569040358067\n",
            "Epoch 18690: train loss: 0.013330942019820213\n",
            "Epoch 18691: train loss: 0.01332931686192751\n",
            "Epoch 18692: train loss: 0.013327693566679955\n",
            "Epoch 18693: train loss: 0.013326064683496952\n",
            "Epoch 18694: train loss: 0.013324443250894547\n",
            "Epoch 18695: train loss: 0.013322818093001842\n",
            "Epoch 18696: train loss: 0.013321192003786564\n",
            "Epoch 18697: train loss: 0.013319568708539009\n",
            "Epoch 18698: train loss: 0.013317948207259178\n",
            "Epoch 18699: train loss: 0.013316328637301922\n",
            "Epoch 18700: train loss: 0.013314705342054367\n",
            "Epoch 18701: train loss: 0.013313083909451962\n",
            "Epoch 18702: train loss: 0.013311467133462429\n",
            "Epoch 18703: train loss: 0.013309846632182598\n",
            "Epoch 18704: train loss: 0.013308227993547916\n",
            "Epoch 18705: train loss: 0.013306613080203533\n",
            "Epoch 18706: train loss: 0.013304989784955978\n",
            "Epoch 18707: train loss: 0.01330337394028902\n",
            "Epoch 18708: train loss: 0.01330175157636404\n",
            "Epoch 18709: train loss: 0.013300135731697083\n",
            "Epoch 18710: train loss: 0.013298515230417252\n",
            "Epoch 18711: train loss: 0.013296899385750294\n",
            "Epoch 18712: train loss: 0.013295278884470463\n",
            "Epoch 18713: train loss: 0.013293666765093803\n",
            "Epoch 18714: train loss: 0.013292050920426846\n",
            "Epoch 18715: train loss: 0.01329043135046959\n",
            "Epoch 18716: train loss: 0.013288814574480057\n",
            "Epoch 18717: train loss: 0.0132871950045228\n",
            "Epoch 18718: train loss: 0.01328558661043644\n",
            "Epoch 18719: train loss: 0.01328396424651146\n",
            "Epoch 18720: train loss: 0.01328235398977995\n",
            "Epoch 18721: train loss: 0.013280732557177544\n",
            "Epoch 18722: train loss: 0.01327911950647831\n",
            "Epoch 18723: train loss: 0.013277512975037098\n",
            "Epoch 18724: train loss: 0.013275902718305588\n",
            "Epoch 18725: train loss: 0.013274284079670906\n",
            "Epoch 18726: train loss: 0.013272671960294247\n",
            "Epoch 18727: train loss: 0.013271063566207886\n",
            "Epoch 18728: train loss: 0.013269453309476376\n",
            "Epoch 18729: train loss: 0.013267838396131992\n",
            "Epoch 18730: train loss: 0.013266227208077908\n",
            "Epoch 18731: train loss: 0.013264616020023823\n",
            "Epoch 18732: train loss: 0.013263007625937462\n",
            "Epoch 18733: train loss: 0.013261402025818825\n",
            "Epoch 18734: train loss: 0.013259789906442165\n",
            "Epoch 18735: train loss: 0.013258181512355804\n",
            "Epoch 18736: train loss: 0.013256575912237167\n",
            "Epoch 18737: train loss: 0.01325497031211853\n",
            "Epoch 18738: train loss: 0.013253362849354744\n",
            "Epoch 18739: train loss: 0.01325176376849413\n",
            "Epoch 18740: train loss: 0.013250153511762619\n",
            "Epoch 18741: train loss: 0.013248546980321407\n",
            "Epoch 18742: train loss: 0.013246947899460793\n",
            "Epoch 18743: train loss: 0.013245342299342155\n",
            "Epoch 18744: train loss: 0.013243733905255795\n",
            "Epoch 18745: train loss: 0.013242131099104881\n",
            "Epoch 18746: train loss: 0.01324052456766367\n",
            "Epoch 18747: train loss: 0.013238922692835331\n",
            "Epoch 18748: train loss: 0.013237319886684418\n",
            "Epoch 18749: train loss: 0.013235715217888355\n",
            "Epoch 18750: train loss: 0.013234111480414867\n",
            "Epoch 18751: train loss: 0.013232508674263954\n",
            "Epoch 18752: train loss: 0.013230904005467892\n",
            "Epoch 18753: train loss: 0.013229307718575\n",
            "Epoch 18754: train loss: 0.013227704912424088\n",
            "Epoch 18755: train loss: 0.013226103037595749\n",
            "Epoch 18756: train loss: 0.013224499300122261\n",
            "Epoch 18757: train loss: 0.013222895562648773\n",
            "Epoch 18758: train loss: 0.013221295550465584\n",
            "Epoch 18759: train loss: 0.013219702057540417\n",
            "Epoch 18760: train loss: 0.013218093663454056\n",
            "Epoch 18761: train loss: 0.01321649644523859\n",
            "Epoch 18762: train loss: 0.013214902020990849\n",
            "Epoch 18763: train loss: 0.01321330200880766\n",
            "Epoch 18764: train loss: 0.013211701065301895\n",
            "Epoch 18765: train loss: 0.013210102915763855\n",
            "Epoch 18766: train loss: 0.013208511285483837\n",
            "Epoch 18767: train loss: 0.013206914067268372\n",
            "Epoch 18768: train loss: 0.013205315917730331\n",
            "Epoch 18769: train loss: 0.013203725218772888\n",
            "Epoch 18770: train loss: 0.013202127069234848\n",
            "Epoch 18771: train loss: 0.013200533576309681\n",
            "Epoch 18772: train loss: 0.013198945671319962\n",
            "Epoch 18773: train loss: 0.01319734938442707\n",
            "Epoch 18774: train loss: 0.013195756822824478\n",
            "Epoch 18775: train loss: 0.013194166123867035\n",
            "Epoch 18776: train loss: 0.013192575424909592\n",
            "Epoch 18777: train loss: 0.0131909791380167\n",
            "Epoch 18778: train loss: 0.013189388439059258\n",
            "Epoch 18779: train loss: 0.013187800534069538\n",
            "Epoch 18780: train loss: 0.013186203315854073\n",
            "Epoch 18781: train loss: 0.013184613548219204\n",
            "Epoch 18782: train loss: 0.013183021917939186\n",
            "Epoch 18783: train loss: 0.013181428425014019\n",
            "Epoch 18784: train loss: 0.013179843313992023\n",
            "Epoch 18785: train loss: 0.01317825261503458\n",
            "Epoch 18786: train loss: 0.01317666471004486\n",
            "Epoch 18787: train loss: 0.013175074011087418\n",
            "Epoch 18788: train loss: 0.0131734823808074\n",
            "Epoch 18789: train loss: 0.013171893544495106\n",
            "Epoch 18790: train loss: 0.013170304708182812\n",
            "Epoch 18791: train loss: 0.013168718665838242\n",
            "Epoch 18792: train loss: 0.013167129829525948\n",
            "Epoch 18793: train loss: 0.013165543787181377\n",
            "Epoch 18794: train loss: 0.013163954950869083\n",
            "Epoch 18795: train loss: 0.01316236425191164\n",
            "Epoch 18796: train loss: 0.013160782866179943\n",
            "Epoch 18797: train loss: 0.013159200549125671\n",
            "Epoch 18798: train loss: 0.013157613575458527\n",
            "Epoch 18799: train loss: 0.013156027533113956\n",
            "Epoch 18800: train loss: 0.013154447078704834\n",
            "Epoch 18801: train loss: 0.013152863830327988\n",
            "Epoch 18802: train loss: 0.013151280581951141\n",
            "Epoch 18803: train loss: 0.013149695470929146\n",
            "Epoch 18804: train loss: 0.013148120604455471\n",
            "Epoch 18805: train loss: 0.013146530836820602\n",
            "Epoch 18806: train loss: 0.013144955970346928\n",
            "Epoch 18807: train loss: 0.013143371790647507\n",
            "Epoch 18808: train loss: 0.013141794130206108\n",
            "Epoch 18809: train loss: 0.013140213675796986\n",
            "Epoch 18810: train loss: 0.013138636015355587\n",
            "Epoch 18811: train loss: 0.01313705462962389\n",
            "Epoch 18812: train loss: 0.013135474175214767\n",
            "Epoch 18813: train loss: 0.013133900240063667\n",
            "Epoch 18814: train loss: 0.013132321648299694\n",
            "Epoch 18815: train loss: 0.01313074305653572\n",
            "Epoch 18816: train loss: 0.013129161670804024\n",
            "Epoch 18817: train loss: 0.013127579353749752\n",
            "Epoch 18818: train loss: 0.013126005418598652\n",
            "Epoch 18819: train loss: 0.013124428689479828\n",
            "Epoch 18820: train loss: 0.013122853823006153\n",
            "Epoch 18821: train loss: 0.013121277093887329\n",
            "Epoch 18822: train loss: 0.013119697570800781\n",
            "Epoch 18823: train loss: 0.013118126429617405\n",
            "Epoch 18824: train loss: 0.013116548769176006\n",
            "Epoch 18825: train loss: 0.013114972040057182\n",
            "Epoch 18826: train loss: 0.01311338972300291\n",
            "Epoch 18827: train loss: 0.013111820444464684\n",
            "Epoch 18828: train loss: 0.013110240921378136\n",
            "Epoch 18829: train loss: 0.01310866978019476\n",
            "Epoch 18830: train loss: 0.013107091188430786\n",
            "Epoch 18831: train loss: 0.01310552004724741\n",
            "Epoch 18832: train loss: 0.01310394611209631\n",
            "Epoch 18833: train loss: 0.013102376833558083\n",
            "Epoch 18834: train loss: 0.013100795447826385\n",
            "Epoch 18835: train loss: 0.013099230825901031\n",
            "Epoch 18836: train loss: 0.013097663410007954\n",
            "Epoch 18837: train loss: 0.013096092268824577\n",
            "Epoch 18838: train loss: 0.013094514608383179\n",
            "Epoch 18839: train loss: 0.013092951849102974\n",
            "Epoch 18840: train loss: 0.01309138722717762\n",
            "Epoch 18841: train loss: 0.013089815154671669\n",
            "Epoch 18842: train loss: 0.01308824960142374\n",
            "Epoch 18843: train loss: 0.013086683116853237\n",
            "Epoch 18844: train loss: 0.013085114769637585\n",
            "Epoch 18845: train loss: 0.013083552941679955\n",
            "Epoch 18846: train loss: 0.013081985525786877\n",
            "Epoch 18847: train loss: 0.013080420903861523\n",
            "Epoch 18848: train loss: 0.013078858144581318\n",
            "Epoch 18849: train loss: 0.013077287934720516\n",
            "Epoch 18850: train loss: 0.013075723312795162\n",
            "Epoch 18851: train loss: 0.013074157759547234\n",
            "Epoch 18852: train loss: 0.013072594068944454\n",
            "Epoch 18853: train loss: 0.013071030378341675\n",
            "Epoch 18854: train loss: 0.013069464825093746\n",
            "Epoch 18855: train loss: 0.013067892752587795\n",
            "Epoch 18856: train loss: 0.013066332787275314\n",
            "Epoch 18857: train loss: 0.013064777478575706\n",
            "Epoch 18858: train loss: 0.01306320820003748\n",
            "Epoch 18859: train loss: 0.013061649166047573\n",
            "Epoch 18860: train loss: 0.013060079887509346\n",
            "Epoch 18861: train loss: 0.013058519922196865\n",
            "Epoch 18862: train loss: 0.013056951574981213\n",
            "Epoch 18863: train loss: 0.013055394403636456\n",
            "Epoch 18864: train loss: 0.01305383164435625\n",
            "Epoch 18865: train loss: 0.013052267953753471\n",
            "Epoch 18866: train loss: 0.013050707057118416\n",
            "Epoch 18867: train loss: 0.013049144297838211\n",
            "Epoch 18868: train loss: 0.013047589920461178\n",
            "Epoch 18869: train loss: 0.0130460225045681\n",
            "Epoch 18870: train loss: 0.013044469989836216\n",
            "Epoch 18871: train loss: 0.013042907230556011\n",
            "Epoch 18872: train loss: 0.013041350059211254\n",
            "Epoch 18873: train loss: 0.013039792887866497\n",
            "Epoch 18874: train loss: 0.01303823571652174\n",
            "Epoch 18875: train loss: 0.013036681339144707\n",
            "Epoch 18876: train loss: 0.013035126030445099\n",
            "Epoch 18877: train loss: 0.013033573515713215\n",
            "Epoch 18878: train loss: 0.013032016344368458\n",
            "Epoch 18879: train loss: 0.013030464760959148\n",
            "Epoch 18880: train loss: 0.01302890945225954\n",
            "Epoch 18881: train loss: 0.013027358800172806\n",
            "Epoch 18882: train loss: 0.013025806285440922\n",
            "Epoch 18883: train loss: 0.013024255633354187\n",
            "Epoch 18884: train loss: 0.013022702187299728\n",
            "Epoch 18885: train loss: 0.01302114687860012\n",
            "Epoch 18886: train loss: 0.01301959902048111\n",
            "Epoch 18887: train loss: 0.013018045574426651\n",
            "Epoch 18888: train loss: 0.013016493059694767\n",
            "Epoch 18889: train loss: 0.013014944270253181\n",
            "Epoch 18890: train loss: 0.013013392686843872\n",
            "Epoch 18891: train loss: 0.013011840172111988\n",
            "Epoch 18892: train loss: 0.013010289520025253\n",
            "Epoch 18893: train loss: 0.013008743524551392\n",
            "Epoch 18894: train loss: 0.013007193803787231\n",
            "Epoch 18895: train loss: 0.01300563383847475\n",
            "Epoch 18896: train loss: 0.01300408598035574\n",
            "Epoch 18897: train loss: 0.013002541847527027\n",
            "Epoch 18898: train loss: 0.01300098467618227\n",
            "Epoch 18899: train loss: 0.012999440543353558\n",
            "Epoch 18900: train loss: 0.012997890822589397\n",
            "Epoch 18901: train loss: 0.012996341101825237\n",
            "Epoch 18902: train loss: 0.0129947979003191\n",
            "Epoch 18903: train loss: 0.012993245385587215\n",
            "Epoch 18904: train loss: 0.012991693802177906\n",
            "Epoch 18905: train loss: 0.012990152463316917\n",
            "Epoch 18906: train loss: 0.012988601811230183\n",
            "Epoch 18907: train loss: 0.012987053953111172\n",
            "Epoch 18908: train loss: 0.01298550982028246\n",
            "Epoch 18909: train loss: 0.012983964756131172\n",
            "Epoch 18910: train loss: 0.012982421554625034\n",
            "Epoch 18911: train loss: 0.012980879284441471\n",
            "Epoch 18912: train loss: 0.012979337014257908\n",
            "Epoch 18913: train loss: 0.01297779381275177\n",
            "Epoch 18914: train loss: 0.01297625433653593\n",
            "Epoch 18915: train loss: 0.012974709272384644\n",
            "Epoch 18916: train loss: 0.012973170727491379\n",
            "Epoch 18917: train loss: 0.01297162938863039\n",
            "Epoch 18918: train loss: 0.012970086187124252\n",
            "Epoch 18919: train loss: 0.01296855416148901\n",
            "Epoch 18920: train loss: 0.012967009097337723\n",
            "Epoch 18921: train loss: 0.012965472415089607\n",
            "Epoch 18922: train loss: 0.012963931076228619\n",
            "Epoch 18923: train loss: 0.012962393462657928\n",
            "Epoch 18924: train loss: 0.012960855849087238\n",
            "Epoch 18925: train loss: 0.012959313578903675\n",
            "Epoch 18926: train loss: 0.012957777827978134\n",
            "Epoch 18927: train loss: 0.012956240214407444\n",
            "Epoch 18928: train loss: 0.01295470166951418\n",
            "Epoch 18929: train loss: 0.012953165918588638\n",
            "Epoch 18930: train loss: 0.012951630167663097\n",
            "Epoch 18931: train loss: 0.012950093485414982\n",
            "Epoch 18932: train loss: 0.012948557734489441\n",
            "Epoch 18933: train loss: 0.012947016395628452\n",
            "Epoch 18934: train loss: 0.012945479713380337\n",
            "Epoch 18935: train loss: 0.012943945825099945\n",
            "Epoch 18936: train loss: 0.012942411005496979\n",
            "Epoch 18937: train loss: 0.012940874323248863\n",
            "Epoch 18938: train loss: 0.012939339503645897\n",
            "Epoch 18939: train loss: 0.012937800958752632\n",
            "Epoch 18940: train loss: 0.012936266139149666\n",
            "Epoch 18941: train loss: 0.0129347313195467\n",
            "Epoch 18942: train loss: 0.012933199293911457\n",
            "Epoch 18943: train loss: 0.012931667268276215\n",
            "Epoch 18944: train loss: 0.012930135242640972\n",
            "Epoch 18945: train loss: 0.012928602285683155\n",
            "Epoch 18946: train loss: 0.012927067466080189\n",
            "Epoch 18947: train loss: 0.012925543822348118\n",
            "Epoch 18948: train loss: 0.01292401272803545\n",
            "Epoch 18949: train loss: 0.012922483496367931\n",
            "Epoch 18950: train loss: 0.012920952402055264\n",
            "Epoch 18951: train loss: 0.01291942410171032\n",
            "Epoch 18952: train loss: 0.012917905114591122\n",
            "Epoch 18953: train loss: 0.01291636936366558\n",
            "Epoch 18954: train loss: 0.012914841063320637\n",
            "Epoch 18955: train loss: 0.012913314625620842\n",
            "Epoch 18956: train loss: 0.012911784462630749\n",
            "Epoch 18957: train loss: 0.012910263612866402\n",
            "Epoch 18958: train loss: 0.012908734381198883\n",
            "Epoch 18959: train loss: 0.012907211668789387\n",
            "Epoch 18960: train loss: 0.012905677780508995\n",
            "Epoch 18961: train loss: 0.012904156930744648\n",
            "Epoch 18962: train loss: 0.012902634218335152\n",
            "Epoch 18963: train loss: 0.012901104986667633\n",
            "Epoch 18964: train loss: 0.012899577617645264\n",
            "Epoch 18965: train loss: 0.012898052111268044\n",
            "Epoch 18966: train loss: 0.01289653405547142\n",
            "Epoch 18967: train loss: 0.012895007617771626\n",
            "Epoch 18968: train loss: 0.012893480248749256\n",
            "Epoch 18969: train loss: 0.01289195753633976\n",
            "Epoch 18970: train loss: 0.012890431098639965\n",
            "Epoch 18971: train loss: 0.012888910248875618\n",
            "Epoch 18972: train loss: 0.012887387536466122\n",
            "Epoch 18973: train loss: 0.012885860167443752\n",
            "Epoch 18974: train loss: 0.012884341180324554\n",
            "Epoch 18975: train loss: 0.012882820330560207\n",
            "Epoch 18976: train loss: 0.012881296686828136\n",
            "Epoch 18977: train loss: 0.012879774905741215\n",
            "Epoch 18978: train loss: 0.012878251262009144\n",
            "Epoch 18979: train loss: 0.012876726686954498\n",
            "Epoch 18980: train loss: 0.01287520956248045\n",
            "Epoch 18981: train loss: 0.012873692438006401\n",
            "Epoch 18982: train loss: 0.012872171588242054\n",
            "Epoch 18983: train loss: 0.01287065539509058\n",
            "Epoch 18984: train loss: 0.012869137339293957\n",
            "Epoch 18985: train loss: 0.012867621146142483\n",
            "Epoch 18986: train loss: 0.012866110540926456\n",
            "Epoch 18987: train loss: 0.01286458969116211\n",
            "Epoch 18988: train loss: 0.012863073498010635\n",
            "Epoch 18989: train loss: 0.012861564755439758\n",
            "Epoch 18990: train loss: 0.01286004763096571\n",
            "Epoch 18991: train loss: 0.012858535163104534\n",
            "Epoch 18992: train loss: 0.012857024557888508\n",
            "Epoch 18993: train loss: 0.012855508364737034\n",
            "Epoch 18994: train loss: 0.012853995896875858\n",
            "Epoch 18995: train loss: 0.01285247690975666\n",
            "Epoch 18996: train loss: 0.012850961647927761\n",
            "Epoch 18997: train loss: 0.012849453836679459\n",
            "Epoch 18998: train loss: 0.012847939506173134\n",
            "Epoch 18999: train loss: 0.012846424244344234\n",
            "Epoch 19000: train loss: 0.012844919227063656\n",
            "Epoch 19001: train loss: 0.012843401171267033\n",
            "Epoch 19002: train loss: 0.01284189336001873\n",
            "Epoch 19003: train loss: 0.012840379029512405\n",
            "Epoch 19004: train loss: 0.012838860973715782\n",
            "Epoch 19005: train loss: 0.012837355956435204\n",
            "Epoch 19006: train loss: 0.012835846282541752\n",
            "Epoch 19007: train loss: 0.012834333814680576\n",
            "Epoch 19008: train loss: 0.012832816690206528\n",
            "Epoch 19009: train loss: 0.012831310741603374\n",
            "Epoch 19010: train loss: 0.012829795479774475\n",
            "Epoch 19011: train loss: 0.012828288599848747\n",
            "Epoch 19012: train loss: 0.012826775200664997\n",
            "Epoch 19013: train loss: 0.012825272046029568\n",
            "Epoch 19014: train loss: 0.012823759578168392\n",
            "Epoch 19015: train loss: 0.012822246178984642\n",
            "Epoch 19016: train loss: 0.012820741161704063\n",
            "Epoch 19017: train loss: 0.012819230556488037\n",
            "Epoch 19018: train loss: 0.012817722745239735\n",
            "Epoch 19019: train loss: 0.012816215865314007\n",
            "Epoch 19020: train loss: 0.012814710848033428\n",
            "Epoch 19021: train loss: 0.01281320583075285\n",
            "Epoch 19022: train loss: 0.01281170267611742\n",
            "Epoch 19023: train loss: 0.01281020138412714\n",
            "Epoch 19024: train loss: 0.01280870009213686\n",
            "Epoch 19025: train loss: 0.012807195074856281\n",
            "Epoch 19026: train loss: 0.012805690988898277\n",
            "Epoch 19027: train loss: 0.012804195284843445\n",
            "Epoch 19028: train loss: 0.012802698649466038\n",
            "Epoch 19029: train loss: 0.012801194563508034\n",
            "Epoch 19030: train loss: 0.012799686752259731\n",
            "Epoch 19031: train loss: 0.012798192910850048\n",
            "Epoch 19032: train loss: 0.012796689756214619\n",
            "Epoch 19033: train loss: 0.012795192189514637\n",
            "Epoch 19034: train loss: 0.012793689034879208\n",
            "Epoch 19035: train loss: 0.012792189605534077\n",
            "Epoch 19036: train loss: 0.012790690176188946\n",
            "Epoch 19037: train loss: 0.012789195403456688\n",
            "Epoch 19038: train loss: 0.012787694111466408\n",
            "Epoch 19039: train loss: 0.012786191888153553\n",
            "Epoch 19040: train loss: 0.012784690596163273\n",
            "Epoch 19041: train loss: 0.01278319675475359\n",
            "Epoch 19042: train loss: 0.012781700119376183\n",
            "Epoch 19043: train loss: 0.012780201621353626\n",
            "Epoch 19044: train loss: 0.012778702192008495\n",
            "Epoch 19045: train loss: 0.012777205556631088\n",
            "Epoch 19046: train loss: 0.012775706127285957\n",
            "Epoch 19047: train loss: 0.01277420949190855\n",
            "Epoch 19048: train loss: 0.012772712856531143\n",
            "Epoch 19049: train loss: 0.012771215289831161\n",
            "Epoch 19050: train loss: 0.01276971586048603\n",
            "Epoch 19051: train loss: 0.01276821456849575\n",
            "Epoch 19052: train loss: 0.012766718864440918\n",
            "Epoch 19053: train loss: 0.01276522409170866\n",
            "Epoch 19054: train loss: 0.012763728387653828\n",
            "Epoch 19055: train loss: 0.012762232683598995\n",
            "Epoch 19056: train loss: 0.012760738842189312\n",
            "Epoch 19057: train loss: 0.01275924313813448\n",
            "Epoch 19058: train loss: 0.012757753022015095\n",
            "Epoch 19059: train loss: 0.012756261974573135\n",
            "Epoch 19060: train loss: 0.012754770927131176\n",
            "Epoch 19061: train loss: 0.012753279879689217\n",
            "Epoch 19062: train loss: 0.01275179535150528\n",
            "Epoch 19063: train loss: 0.01275030616670847\n",
            "Epoch 19064: train loss: 0.012748816050589085\n",
            "Epoch 19065: train loss: 0.0127473259344697\n",
            "Epoch 19066: train loss: 0.012745841406285763\n",
            "Epoch 19067: train loss: 0.012744348496198654\n",
            "Epoch 19068: train loss: 0.012742864899337292\n",
            "Epoch 19069: train loss: 0.012741375714540482\n",
            "Epoch 19070: train loss: 0.012739886529743671\n",
            "Epoch 19071: train loss: 0.012738400138914585\n",
            "Epoch 19072: train loss: 0.012736912816762924\n",
            "Epoch 19073: train loss: 0.012735427357256413\n",
            "Epoch 19074: train loss: 0.012733935378491879\n",
            "Epoch 19075: train loss: 0.012732449918985367\n",
            "Epoch 19076: train loss: 0.012730961665511131\n",
            "Epoch 19077: train loss: 0.012729479931294918\n",
            "Epoch 19078: train loss: 0.012727989815175533\n",
            "Epoch 19079: train loss: 0.01272650621831417\n",
            "Epoch 19080: train loss: 0.012725017964839935\n",
            "Epoch 19081: train loss: 0.012723535299301147\n",
            "Epoch 19082: train loss: 0.01272205263376236\n",
            "Epoch 19083: train loss: 0.012720565311610699\n",
            "Epoch 19084: train loss: 0.012719081714749336\n",
            "Epoch 19085: train loss: 0.01271759532392025\n",
            "Epoch 19086: train loss: 0.012716108933091164\n",
            "Epoch 19087: train loss: 0.012714624404907227\n",
            "Epoch 19088: train loss: 0.012713140808045864\n",
            "Epoch 19089: train loss: 0.012711657211184502\n",
            "Epoch 19090: train loss: 0.012710176408290863\n",
            "Epoch 19091: train loss: 0.012708693742752075\n",
            "Epoch 19092: train loss: 0.012707206420600414\n",
            "Epoch 19093: train loss: 0.012705724686384201\n",
            "Epoch 19094: train loss: 0.012704241089522839\n",
            "Epoch 19095: train loss: 0.012702761217951775\n",
            "Epoch 19096: train loss: 0.012701280415058136\n",
            "Epoch 19097: train loss: 0.012699801474809647\n",
            "Epoch 19098: train loss: 0.01269832719117403\n",
            "Epoch 19099: train loss: 0.01269685011357069\n",
            "Epoch 19100: train loss: 0.01269537303596735\n",
            "Epoch 19101: train loss: 0.012693902477622032\n",
            "Epoch 19102: train loss: 0.012692421674728394\n",
            "Epoch 19103: train loss: 0.012690944597125053\n",
            "Epoch 19104: train loss: 0.012689466588199139\n",
            "Epoch 19105: train loss: 0.012687993235886097\n",
            "Epoch 19106: train loss: 0.01268652081489563\n",
            "Epoch 19107: train loss: 0.012685040943324566\n",
            "Epoch 19108: train loss: 0.012683575041592121\n",
            "Epoch 19109: train loss: 0.012682094238698483\n",
            "Epoch 19110: train loss: 0.01268062088638544\n",
            "Epoch 19111: train loss: 0.012679148465394974\n",
            "Epoch 19112: train loss: 0.012677669525146484\n",
            "Epoch 19113: train loss: 0.012676197104156017\n",
            "Epoch 19114: train loss: 0.012674722820520401\n",
            "Epoch 19115: train loss: 0.012673248536884785\n",
            "Epoch 19116: train loss: 0.012671778909862041\n",
            "Epoch 19117: train loss: 0.012670305557549\n",
            "Epoch 19118: train loss: 0.012668829411268234\n",
            "Epoch 19119: train loss: 0.012667357921600342\n",
            "Epoch 19120: train loss: 0.012665888294577599\n",
            "Epoch 19121: train loss: 0.012664414942264557\n",
            "Epoch 19122: train loss: 0.012662939727306366\n",
            "Epoch 19123: train loss: 0.012661467306315899\n",
            "Epoch 19124: train loss: 0.012659996747970581\n",
            "Epoch 19125: train loss: 0.012658524326980114\n",
            "Epoch 19126: train loss: 0.012657055631279945\n",
            "Epoch 19127: train loss: 0.012655584141612053\n",
            "Epoch 19128: train loss: 0.01265411265194416\n",
            "Epoch 19129: train loss: 0.012652643024921417\n",
            "Epoch 19130: train loss: 0.012651165015995502\n",
            "Epoch 19131: train loss: 0.012649700045585632\n",
            "Epoch 19132: train loss: 0.012648232281208038\n",
            "Epoch 19133: train loss: 0.012646760791540146\n",
            "Epoch 19134: train loss: 0.012645293958485126\n",
            "Epoch 19135: train loss: 0.012643826194107533\n",
            "Epoch 19136: train loss: 0.012642361223697662\n",
            "Epoch 19137: train loss: 0.012640897184610367\n",
            "Epoch 19138: train loss: 0.012639437802135944\n",
            "Epoch 19139: train loss: 0.012637974694371223\n",
            "Epoch 19140: train loss: 0.012636510655283928\n",
            "Epoch 19141: train loss: 0.01263504195958376\n",
            "Epoch 19142: train loss: 0.01263358537107706\n",
            "Epoch 19143: train loss: 0.012632118538022041\n",
            "Epoch 19144: train loss: 0.01263065729290247\n",
            "Epoch 19145: train loss: 0.012629196047782898\n",
            "Epoch 19146: train loss: 0.012627733871340752\n",
            "Epoch 19147: train loss: 0.012626267969608307\n",
            "Epoch 19148: train loss: 0.012624810449779034\n",
            "Epoch 19149: train loss: 0.012623345479369164\n",
            "Epoch 19150: train loss: 0.012621883302927017\n",
            "Epoch 19151: train loss: 0.012620425783097744\n",
            "Epoch 19152: train loss: 0.012618961744010448\n",
            "Epoch 19153: train loss: 0.012617500498890877\n",
            "Epoch 19154: train loss: 0.012616039253771305\n",
            "Epoch 19155: train loss: 0.012614578939974308\n",
            "Epoch 19156: train loss: 0.01261312048882246\n",
            "Epoch 19157: train loss: 0.012611656449735165\n",
            "Epoch 19158: train loss: 0.012610195204615593\n",
            "Epoch 19159: train loss: 0.01260873768478632\n",
            "Epoch 19160: train loss: 0.012607275508344173\n",
            "Epoch 19161: train loss: 0.012605816125869751\n",
            "Epoch 19162: train loss: 0.012604361400008202\n",
            "Epoch 19163: train loss: 0.01260290201753378\n",
            "Epoch 19164: train loss: 0.012601438909769058\n",
            "Epoch 19165: train loss: 0.012599985115230083\n",
            "Epoch 19166: train loss: 0.012598524801433086\n",
            "Epoch 19167: train loss: 0.012597071006894112\n",
            "Epoch 19168: train loss: 0.012595615349709988\n",
            "Epoch 19169: train loss: 0.012594153173267841\n",
            "Epoch 19170: train loss: 0.012592692859470844\n",
            "Epoch 19171: train loss: 0.012591238133609295\n",
            "Epoch 19172: train loss: 0.012589778751134872\n",
            "Epoch 19173: train loss: 0.012588328681886196\n",
            "Epoch 19174: train loss: 0.012586873956024647\n",
            "Epoch 19175: train loss: 0.012585427612066269\n",
            "Epoch 19176: train loss: 0.01258397102355957\n",
            "Epoch 19177: train loss: 0.012582524679601192\n",
            "Epoch 19178: train loss: 0.012581069022417068\n",
            "Epoch 19179: train loss: 0.012579621747136116\n",
            "Epoch 19180: train loss: 0.01257816981524229\n",
            "Epoch 19181: train loss: 0.012576722539961338\n",
            "Epoch 19182: train loss: 0.012575268745422363\n",
            "Epoch 19183: train loss: 0.012573815882205963\n",
            "Epoch 19184: train loss: 0.01257237046957016\n",
            "Epoch 19185: train loss: 0.012570918537676334\n",
            "Epoch 19186: train loss: 0.012569468468427658\n",
            "Epoch 19187: train loss: 0.012568016536533833\n",
            "Epoch 19188: train loss: 0.012566572986543179\n",
            "Epoch 19189: train loss: 0.012565127573907375\n",
            "Epoch 19190: train loss: 0.01256367564201355\n",
            "Epoch 19191: train loss: 0.012562226504087448\n",
            "Epoch 19192: train loss: 0.012560778297483921\n",
            "Epoch 19193: train loss: 0.012559328228235245\n",
            "Epoch 19194: train loss: 0.012557880952954292\n",
            "Epoch 19195: train loss: 0.012556436471641064\n",
            "Epoch 19196: train loss: 0.012554987333714962\n",
            "Epoch 19197: train loss: 0.012553539127111435\n",
            "Epoch 19198: train loss: 0.01255208719521761\n",
            "Epoch 19199: train loss: 0.012550645507872105\n",
            "Epoch 19200: train loss: 0.012549202889204025\n",
            "Epoch 19201: train loss: 0.012547754682600498\n",
            "Epoch 19202: train loss: 0.012546309269964695\n",
            "Epoch 19203: train loss: 0.012544860132038593\n",
            "Epoch 19204: train loss: 0.012543410994112492\n",
            "Epoch 19205: train loss: 0.012541970238089561\n",
            "Epoch 19206: train loss: 0.012540527619421482\n",
            "Epoch 19207: train loss: 0.012539071962237358\n",
            "Epoch 19208: train loss: 0.012537630274891853\n",
            "Epoch 19209: train loss: 0.012536185793578625\n",
            "Epoch 19210: train loss: 0.012534746900200844\n",
            "Epoch 19211: train loss: 0.01253330148756504\n",
            "Epoch 19212: train loss: 0.01253186259418726\n",
            "Epoch 19213: train loss: 0.012530420906841755\n",
            "Epoch 19214: train loss: 0.0125289810821414\n",
            "Epoch 19215: train loss: 0.012527539394795895\n",
            "Epoch 19216: train loss: 0.012526103295385838\n",
            "Epoch 19217: train loss: 0.012524668127298355\n",
            "Epoch 19218: train loss: 0.012523224577307701\n",
            "Epoch 19219: train loss: 0.012521790340542793\n",
            "Epoch 19220: train loss: 0.012520350515842438\n",
            "Epoch 19221: train loss: 0.01251891441643238\n",
            "Epoch 19222: train loss: 0.012517477385699749\n",
            "Epoch 19223: train loss: 0.01251604501157999\n",
            "Epoch 19224: train loss: 0.012514603324234486\n",
            "Epoch 19225: train loss: 0.012513162568211555\n",
            "Epoch 19226: train loss: 0.012511725537478924\n",
            "Epoch 19227: train loss: 0.012510295957326889\n",
            "Epoch 19228: train loss: 0.012508857995271683\n",
            "Epoch 19229: train loss: 0.012507425621151924\n",
            "Epoch 19230: train loss: 0.012505986727774143\n",
            "Epoch 19231: train loss: 0.012504547834396362\n",
            "Epoch 19232: train loss: 0.012503115460276604\n",
            "Epoch 19233: train loss: 0.012501676566898823\n",
            "Epoch 19234: train loss: 0.012500245124101639\n",
            "Epoch 19235: train loss: 0.012498805299401283\n",
            "Epoch 19236: train loss: 0.012497372925281525\n",
            "Epoch 19237: train loss: 0.01249594148248434\n",
            "Epoch 19238: train loss: 0.012494503520429134\n",
            "Epoch 19239: train loss: 0.012493068352341652\n",
            "Epoch 19240: train loss: 0.012491636909544468\n",
            "Epoch 19241: train loss: 0.01249020267277956\n",
            "Epoch 19242: train loss: 0.012488770298659801\n",
            "Epoch 19243: train loss: 0.012487338855862617\n",
            "Epoch 19244: train loss: 0.01248590275645256\n",
            "Epoch 19245: train loss: 0.0124844741076231\n",
            "Epoch 19246: train loss: 0.01248303335160017\n",
            "Epoch 19247: train loss: 0.012481607496738434\n",
            "Epoch 19248: train loss: 0.012480176985263824\n",
            "Epoch 19249: train loss: 0.012478744611144066\n",
            "Epoch 19250: train loss: 0.012477309443056583\n",
            "Epoch 19251: train loss: 0.012475885450839996\n",
            "Epoch 19252: train loss: 0.012474456802010536\n",
            "Epoch 19253: train loss: 0.012473033741116524\n",
            "Epoch 19254: train loss: 0.01247160229831934\n",
            "Epoch 19255: train loss: 0.012470178306102753\n",
            "Epoch 19256: train loss: 0.012468752451241016\n",
            "Epoch 19257: train loss: 0.012467323802411556\n",
            "Epoch 19258: train loss: 0.012465901672840118\n",
            "Epoch 19259: train loss: 0.012464473955333233\n",
            "Epoch 19260: train loss: 0.012463048100471497\n",
            "Epoch 19261: train loss: 0.012461626902222633\n",
            "Epoch 19262: train loss: 0.012460201978683472\n",
            "Epoch 19263: train loss: 0.012458774261176586\n",
            "Epoch 19264: train loss: 0.012457351200282574\n",
            "Epoch 19265: train loss: 0.012455924414098263\n",
            "Epoch 19266: train loss: 0.012454504147171974\n",
            "Epoch 19267: train loss: 0.012453081086277962\n",
            "Epoch 19268: train loss: 0.01245165430009365\n",
            "Epoch 19269: train loss: 0.012450234033167362\n",
            "Epoch 19270: train loss: 0.0124488091096282\n",
            "Epoch 19271: train loss: 0.012447386048734188\n",
            "Epoch 19272: train loss: 0.0124459657818079\n",
            "Epoch 19273: train loss: 0.012444538995623589\n",
            "Epoch 19274: train loss: 0.01244312059134245\n",
            "Epoch 19275: train loss: 0.012441694736480713\n",
            "Epoch 19276: train loss: 0.012440275400876999\n",
            "Epoch 19277: train loss: 0.01243885513395071\n",
            "Epoch 19278: train loss: 0.012437431141734123\n",
            "Epoch 19279: train loss: 0.012436010874807835\n",
            "Epoch 19280: train loss: 0.01243459153920412\n",
            "Epoch 19281: train loss: 0.012433169409632683\n",
            "Epoch 19282: train loss: 0.012431751005351543\n",
            "Epoch 19283: train loss: 0.012430327013134956\n",
            "Epoch 19284: train loss: 0.012428905814886093\n",
            "Epoch 19285: train loss: 0.01242748275399208\n",
            "Epoch 19286: train loss: 0.01242606807500124\n",
            "Epoch 19287: train loss: 0.012424646876752377\n",
            "Epoch 19288: train loss: 0.012423227541148663\n",
            "Epoch 19289: train loss: 0.012421808205544949\n",
            "Epoch 19290: train loss: 0.012420388869941235\n",
            "Epoch 19291: train loss: 0.012418975122272968\n",
            "Epoch 19292: train loss: 0.012417562305927277\n",
            "Epoch 19293: train loss: 0.012416140176355839\n",
            "Epoch 19294: train loss: 0.012414731085300446\n",
            "Epoch 19295: train loss: 0.012413312681019306\n",
            "Epoch 19296: train loss: 0.012411900795996189\n",
            "Epoch 19297: train loss: 0.012410493567585945\n",
            "Epoch 19298: train loss: 0.012409073300659657\n",
            "Epoch 19299: train loss: 0.012407665140926838\n",
            "Epoch 19300: train loss: 0.01240625511854887\n",
            "Epoch 19301: train loss: 0.01240483857691288\n",
            "Epoch 19302: train loss: 0.012403429485857487\n",
            "Epoch 19303: train loss: 0.01240201760083437\n",
            "Epoch 19304: train loss: 0.012400605715811253\n",
            "Epoch 19305: train loss: 0.01239919476211071\n",
            "Epoch 19306: train loss: 0.012397784739732742\n",
            "Epoch 19307: train loss: 0.0123963737860322\n",
            "Epoch 19308: train loss: 0.012394960038363934\n",
            "Epoch 19309: train loss: 0.012393545359373093\n",
            "Epoch 19310: train loss: 0.012392139993607998\n",
            "Epoch 19311: train loss: 0.012390733696520329\n",
            "Epoch 19312: train loss: 0.012389318086206913\n",
            "Epoch 19313: train loss: 0.012387909926474094\n",
            "Epoch 19314: train loss: 0.012386499904096127\n",
            "Epoch 19315: train loss: 0.01238508615642786\n",
            "Epoch 19316: train loss: 0.012383679859340191\n",
            "Epoch 19317: train loss: 0.012382272630929947\n",
            "Epoch 19318: train loss: 0.012380864471197128\n",
            "Epoch 19319: train loss: 0.012379450723528862\n",
            "Epoch 19320: train loss: 0.012378044426441193\n",
            "Epoch 19321: train loss: 0.012376640923321247\n",
            "Epoch 19322: train loss: 0.012375231832265854\n",
            "Epoch 19323: train loss: 0.012373819015920162\n",
            "Epoch 19324: train loss: 0.012372413650155067\n",
            "Epoch 19325: train loss: 0.012371002696454525\n",
            "Epoch 19326: train loss: 0.012369596399366856\n",
            "Epoch 19327: train loss: 0.01236819475889206\n",
            "Epoch 19328: train loss: 0.01236678659915924\n",
            "Epoch 19329: train loss: 0.012365380302071571\n",
            "Epoch 19330: train loss: 0.012363976798951626\n",
            "Epoch 19331: train loss: 0.012362572364509106\n",
            "Epoch 19332: train loss: 0.012361167930066586\n",
            "Epoch 19333: train loss: 0.01235976442694664\n",
            "Epoch 19334: train loss: 0.012358360923826694\n",
            "Epoch 19335: train loss: 0.012356961145997047\n",
            "Epoch 19336: train loss: 0.012355560436844826\n",
            "Epoch 19337: train loss: 0.012354160659015179\n",
            "Epoch 19338: train loss: 0.012352756224572659\n",
            "Epoch 19339: train loss: 0.01235135830938816\n",
            "Epoch 19340: train loss: 0.01234995387494564\n",
            "Epoch 19341: train loss: 0.012348550371825695\n",
            "Epoch 19342: train loss: 0.012347153387963772\n",
            "Epoch 19343: train loss: 0.0123457545414567\n",
            "Epoch 19344: train loss: 0.012344353832304478\n",
            "Epoch 19345: train loss: 0.012342954985797405\n",
            "Epoch 19346: train loss: 0.01234156172722578\n",
            "Epoch 19347: train loss: 0.012340161018073559\n",
            "Epoch 19348: train loss: 0.012338761240243912\n",
            "Epoch 19349: train loss: 0.01233735866844654\n",
            "Epoch 19350: train loss: 0.012335965409874916\n",
            "Epoch 19351: train loss: 0.01233456376940012\n",
            "Epoch 19352: train loss: 0.012333164922893047\n",
            "Epoch 19353: train loss: 0.012331771664321423\n",
            "Epoch 19354: train loss: 0.012330375611782074\n",
            "Epoch 19355: train loss: 0.012328972108662128\n",
            "Epoch 19356: train loss: 0.012327581644058228\n",
            "Epoch 19357: train loss: 0.012326175346970558\n",
            "Epoch 19358: train loss: 0.012324785813689232\n",
            "Epoch 19359: train loss: 0.012323392555117607\n",
            "Epoch 19360: train loss: 0.012321995571255684\n",
            "Epoch 19361: train loss: 0.01232059858739376\n",
            "Epoch 19362: train loss: 0.012319201603531837\n",
            "Epoch 19363: train loss: 0.01231780368834734\n",
            "Epoch 19364: train loss: 0.012316416017711163\n",
            "Epoch 19365: train loss: 0.01231501717120409\n",
            "Epoch 19366: train loss: 0.012313623912632465\n",
            "Epoch 19367: train loss: 0.012312229722738266\n",
            "Epoch 19368: train loss: 0.012310832738876343\n",
            "Epoch 19369: train loss: 0.012309443205595016\n",
            "Epoch 19370: train loss: 0.012308052740991116\n",
            "Epoch 19371: train loss: 0.012306655757129192\n",
            "Epoch 19372: train loss: 0.012305272743105888\n",
            "Epoch 19373: train loss: 0.012303881347179413\n",
            "Epoch 19374: train loss: 0.012302486225962639\n",
            "Epoch 19375: train loss: 0.012301097624003887\n",
            "Epoch 19376: train loss: 0.012299708090722561\n",
            "Epoch 19377: train loss: 0.012298318557441235\n",
            "Epoch 19378: train loss: 0.012296932749450207\n",
            "Epoch 19379: train loss: 0.012295540422201157\n",
            "Epoch 19380: train loss: 0.012294151820242405\n",
            "Epoch 19381: train loss: 0.012292767874896526\n",
            "Epoch 19382: train loss: 0.012291382066905499\n",
            "Epoch 19383: train loss: 0.012289993464946747\n",
            "Epoch 19384: train loss: 0.012288607656955719\n",
            "Epoch 19385: train loss: 0.012287219986319542\n",
            "Epoch 19386: train loss: 0.01228583138436079\n",
            "Epoch 19387: train loss: 0.012284445576369762\n",
            "Epoch 19388: train loss: 0.01228306069970131\n",
            "Epoch 19389: train loss: 0.012281671166419983\n",
            "Epoch 19390: train loss: 0.012280293740332127\n",
            "Epoch 19391: train loss: 0.012278902344405651\n",
            "Epoch 19392: train loss: 0.012277523055672646\n",
            "Epoch 19393: train loss: 0.01227613352239132\n",
            "Epoch 19394: train loss: 0.012274742126464844\n",
            "Epoch 19395: train loss: 0.012273360975086689\n",
            "Epoch 19396: train loss: 0.012271973304450512\n",
            "Epoch 19397: train loss: 0.012270591221749783\n",
            "Epoch 19398: train loss: 0.01226920634508133\n",
            "Epoch 19399: train loss: 0.01226782239973545\n",
            "Epoch 19400: train loss: 0.012266432866454124\n",
            "Epoch 19401: train loss: 0.01226505171507597\n",
            "Epoch 19402: train loss: 0.012263666838407516\n",
            "Epoch 19403: train loss: 0.012262281030416489\n",
            "Epoch 19404: train loss: 0.012260902673006058\n",
            "Epoch 19405: train loss: 0.01225951686501503\n",
            "Epoch 19406: train loss: 0.01225813664495945\n",
            "Epoch 19407: train loss: 0.012256755493581295\n",
            "Epoch 19408: train loss: 0.012255375273525715\n",
            "Epoch 19409: train loss: 0.01225399412214756\n",
            "Epoch 19410: train loss: 0.01225261390209198\n",
            "Epoch 19411: train loss: 0.012251234613358974\n",
            "Epoch 19412: train loss: 0.012249855324625969\n",
            "Epoch 19413: train loss: 0.012248476035892963\n",
            "Epoch 19414: train loss: 0.012247094884514809\n",
            "Epoch 19415: train loss: 0.012245718389749527\n",
            "Epoch 19416: train loss: 0.012244338169693947\n",
            "Epoch 19417: train loss: 0.012242966331541538\n",
            "Epoch 19418: train loss: 0.012241588905453682\n",
            "Epoch 19419: train loss: 0.012240211479365826\n",
            "Epoch 19420: train loss: 0.012238838709890842\n",
            "Epoch 19421: train loss: 0.012237463146448135\n",
            "Epoch 19422: train loss: 0.012236089445650578\n",
            "Epoch 19423: train loss: 0.012234714813530445\n",
            "Epoch 19424: train loss: 0.012233336456120014\n",
            "Epoch 19425: train loss: 0.012231961823999882\n",
            "Epoch 19426: train loss: 0.012230591848492622\n",
            "Epoch 19427: train loss: 0.012229220010340214\n",
            "Epoch 19428: train loss: 0.012227844446897507\n",
            "Epoch 19429: train loss: 0.012226467952132225\n",
            "Epoch 19430: train loss: 0.012225097976624966\n",
            "Epoch 19431: train loss: 0.01222372055053711\n",
            "Epoch 19432: train loss: 0.012222344987094402\n",
            "Epoch 19433: train loss: 0.012220971286296844\n",
            "Epoch 19434: train loss: 0.012219597585499287\n",
            "Epoch 19435: train loss: 0.012218226678669453\n",
            "Epoch 19436: train loss: 0.012216855771839619\n",
            "Epoch 19437: train loss: 0.012215481139719486\n",
            "Epoch 19438: train loss: 0.012214111164212227\n",
            "Epoch 19439: train loss: 0.012212736532092094\n",
            "Epoch 19440: train loss: 0.01221136748790741\n",
            "Epoch 19441: train loss: 0.012209990061819553\n",
            "Epoch 19442: train loss: 0.012208625674247742\n",
            "Epoch 19443: train loss: 0.01220724917948246\n",
            "Epoch 19444: train loss: 0.012205880135297775\n",
            "Epoch 19445: train loss: 0.012204510159790516\n",
            "Epoch 19446: train loss: 0.012203138321638107\n",
            "Epoch 19447: train loss: 0.012201766483485699\n",
            "Epoch 19448: train loss: 0.012200404889881611\n",
            "Epoch 19449: train loss: 0.012199030257761478\n",
            "Epoch 19450: train loss: 0.01219766866415739\n",
            "Epoch 19451: train loss: 0.012196295894682407\n",
            "Epoch 19452: train loss: 0.012194933369755745\n",
            "Epoch 19453: train loss: 0.012193561531603336\n",
            "Epoch 19454: train loss: 0.01219219621270895\n",
            "Epoch 19455: train loss: 0.012190830893814564\n",
            "Epoch 19456: train loss: 0.012189464643597603\n",
            "Epoch 19457: train loss: 0.012188100256025791\n",
            "Epoch 19458: train loss: 0.01218673586845398\n",
            "Epoch 19459: train loss: 0.012185366824269295\n",
            "Epoch 19460: train loss: 0.012184004299342632\n",
            "Epoch 19461: train loss: 0.012182642705738544\n",
            "Epoch 19462: train loss: 0.012181280180811882\n",
            "Epoch 19463: train loss: 0.012179912067949772\n",
            "Epoch 19464: train loss: 0.012178552336990833\n",
            "Epoch 19465: train loss: 0.012177187949419022\n",
            "Epoch 19466: train loss: 0.012175827287137508\n",
            "Epoch 19467: train loss: 0.012174461036920547\n",
            "Epoch 19468: train loss: 0.012173101305961609\n",
            "Epoch 19469: train loss: 0.012171739712357521\n",
            "Epoch 19470: train loss: 0.012170379050076008\n",
            "Epoch 19471: train loss: 0.012169010937213898\n",
            "Epoch 19472: train loss: 0.012167656794190407\n",
            "Epoch 19473: train loss: 0.01216629333794117\n",
            "Epoch 19474: train loss: 0.012164936400949955\n",
            "Epoch 19475: train loss: 0.012163578532636166\n",
            "Epoch 19476: train loss: 0.012162212282419205\n",
            "Epoch 19477: train loss: 0.012160856276750565\n",
            "Epoch 19478: train loss: 0.012159493751823902\n",
            "Epoch 19479: train loss: 0.012158134020864964\n",
            "Epoch 19480: train loss: 0.01215677335858345\n",
            "Epoch 19481: train loss: 0.012155415490269661\n",
            "Epoch 19482: train loss: 0.012154055759310722\n",
            "Epoch 19483: train loss: 0.01215269137173891\n",
            "Epoch 19484: train loss: 0.01215133722871542\n",
            "Epoch 19485: train loss: 0.012149983085691929\n",
            "Epoch 19486: train loss: 0.012148624286055565\n",
            "Epoch 19487: train loss: 0.012147261761128902\n",
            "Epoch 19488: train loss: 0.012145902030169964\n",
            "Epoch 19489: train loss: 0.012144546955823898\n",
            "Epoch 19490: train loss: 0.012143190018832684\n",
            "Epoch 19491: train loss: 0.012141834013164043\n",
            "Epoch 19492: train loss: 0.012140479870140553\n",
            "Epoch 19493: train loss: 0.012139126658439636\n",
            "Epoch 19494: train loss: 0.01213777344673872\n",
            "Epoch 19495: train loss: 0.012136418372392654\n",
            "Epoch 19496: train loss: 0.012135063298046589\n",
            "Epoch 19497: train loss: 0.012133707292377949\n",
            "Epoch 19498: train loss: 0.012132355011999607\n",
            "Epoch 19499: train loss: 0.012131000868976116\n",
            "Epoch 19500: train loss: 0.0121296476572752\n",
            "Epoch 19501: train loss: 0.012128299102187157\n",
            "Epoch 19502: train loss: 0.01212694775313139\n",
            "Epoch 19503: train loss: 0.012125593610107899\n",
            "Epoch 19504: train loss: 0.01212424598634243\n",
            "Epoch 19505: train loss: 0.012122894637286663\n",
            "Epoch 19506: train loss: 0.012121543288230896\n",
            "Epoch 19507: train loss: 0.012120192870497704\n",
            "Epoch 19508: train loss: 0.012118839658796787\n",
            "Epoch 19509: train loss: 0.012117494828999043\n",
            "Epoch 19510: train loss: 0.012116143479943275\n",
            "Epoch 19511: train loss: 0.012114792130887508\n",
            "Epoch 19512: train loss: 0.01211344450712204\n",
            "Epoch 19513: train loss: 0.012112095952033997\n",
            "Epoch 19514: train loss: 0.012110745534300804\n",
            "Epoch 19515: train loss: 0.012109390459954739\n",
            "Epoch 19516: train loss: 0.012108043767511845\n",
            "Epoch 19517: train loss: 0.012106696143746376\n",
            "Epoch 19518: train loss: 0.012105351313948631\n",
            "Epoch 19519: train loss: 0.012104004621505737\n",
            "Epoch 19520: train loss: 0.01210265327244997\n",
            "Epoch 19521: train loss: 0.012101302854716778\n",
            "Epoch 19522: train loss: 0.012099958024919033\n",
            "Epoch 19523: train loss: 0.01209860946983099\n",
            "Epoch 19524: train loss: 0.012097262777388096\n",
            "Epoch 19525: train loss: 0.012095917947590351\n",
            "Epoch 19526: train loss: 0.012094567529857159\n",
            "Epoch 19527: train loss: 0.012093224562704563\n",
            "Epoch 19528: train loss: 0.012091873213648796\n",
            "Epoch 19529: train loss: 0.012090533971786499\n",
            "Epoch 19530: train loss: 0.01208918821066618\n",
            "Epoch 19531: train loss: 0.012087841518223286\n",
            "Epoch 19532: train loss: 0.01208650041371584\n",
            "Epoch 19533: train loss: 0.01208515465259552\n",
            "Epoch 19534: train loss: 0.012083816342055798\n",
            "Epoch 19535: train loss: 0.012082469649612904\n",
            "Epoch 19536: train loss: 0.012081126682460308\n",
            "Epoch 19537: train loss: 0.012079779990017414\n",
            "Epoch 19538: train loss: 0.012078443542122841\n",
            "Epoch 19539: train loss: 0.01207710150629282\n",
            "Epoch 19540: train loss: 0.012075756676495075\n",
            "Epoch 19541: train loss: 0.012074419297277927\n",
            "Epoch 19542: train loss: 0.012073077261447906\n",
            "Epoch 19543: train loss: 0.012071742676198483\n",
            "Epoch 19544: train loss: 0.012070397846400738\n",
            "Epoch 19545: train loss: 0.01206906046718359\n",
            "Epoch 19546: train loss: 0.012067719362676144\n",
            "Epoch 19547: train loss: 0.012066380120813847\n",
            "Epoch 19548: train loss: 0.012065045535564423\n",
            "Epoch 19549: train loss: 0.012063703499734402\n",
            "Epoch 19550: train loss: 0.012062369845807552\n",
            "Epoch 19551: train loss: 0.012061027809977531\n",
            "Epoch 19552: train loss: 0.012059696950018406\n",
            "Epoch 19553: train loss: 0.01205835584551096\n",
            "Epoch 19554: train loss: 0.012057021260261536\n",
            "Epoch 19555: train loss: 0.012055683881044388\n",
            "Epoch 19556: train loss: 0.012054343707859516\n",
            "Epoch 19557: train loss: 0.012053010053932667\n",
            "Epoch 19558: train loss: 0.01205167081207037\n",
            "Epoch 19559: train loss: 0.012050333432853222\n",
            "Epoch 19560: train loss: 0.012048998847603798\n",
            "Epoch 19561: train loss: 0.012047658674418926\n",
            "Epoch 19562: train loss: 0.01204632967710495\n",
            "Epoch 19563: train loss: 0.012044992297887802\n",
            "Epoch 19564: train loss: 0.012043657712638378\n",
            "Epoch 19565: train loss: 0.012042323127388954\n",
            "Epoch 19566: train loss: 0.012040985748171806\n",
            "Epoch 19567: train loss: 0.012039653025567532\n",
            "Epoch 19568: train loss: 0.012038316577672958\n",
            "Epoch 19569: train loss: 0.012036982923746109\n",
            "Epoch 19570: train loss: 0.012035650201141834\n",
            "Epoch 19571: train loss: 0.01203431561589241\n",
            "Epoch 19572: train loss: 0.012032984755933285\n",
            "Epoch 19573: train loss: 0.012031651102006435\n",
            "Epoch 19574: train loss: 0.012030312791466713\n",
            "Epoch 19575: train loss: 0.01202898658812046\n",
            "Epoch 19576: train loss: 0.012027658522129059\n",
            "Epoch 19577: train loss: 0.012026328593492508\n",
            "Epoch 19578: train loss: 0.012024994008243084\n",
            "Epoch 19579: train loss: 0.012023665010929108\n",
            "Epoch 19580: train loss: 0.01202233973890543\n",
            "Epoch 19581: train loss: 0.012021004222333431\n",
            "Epoch 19582: train loss: 0.01201967615634203\n",
            "Epoch 19583: train loss: 0.012018345296382904\n",
            "Epoch 19584: train loss: 0.012017018161714077\n",
            "Epoch 19585: train loss: 0.012015690095722675\n",
            "Epoch 19586: train loss: 0.01201435923576355\n",
            "Epoch 19587: train loss: 0.012013031169772148\n",
            "Epoch 19588: train loss: 0.012011708691716194\n",
            "Epoch 19589: train loss: 0.012010381557047367\n",
            "Epoch 19590: train loss: 0.01200905442237854\n",
            "Epoch 19591: train loss: 0.012007725425064564\n",
            "Epoch 19592: train loss: 0.012006402015686035\n",
            "Epoch 19593: train loss: 0.012005076743662357\n",
            "Epoch 19594: train loss: 0.01200374960899353\n",
            "Epoch 19595: train loss: 0.012002428993582726\n",
            "Epoch 19596: train loss: 0.01200109999626875\n",
            "Epoch 19597: train loss: 0.01199977658689022\n",
            "Epoch 19598: train loss: 0.011998455040156841\n",
            "Epoch 19599: train loss: 0.011997123248875141\n",
            "Epoch 19600: train loss: 0.011995802633464336\n",
            "Epoch 19601: train loss: 0.011994481086730957\n",
            "Epoch 19602: train loss: 0.011993154883384705\n",
            "Epoch 19603: train loss: 0.011991828680038452\n",
            "Epoch 19604: train loss: 0.011990511789917946\n",
            "Epoch 19605: train loss: 0.01198918279260397\n",
            "Epoch 19606: train loss: 0.011987867765128613\n",
            "Epoch 19607: train loss: 0.011986537836492062\n",
            "Epoch 19608: train loss: 0.011985216289758682\n",
            "Epoch 19609: train loss: 0.011983896605670452\n",
            "Epoch 19610: train loss: 0.011982572264969349\n",
            "Epoch 19611: train loss: 0.011981246992945671\n",
            "Epoch 19612: train loss: 0.011979930102825165\n",
            "Epoch 19613: train loss: 0.011978602968156338\n",
            "Epoch 19614: train loss: 0.011977286078035831\n",
            "Epoch 19615: train loss: 0.0119759701192379\n",
            "Epoch 19616: train loss: 0.011974649503827095\n",
            "Epoch 19617: train loss: 0.011973329819738865\n",
            "Epoch 19618: train loss: 0.01197201106697321\n",
            "Epoch 19619: train loss: 0.011970688588917255\n",
            "Epoch 19620: train loss: 0.011969374492764473\n",
            "Epoch 19621: train loss: 0.011968052946031094\n",
            "Epoch 19622: train loss: 0.011966736055910587\n",
            "Epoch 19623: train loss: 0.011965416371822357\n",
            "Epoch 19624: train loss: 0.011964095756411552\n",
            "Epoch 19625: train loss: 0.011962775141000748\n",
            "Epoch 19626: train loss: 0.011961465701460838\n",
            "Epoch 19627: train loss: 0.011960145086050034\n",
            "Epoch 19628: train loss: 0.011958828195929527\n",
            "Epoch 19629: train loss: 0.01195751316845417\n",
            "Epoch 19630: train loss: 0.011956197209656239\n",
            "Epoch 19631: train loss: 0.01195488404482603\n",
            "Epoch 19632: train loss: 0.011953568086028099\n",
            "Epoch 19633: train loss: 0.011952253989875317\n",
            "Epoch 19634: train loss: 0.011950941756367683\n",
            "Epoch 19635: train loss: 0.011949625797569752\n",
            "Epoch 19636: train loss: 0.011948313564062119\n",
            "Epoch 19637: train loss: 0.01194700039923191\n",
            "Epoch 19638: train loss: 0.01194569282233715\n",
            "Epoch 19639: train loss: 0.011944374069571495\n",
            "Epoch 19640: train loss: 0.011943059973418713\n",
            "Epoch 19641: train loss: 0.011941749602556229\n",
            "Epoch 19642: train loss: 0.011940435506403446\n",
            "Epoch 19643: train loss: 0.011939124204218388\n",
            "Epoch 19644: train loss: 0.01193781103938818\n",
            "Epoch 19645: train loss: 0.011936498805880547\n",
            "Epoch 19646: train loss: 0.011935187503695488\n",
            "Epoch 19647: train loss: 0.011933878995478153\n",
            "Epoch 19648: train loss: 0.011932563036680222\n",
            "Epoch 19649: train loss: 0.011931252665817738\n",
            "Epoch 19650: train loss: 0.011929943226277828\n",
            "Epoch 19651: train loss: 0.011928629130125046\n",
            "Epoch 19652: train loss: 0.01192732248455286\n",
            "Epoch 19653: train loss: 0.011926010251045227\n",
            "Epoch 19654: train loss: 0.011924704536795616\n",
            "Epoch 19655: train loss: 0.011923388577997684\n",
            "Epoch 19656: train loss: 0.011922089383006096\n",
            "Epoch 19657: train loss: 0.011920779943466187\n",
            "Epoch 19658: train loss: 0.011919471435248852\n",
            "Epoch 19659: train loss: 0.011918162927031517\n",
            "Epoch 19660: train loss: 0.011916859075427055\n",
            "Epoch 19661: train loss: 0.011915546841919422\n",
            "Epoch 19662: train loss: 0.011914248578250408\n",
            "Epoch 19663: train loss: 0.011912936344742775\n",
            "Epoch 19664: train loss: 0.011911635287106037\n",
            "Epoch 19665: train loss: 0.011910329572856426\n",
            "Epoch 19666: train loss: 0.011909021995961666\n",
            "Epoch 19667: train loss: 0.011907712556421757\n",
            "Epoch 19668: train loss: 0.011906408704817295\n",
            "Epoch 19669: train loss: 0.011905103921890259\n",
            "Epoch 19670: train loss: 0.011903798207640648\n",
            "Epoch 19671: train loss: 0.011902491562068462\n",
            "Epoch 19672: train loss: 0.01190118957310915\n",
            "Epoch 19673: train loss: 0.011899885721504688\n",
            "Epoch 19674: train loss: 0.011898585595190525\n",
            "Epoch 19675: train loss: 0.01189727894961834\n",
            "Epoch 19676: train loss: 0.011895976029336452\n",
            "Epoch 19677: train loss: 0.01189467590302229\n",
            "Epoch 19678: train loss: 0.011893375776708126\n",
            "Epoch 19679: train loss: 0.011892070062458515\n",
            "Epoch 19680: train loss: 0.011890772730112076\n",
            "Epoch 19681: train loss: 0.011889471672475338\n",
            "Epoch 19682: train loss: 0.011888164095580578\n",
            "Epoch 19683: train loss: 0.011886866763234138\n",
            "Epoch 19684: train loss: 0.011885570362210274\n",
            "Epoch 19685: train loss: 0.011884267441928387\n",
            "Epoch 19686: train loss: 0.011882970109581947\n",
            "Epoch 19687: train loss: 0.011881669983267784\n",
            "Epoch 19688: train loss: 0.011880367994308472\n",
            "Epoch 19689: train loss: 0.011879065074026585\n",
            "Epoch 19690: train loss: 0.011877764947712421\n",
            "Epoch 19691: train loss: 0.011876469478011131\n",
            "Epoch 19692: train loss: 0.011875170283019543\n",
            "Epoch 19693: train loss: 0.011873871088027954\n",
            "Epoch 19694: train loss: 0.011872569099068642\n",
            "Epoch 19695: train loss: 0.011871271766722202\n",
            "Epoch 19696: train loss: 0.011869975365698338\n",
            "Epoch 19697: train loss: 0.011868677102029324\n",
            "Epoch 19698: train loss: 0.01186737697571516\n",
            "Epoch 19699: train loss: 0.011866082437336445\n",
            "Epoch 19700: train loss: 0.011864783242344856\n",
            "Epoch 19701: train loss: 0.011863487772643566\n",
            "Epoch 19702: train loss: 0.011862194165587425\n",
            "Epoch 19703: train loss: 0.01186089962720871\n",
            "Epoch 19704: train loss: 0.01185960415750742\n",
            "Epoch 19705: train loss: 0.011858312413096428\n",
            "Epoch 19706: train loss: 0.011857020668685436\n",
            "Epoch 19707: train loss: 0.011855721473693848\n",
            "Epoch 19708: train loss: 0.011854429729282856\n",
            "Epoch 19709: train loss: 0.011853131465613842\n",
            "Epoch 19710: train loss: 0.011851842515170574\n",
            "Epoch 19711: train loss: 0.011850548908114433\n",
            "Epoch 19712: train loss: 0.011849255301058292\n",
            "Epoch 19713: train loss: 0.011847958900034428\n",
            "Epoch 19714: train loss: 0.011846667155623436\n",
            "Epoch 19715: train loss: 0.011845378205180168\n",
            "Epoch 19716: train loss: 0.011844084598124027\n",
            "Epoch 19717: train loss: 0.011842796579003334\n",
            "Epoch 19718: train loss: 0.011841502040624619\n",
            "Epoch 19719: train loss: 0.0118402149528265\n",
            "Epoch 19720: train loss: 0.011838923208415508\n",
            "Epoch 19721: train loss: 0.011837631464004517\n",
            "Epoch 19722: train loss: 0.011836344376206398\n",
            "Epoch 19723: train loss: 0.011835051700472832\n",
            "Epoch 19724: train loss: 0.011833765543997288\n",
            "Epoch 19725: train loss: 0.01183247845619917\n",
            "Epoch 19726: train loss: 0.011831186711788177\n",
            "Epoch 19727: train loss: 0.01182989776134491\n",
            "Epoch 19728: train loss: 0.011828610673546791\n",
            "Epoch 19729: train loss: 0.011827319860458374\n",
            "Epoch 19730: train loss: 0.011826034635305405\n",
            "Epoch 19731: train loss: 0.011824743822216988\n",
            "Epoch 19732: train loss: 0.011823453940451145\n",
            "Epoch 19733: train loss: 0.011822168715298176\n",
            "Epoch 19734: train loss: 0.011820880696177483\n",
            "Epoch 19735: train loss: 0.011819591745734215\n",
            "Epoch 19736: train loss: 0.01181830558925867\n",
            "Epoch 19737: train loss: 0.011817019432783127\n",
            "Epoch 19738: train loss: 0.011815732344985008\n",
            "Epoch 19739: train loss: 0.011814446188509464\n",
            "Epoch 19740: train loss: 0.01181316003203392\n",
            "Epoch 19741: train loss: 0.011811873875558376\n",
            "Epoch 19742: train loss: 0.011810592375695705\n",
            "Epoch 19743: train loss: 0.01180930994451046\n",
            "Epoch 19744: train loss: 0.011808027513325214\n",
            "Epoch 19745: train loss: 0.011806740425527096\n",
            "Epoch 19746: train loss: 0.011805459856987\n",
            "Epoch 19747: train loss: 0.011804171837866306\n",
            "Epoch 19748: train loss: 0.011802892200648785\n",
            "Epoch 19749: train loss: 0.011801610700786114\n",
            "Epoch 19750: train loss: 0.011800326406955719\n",
            "Epoch 19751: train loss: 0.01179904118180275\n",
            "Epoch 19752: train loss: 0.01179775595664978\n",
            "Epoch 19753: train loss: 0.011796480044722557\n",
            "Epoch 19754: train loss: 0.011795193888247013\n",
            "Epoch 19755: train loss: 0.011793914251029491\n",
            "Epoch 19756: train loss: 0.011792631819844246\n",
            "Epoch 19757: train loss: 0.011791347526013851\n",
            "Epoch 19758: train loss: 0.011790072545409203\n",
            "Epoch 19759: train loss: 0.011788791976869106\n",
            "Epoch 19760: train loss: 0.011787506751716137\n",
            "Epoch 19761: train loss: 0.011786232702434063\n",
            "Epoch 19762: train loss: 0.011784948408603668\n",
            "Epoch 19763: train loss: 0.011783670634031296\n",
            "Epoch 19764: train loss: 0.011782393790781498\n",
            "Epoch 19765: train loss: 0.011781116016209126\n",
            "Epoch 19766: train loss: 0.011779840104281902\n",
            "Epoch 19767: train loss: 0.011778558604419231\n",
            "Epoch 19768: train loss: 0.011777283623814583\n",
            "Epoch 19769: train loss: 0.01177600584924221\n",
            "Epoch 19770: train loss: 0.011774729937314987\n",
            "Epoch 19771: train loss: 0.011773449368774891\n",
            "Epoch 19772: train loss: 0.011772171594202518\n",
            "Epoch 19773: train loss: 0.011770893819630146\n",
            "Epoch 19774: train loss: 0.01176962349563837\n",
            "Epoch 19775: train loss: 0.011768348515033722\n",
            "Epoch 19776: train loss: 0.011767067015171051\n",
            "Epoch 19777: train loss: 0.011765793897211552\n",
            "Epoch 19778: train loss: 0.01176451612263918\n",
            "Epoch 19779: train loss: 0.011763244867324829\n",
            "Epoch 19780: train loss: 0.011761964298784733\n",
            "Epoch 19781: train loss: 0.011760694906115532\n",
            "Epoch 19782: train loss: 0.011759420856833458\n",
            "Epoch 19783: train loss: 0.011758147738873959\n",
            "Epoch 19784: train loss: 0.011756867170333862\n",
            "Epoch 19785: train loss: 0.01175559964030981\n",
            "Epoch 19786: train loss: 0.011754324659705162\n",
            "Epoch 19787: train loss: 0.011753052473068237\n",
            "Epoch 19788: train loss: 0.011751783080399036\n",
            "Epoch 19789: train loss: 0.011750512756407261\n",
            "Epoch 19790: train loss: 0.011749242432415485\n",
            "Epoch 19791: train loss: 0.011747967451810837\n",
            "Epoch 19792: train loss: 0.011746698059141636\n",
            "Epoch 19793: train loss: 0.011745430529117584\n",
            "Epoch 19794: train loss: 0.01174415647983551\n",
            "Epoch 19795: train loss: 0.011742888018488884\n",
            "Epoch 19796: train loss: 0.01174161583185196\n",
            "Epoch 19797: train loss: 0.011740344576537609\n",
            "Epoch 19798: train loss: 0.011739079840481281\n",
            "Epoch 19799: train loss: 0.011737802065908909\n",
            "Epoch 19800: train loss: 0.011736538261175156\n",
            "Epoch 19801: train loss: 0.011735265143215656\n",
            "Epoch 19802: train loss: 0.011733997613191605\n",
            "Epoch 19803: train loss: 0.011732726357877254\n",
            "Epoch 19804: train loss: 0.011731458827853203\n",
            "Epoch 19805: train loss: 0.011730189435184002\n",
            "Epoch 19806: train loss: 0.011728922836482525\n",
            "Epoch 19807: train loss: 0.011727658100426197\n",
            "Epoch 19808: train loss: 0.01172639336436987\n",
            "Epoch 19809: train loss: 0.01172513049095869\n",
            "Epoch 19810: train loss: 0.011723856441676617\n",
            "Epoch 19811: train loss: 0.011722591705620289\n",
            "Epoch 19812: train loss: 0.011721324175596237\n",
            "Epoch 19813: train loss: 0.01172005943953991\n",
            "Epoch 19814: train loss: 0.011718795634806156\n",
            "Epoch 19815: train loss: 0.011717526242136955\n",
            "Epoch 19816: train loss: 0.011716262437403202\n",
            "Epoch 19817: train loss: 0.011714999563992023\n",
            "Epoch 19818: train loss: 0.011713734827935696\n",
            "Epoch 19819: train loss: 0.01171246636658907\n",
            "Epoch 19820: train loss: 0.011711206287145615\n",
            "Epoch 19821: train loss: 0.011709942482411861\n",
            "Epoch 19822: train loss: 0.011708677746355534\n",
            "Epoch 19823: train loss: 0.011707414872944355\n",
            "Epoch 19824: train loss: 0.01170615665614605\n",
            "Epoch 19825: train loss: 0.011704888194799423\n",
            "Epoch 19826: train loss: 0.011703628115355968\n",
            "Epoch 19827: train loss: 0.01170236337929964\n",
            "Epoch 19828: train loss: 0.011701101437211037\n",
            "Epoch 19829: train loss: 0.011699837632477283\n",
            "Epoch 19830: train loss: 0.011698580347001553\n",
            "Epoch 19831: train loss: 0.011697319336235523\n",
            "Epoch 19832: train loss: 0.01169605739414692\n",
            "Epoch 19833: train loss: 0.011694801039993763\n",
            "Epoch 19834: train loss: 0.011693534441292286\n",
            "Epoch 19835: train loss: 0.011692275293171406\n",
            "Epoch 19836: train loss: 0.011691022664308548\n",
            "Epoch 19837: train loss: 0.01168975792825222\n",
            "Epoch 19838: train loss: 0.01168850064277649\n",
            "Epoch 19839: train loss: 0.011687240563333035\n",
            "Epoch 19840: train loss: 0.01168598048388958\n",
            "Epoch 19841: train loss: 0.011684725061058998\n",
            "Epoch 19842: train loss: 0.011683467775583267\n",
            "Epoch 19843: train loss: 0.011682209558784962\n",
            "Epoch 19844: train loss: 0.011680949479341507\n",
            "Epoch 19845: train loss: 0.011679694056510925\n",
            "Epoch 19846: train loss: 0.011678433045744896\n",
            "Epoch 19847: train loss: 0.011677179485559464\n",
            "Epoch 19848: train loss: 0.011675924994051456\n",
            "Epoch 19849: train loss: 0.011674662120640278\n",
            "Epoch 19850: train loss: 0.01167340762913227\n",
            "Epoch 19851: train loss: 0.011672158725559711\n",
            "Epoch 19852: train loss: 0.011670897714793682\n",
            "Epoch 19853: train loss: 0.011669641360640526\n",
            "Epoch 19854: train loss: 0.011668391525745392\n",
            "Epoch 19855: train loss: 0.011667130514979362\n",
            "Epoch 19856: train loss: 0.011665879748761654\n",
            "Epoch 19857: train loss: 0.011664621531963348\n",
            "Epoch 19858: train loss: 0.011663376353681087\n",
            "Epoch 19859: train loss: 0.011662125587463379\n",
            "Epoch 19860: train loss: 0.011660868301987648\n",
            "Epoch 19861: train loss: 0.011659611947834492\n",
            "Epoch 19862: train loss: 0.01165835652500391\n",
            "Epoch 19863: train loss: 0.01165711134672165\n",
            "Epoch 19864: train loss: 0.011655854061245918\n",
            "Epoch 19865: train loss: 0.011654606088995934\n",
            "Epoch 19866: train loss: 0.011653349734842777\n",
            "Epoch 19867: train loss: 0.011652102693915367\n",
            "Epoch 19868: train loss: 0.011650847271084785\n",
            "Epoch 19869: train loss: 0.011649597436189651\n",
            "Epoch 19870: train loss: 0.011648348532617092\n",
            "Epoch 19871: train loss: 0.011647099629044533\n",
            "Epoch 19872: train loss: 0.011645843274891376\n",
            "Epoch 19873: train loss: 0.011644593439996243\n",
            "Epoch 19874: train loss: 0.011643347330391407\n",
            "Epoch 19875: train loss: 0.011642096564173698\n",
            "Epoch 19876: train loss: 0.011640850454568863\n",
            "Epoch 19877: train loss: 0.011639602482318878\n",
            "Epoch 19878: train loss: 0.01163835171610117\n",
            "Epoch 19879: train loss: 0.011637103743851185\n",
            "Epoch 19880: train loss: 0.011635860428214073\n",
            "Epoch 19881: train loss: 0.011634612455964088\n",
            "Epoch 19882: train loss: 0.011633366346359253\n",
            "Epoch 19883: train loss: 0.011632117442786694\n",
            "Epoch 19884: train loss: 0.011630866676568985\n",
            "Epoch 19885: train loss: 0.01162962056696415\n",
            "Epoch 19886: train loss: 0.01162837352603674\n",
            "Epoch 19887: train loss: 0.011627128347754478\n",
            "Epoch 19888: train loss: 0.011625878512859344\n",
            "Epoch 19889: train loss: 0.011624635197222233\n",
            "Epoch 19890: train loss: 0.011623390018939972\n",
            "Epoch 19891: train loss: 0.011622141115367413\n",
            "Epoch 19892: train loss: 0.01162089966237545\n",
            "Epoch 19893: train loss: 0.011619651690125465\n",
            "Epoch 19894: train loss: 0.011618404649198055\n",
            "Epoch 19895: train loss: 0.011617164127528667\n",
            "Epoch 19896: train loss: 0.011615915223956108\n",
            "Epoch 19897: train loss: 0.01161467470228672\n",
            "Epoch 19898: train loss: 0.011613431386649609\n",
            "Epoch 19899: train loss: 0.011612189002335072\n",
            "Epoch 19900: train loss: 0.01161094382405281\n",
            "Epoch 19901: train loss: 0.011609702371060848\n",
            "Epoch 19902: train loss: 0.011608459986746311\n",
            "Epoch 19903: train loss: 0.0116072166711092\n",
            "Epoch 19904: train loss: 0.011605972424149513\n",
            "Epoch 19905: train loss: 0.01160473097115755\n",
            "Epoch 19906: train loss: 0.011603495106101036\n",
            "Epoch 19907: train loss: 0.011602247133851051\n",
            "Epoch 19908: train loss: 0.01160101406276226\n",
            "Epoch 19909: train loss: 0.011599769815802574\n",
            "Epoch 19910: train loss: 0.011598528362810612\n",
            "Epoch 19911: train loss: 0.011597287841141224\n",
            "Epoch 19912: train loss: 0.01159604825079441\n",
            "Epoch 19913: train loss: 0.011594811454415321\n",
            "Epoch 19914: train loss: 0.01159356813877821\n",
            "Epoch 19915: train loss: 0.011592332273721695\n",
            "Epoch 19916: train loss: 0.011591092683374882\n",
            "Epoch 19917: train loss: 0.011589854955673218\n",
            "Epoch 19918: train loss: 0.01158861257135868\n",
            "Epoch 19919: train loss: 0.011587376706302166\n",
            "Epoch 19920: train loss: 0.011586138978600502\n",
            "Epoch 19921: train loss: 0.011584902182221413\n",
            "Epoch 19922: train loss: 0.011583666317164898\n",
            "Epoch 19923: train loss: 0.011582431383430958\n",
            "Epoch 19924: train loss: 0.011581196449697018\n",
            "Epoch 19925: train loss: 0.011579957790672779\n",
            "Epoch 19926: train loss: 0.011578728444874287\n",
            "Epoch 19927: train loss: 0.011577488854527473\n",
            "Epoch 19928: train loss: 0.011576253920793533\n",
            "Epoch 19929: train loss: 0.011575020849704742\n",
            "Epoch 19930: train loss: 0.011573784053325653\n",
            "Epoch 19931: train loss: 0.011572550050914288\n",
            "Epoch 19932: train loss: 0.011571316979825497\n",
            "Epoch 19933: train loss: 0.011570077389478683\n",
            "Epoch 19934: train loss: 0.011568842455744743\n",
            "Epoch 19935: train loss: 0.011567612178623676\n",
            "Epoch 19936: train loss: 0.011566373519599438\n",
            "Epoch 19937: train loss: 0.011565144173800945\n",
            "Epoch 19938: train loss: 0.011563907377421856\n",
            "Epoch 19939: train loss: 0.01156267523765564\n",
            "Epoch 19940: train loss: 0.011561434715986252\n",
            "Epoch 19941: train loss: 0.011560209095478058\n",
            "Epoch 19942: train loss: 0.011558971367776394\n",
            "Epoch 19943: train loss: 0.011557742953300476\n",
            "Epoch 19944: train loss: 0.011556516401469707\n",
            "Epoch 19945: train loss: 0.011555279605090618\n",
            "Epoch 19946: train loss: 0.01155405305325985\n",
            "Epoch 19947: train loss: 0.011552820913493633\n",
            "Epoch 19948: train loss: 0.011551588773727417\n",
            "Epoch 19949: train loss: 0.0115503566339612\n",
            "Epoch 19950: train loss: 0.011549125425517559\n",
            "Epoch 19951: train loss: 0.011547897011041641\n",
            "Epoch 19952: train loss: 0.011546670459210873\n",
            "Epoch 19953: train loss: 0.011545435525476933\n",
            "Epoch 19954: train loss: 0.011544209904968739\n",
            "Epoch 19955: train loss: 0.01154298149049282\n",
            "Epoch 19956: train loss: 0.011541752144694328\n",
            "Epoch 19957: train loss: 0.011540526524186134\n",
            "Epoch 19958: train loss: 0.011539297178387642\n",
            "Epoch 19959: train loss: 0.011538066901266575\n",
            "Epoch 19960: train loss: 0.011536839418113232\n",
            "Epoch 19961: train loss: 0.011535607278347015\n",
            "Epoch 19962: train loss: 0.011534378863871098\n",
            "Epoch 19963: train loss: 0.011533156037330627\n",
            "Epoch 19964: train loss: 0.011531929485499859\n",
            "Epoch 19965: train loss: 0.011530702002346516\n",
            "Epoch 19966: train loss: 0.011529481038451195\n",
            "Epoch 19967: train loss: 0.011528251692652702\n",
            "Epoch 19968: train loss: 0.011527027934789658\n",
            "Epoch 19969: train loss: 0.011525806039571762\n",
            "Epoch 19970: train loss: 0.011524583213031292\n",
            "Epoch 19971: train loss: 0.011523360386490822\n",
            "Epoch 19972: train loss: 0.011522131972014904\n",
            "Epoch 19973: train loss: 0.011520909145474434\n",
            "Epoch 19974: train loss: 0.011519684456288815\n",
            "Epoch 19975: train loss: 0.01151846069842577\n",
            "Epoch 19976: train loss: 0.011517236940562725\n",
            "Epoch 19977: train loss: 0.01151601504534483\n",
            "Epoch 19978: train loss: 0.011514793150126934\n",
            "Epoch 19979: train loss: 0.01151356752961874\n",
            "Epoch 19980: train loss: 0.011512345634400845\n",
            "Epoch 19981: train loss: 0.011511123739182949\n",
            "Epoch 19982: train loss: 0.011509900912642479\n",
            "Epoch 19983: train loss: 0.011508681811392307\n",
            "Epoch 19984: train loss: 0.011507454328238964\n",
            "Epoch 19985: train loss: 0.011506233364343643\n",
            "Epoch 19986: train loss: 0.011505010537803173\n",
            "Epoch 19987: train loss: 0.011503792367875576\n",
            "Epoch 19988: train loss: 0.011502576060593128\n",
            "Epoch 19989: train loss: 0.011501353234052658\n",
            "Epoch 19990: train loss: 0.011500130407512188\n",
            "Epoch 19991: train loss: 0.011498912237584591\n",
            "Epoch 19992: train loss: 0.01149769127368927\n",
            "Epoch 19993: train loss: 0.011496474035084248\n",
            "Epoch 19994: train loss: 0.0114952577278018\n",
            "Epoch 19995: train loss: 0.011494038626551628\n",
            "Epoch 19996: train loss: 0.011492813006043434\n",
            "Epoch 19997: train loss: 0.01149159949272871\n",
            "Epoch 19998: train loss: 0.011490379460155964\n",
            "Epoch 19999: train loss: 0.011489161290228367\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "#train\n",
        "y_pred = model(x_train) #predict\n",
        "y_pred=(y_pred>0.5).int().flatten() #argmax class lable\n",
        "train_acc =torch.sum(y_pred == y_train.int())/ y_train.shape[0]\n",
        "print(\"train ACC: \",train_acc.float())"
      ],
      "metadata": {
        "id": "yXh19WH5JezP",
        "outputId": "49fc74e0-847a-45ae-de24-6ac45dd5f4f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train ACC:  tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "y_pred = model(x_test) #predict\n",
        "y_pred = (y_pred>0.5).int().flatten() #argmax class lable\n",
        "test_acc = torch.sum(y_pred == y_test.int()) / y_test.shape[0]\n",
        "print(\"test ACC: \",test_acc.float())"
      ],
      "metadata": {
        "id": "1lNr1QnpKopY",
        "outputId": "fe67e223-4bc3-4ba1-b9d3-4d9684ee31b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test ACC:  tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(x_test[:,0], x_test[:,1], c=y_pred)\n",
        "plt.colorbar()"
      ],
      "metadata": {
        "id": "XzxvYA1oKp64",
        "outputId": "02c8ae62-1b1f-4fa6-ac86-e16164bb1835",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.colorbar.Colorbar at 0x7d1e4c7ef650>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Assignment_Basic_MLP_in_Pytorch.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}