{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8Cwq-uPYvpa"
      },
      "source": [
        "# Build basic 2-Layer MLP to solve the xor-Problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 471,
      "metadata": {
        "id": "K4FDsqgaYvps"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs #for data generatio\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 472,
      "metadata": {
        "id": "2x1wijYZYvpu"
      },
      "outputs": [],
      "source": [
        "X, y = make_blobs(n_samples=200, n_features=2, cluster_std=.1\n",
        "                  ,centers= [(1,1), (1,0), (0,0),(0,1)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 473,
      "metadata": {
        "id": "aoAh4a4KYvpv"
      },
      "outputs": [],
      "source": [
        "#make blobs into binary problem\n",
        "y[y==2]=0\n",
        "y[y==3]=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 474,
      "metadata": {
        "id": "RMPaCrKBYvpw",
        "outputId": "91812e84-4a3d-4654-ca72-4338710dc7e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7e78ccc36e10>"
            ]
          },
          "metadata": {},
          "execution_count": 474
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.scatter(X[:,0],X[:,1],c=y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Q9Y4g0NYvpy"
      },
      "source": [
        "## Steps:\n",
        "* build train and test sets\n",
        "* write MLP class in Pytorch with two layers with adjustable number of perceptrons\n",
        "* use nn.linear and nn.Sigmoid() units\n",
        "* train your model\n",
        "* test your model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "\n",
        "x_train = torch.FloatTensor(x_train)\n",
        "x_test = torch.FloatTensor(x_test)\n",
        "y_train = torch.FloatTensor(y_train)\n",
        "y_test = torch.FloatTensor(y_test)"
      ],
      "metadata": {
        "id": "piXsTVTrZssQ"
      },
      "execution_count": 475,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "id": "MGwlyPvALJvY",
        "outputId": "946d40a7-4202-4ad6-ec23-98ba4a832fb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 476,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
              "        1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
              "        0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0.,\n",
              "        0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
              "        0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1.,\n",
              "        1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0.,\n",
              "        0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
              "        1., 0., 0., 1., 1., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 476
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "class MLP(torch.nn.Module): #all nets inherit from nn.Module\n",
        "    def __init__(self): #define layer types\n",
        "        super(MLP, self).__init__()\n",
        "        #hidden_sizes=[64, 32]\n",
        "        #self.fc1 = torch.nn.Linear(2,2,bias=False) # Perceptron is single neuron \"fully connected\" (fc) -> linear unit with 2 inputs and 1 output\n",
        "        #self.fc2 = torch.nn.Linear(2,2,bias=False)\n",
        "        #self.fc3 = torch.nn.Linear(2,1,bias=False) # Perceptron is single neuron \"fully connected\" (fc) -> linear unit with 2 inputs and 1 output\n",
        "\n",
        "        #self.fc1 = torch.nn.Linear(2,hidden_sizes[0],bias=False)\n",
        "        #self.fc2 = torch.nn.Linear(hidden_sizes[0],hidden_sizes[1],bias=False)\n",
        "        #self.fc3 = torch.nn.Linear(hidden_sizes[1],1,bias=False)\n",
        "\n",
        "        self.fc1 = torch.nn.Linear(2, 2)\n",
        "        self.fc2 = torch.nn.Linear(2, 1)\n",
        "\n",
        "        self.non_linear = torch.nn.Sigmoid() #non-linear activation\n",
        "\n",
        "    def forward(self, x): #build network\n",
        "        #output = self.fc1(x) #w*X\n",
        "        #output = self.fc2(x) #w*X\n",
        "        #output = self.fc3(output) #w*X\n",
        "        #output = self.non_linear(output) # activation\n",
        "        #return output\n",
        "        x = self.non_linear(self.fc1(x))\n",
        "        x = self.non_linear(self.fc2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "EaSKRu1lGiF6"
      },
      "execution_count": 477,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get instance of perceptron model\n",
        "model = MLP()\n",
        "\n",
        "#define loss function\n",
        "criterion = torch.nn.BCELoss()\n",
        "\n",
        "#define optimizer -> SGD with learning rate lr\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1)"
      ],
      "metadata": {
        "id": "GQIEin9nGnhU"
      },
      "execution_count": 478,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "model(x_test)\n",
        "print()"
      ],
      "metadata": {
        "id": "1LksIOlRH2eB",
        "outputId": "c0ca0183-b895-41c8-f76f-a9c14beb5ebf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 479,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.train() #set to train mode\n",
        "epoch = 20000\n",
        "for epoch in range(epoch):\n",
        "    optimizer.zero_grad()\n",
        "    # Forward pass\n",
        "    y_pred = model(x_train)\n",
        "    # Compute Loss\n",
        "    loss = criterion(y_pred.squeeze(), y_train)\n",
        "\n",
        "    print('Epoch {}: train loss: {}'.format(epoch, loss.item()))\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    #make gradient update\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "2McooTiKH6sm",
        "outputId": "2e1b80e4-a20e-4ab9-c7bb-2e0d674e11f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 480,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch 15000: train loss: 0.014258485287427902\n",
            "Epoch 15001: train loss: 0.014256884343922138\n",
            "Epoch 15002: train loss: 0.014255271293222904\n",
            "Epoch 15003: train loss: 0.014253668487071991\n",
            "Epoch 15004: train loss: 0.014252054505050182\n",
            "Epoch 15005: train loss: 0.014250442385673523\n",
            "Epoch 15006: train loss: 0.014248840510845184\n",
            "Epoch 15007: train loss: 0.014247233048081398\n",
            "Epoch 15008: train loss: 0.01424561906605959\n",
            "Epoch 15009: train loss: 0.014244016259908676\n",
            "Epoch 15010: train loss: 0.014242403209209442\n",
            "Epoch 15011: train loss: 0.014240794815123081\n",
            "Epoch 15012: train loss: 0.01423918642103672\n",
            "Epoch 15013: train loss: 0.014237584546208382\n",
            "Epoch 15014: train loss: 0.01423597615212202\n",
            "Epoch 15015: train loss: 0.014234374277293682\n",
            "Epoch 15016: train loss: 0.014232764020562172\n",
            "Epoch 15017: train loss: 0.014231158420443535\n",
            "Epoch 15018: train loss: 0.014229552820324898\n",
            "Epoch 15019: train loss: 0.014227946288883686\n",
            "Epoch 15020: train loss: 0.014226345345377922\n",
            "Epoch 15021: train loss: 0.014224736019968987\n",
            "Epoch 15022: train loss: 0.014223136007785797\n",
            "Epoch 15023: train loss: 0.014221531338989735\n",
            "Epoch 15024: train loss: 0.01421993225812912\n",
            "Epoch 15025: train loss: 0.014218329451978207\n",
            "Epoch 15026: train loss: 0.014216727577149868\n",
            "Epoch 15027: train loss: 0.014215131290256977\n",
            "Epoch 15028: train loss: 0.014213529415428638\n",
            "Epoch 15029: train loss: 0.014211926609277725\n",
            "Epoch 15030: train loss: 0.01421032939106226\n",
            "Epoch 15031: train loss: 0.01420872937887907\n",
            "Epoch 15032: train loss: 0.014207130298018456\n",
            "Epoch 15033: train loss: 0.01420553494244814\n",
            "Epoch 15034: train loss: 0.014203936792910099\n",
            "Epoch 15035: train loss: 0.01420233491808176\n",
            "Epoch 15036: train loss: 0.014200735837221146\n",
            "Epoch 15037: train loss: 0.014199137687683105\n",
            "Epoch 15038: train loss: 0.014197539538145065\n",
            "Epoch 15039: train loss: 0.014195944182574749\n",
            "Epoch 15040: train loss: 0.014194349758327007\n",
            "Epoch 15041: train loss: 0.014192750677466393\n",
            "Epoch 15042: train loss: 0.014191151596605778\n",
            "Epoch 15043: train loss: 0.014189551584422588\n",
            "Epoch 15044: train loss: 0.014187955297529697\n",
            "Epoch 15045: train loss: 0.014186364598572254\n",
            "Epoch 15046: train loss: 0.014184771105647087\n",
            "Epoch 15047: train loss: 0.014183171093463898\n",
            "Epoch 15048: train loss: 0.014181585982441902\n",
            "Epoch 15049: train loss: 0.014179988764226437\n",
            "Epoch 15050: train loss: 0.01417839527130127\n",
            "Epoch 15051: train loss: 0.014176798984408379\n",
            "Epoch 15052: train loss: 0.014175212942063808\n",
            "Epoch 15053: train loss: 0.014173616655170918\n",
            "Epoch 15054: train loss: 0.014172025956213474\n",
            "Epoch 15055: train loss: 0.014170438051223755\n",
            "Epoch 15056: train loss: 0.01416883897036314\n",
            "Epoch 15057: train loss: 0.014167246408760548\n",
            "Epoch 15058: train loss: 0.014165658503770828\n",
            "Epoch 15059: train loss: 0.014164076186716557\n",
            "Epoch 15060: train loss: 0.014162478037178516\n",
            "Epoch 15061: train loss: 0.01416089292615652\n",
            "Epoch 15062: train loss: 0.014159300364553928\n",
            "Epoch 15063: train loss: 0.014157715253531933\n",
            "Epoch 15064: train loss: 0.014156132005155087\n",
            "Epoch 15065: train loss: 0.014154543168842793\n",
            "Epoch 15066: train loss: 0.014152955263853073\n",
            "Epoch 15067: train loss: 0.014151371084153652\n",
            "Epoch 15068: train loss: 0.014149784110486507\n",
            "Epoch 15069: train loss: 0.014148194342851639\n",
            "Epoch 15070: train loss: 0.014146609231829643\n",
            "Epoch 15071: train loss: 0.014145025983452797\n",
            "Epoch 15072: train loss: 0.014143441803753376\n",
            "Epoch 15073: train loss: 0.014141855761408806\n",
            "Epoch 15074: train loss: 0.014140267856419086\n",
            "Epoch 15075: train loss: 0.01413868460804224\n",
            "Epoch 15076: train loss: 0.014137101359665394\n",
            "Epoch 15077: train loss: 0.014135516248643398\n",
            "Epoch 15078: train loss: 0.01413393672555685\n",
            "Epoch 15079: train loss: 0.014132356271147728\n",
            "Epoch 15080: train loss: 0.014130768366158009\n",
            "Epoch 15081: train loss: 0.014129186980426311\n",
            "Epoch 15082: train loss: 0.014127601869404316\n",
            "Epoch 15083: train loss: 0.014126020483672619\n",
            "Epoch 15084: train loss: 0.01412444282323122\n",
            "Epoch 15085: train loss: 0.014122852124273777\n",
            "Epoch 15086: train loss: 0.01412127260118723\n",
            "Epoch 15087: train loss: 0.014119693078100681\n",
            "Epoch 15088: train loss: 0.01411810889840126\n",
            "Epoch 15089: train loss: 0.014116523787379265\n",
            "Epoch 15090: train loss: 0.014114941470324993\n",
            "Epoch 15091: train loss: 0.014113365672528744\n",
            "Epoch 15092: train loss: 0.014111792668700218\n",
            "Epoch 15093: train loss: 0.01411021500825882\n",
            "Epoch 15094: train loss: 0.014108628034591675\n",
            "Epoch 15095: train loss: 0.01410705130547285\n",
            "Epoch 15096: train loss: 0.014105468988418579\n",
            "Epoch 15097: train loss: 0.014103898778557777\n",
            "Epoch 15098: train loss: 0.014102324843406677\n",
            "Epoch 15099: train loss: 0.014100750908255577\n",
            "Epoch 15100: train loss: 0.014099177904427052\n",
            "Epoch 15101: train loss: 0.014097600243985653\n",
            "Epoch 15102: train loss: 0.014096027240157127\n",
            "Epoch 15103: train loss: 0.014094454236328602\n",
            "Epoch 15104: train loss: 0.0140928840264678\n",
            "Epoch 15105: train loss: 0.01409131195396185\n",
            "Epoch 15106: train loss: 0.014089732430875301\n",
            "Epoch 15107: train loss: 0.014088165014982224\n",
            "Epoch 15108: train loss: 0.014086593873798847\n",
            "Epoch 15109: train loss: 0.014085018076002598\n",
            "Epoch 15110: train loss: 0.014083447866141796\n",
            "Epoch 15111: train loss: 0.01408187486231327\n",
            "Epoch 15112: train loss: 0.014080309309065342\n",
            "Epoch 15113: train loss: 0.014078736305236816\n",
            "Epoch 15114: train loss: 0.01407716516405344\n",
            "Epoch 15115: train loss: 0.01407558936625719\n",
            "Epoch 15116: train loss: 0.014074021019041538\n",
            "Epoch 15117: train loss: 0.014072449877858162\n",
            "Epoch 15118: train loss: 0.014070880599319935\n",
            "Epoch 15119: train loss: 0.014069310389459133\n",
            "Epoch 15120: train loss: 0.014067742973566055\n",
            "Epoch 15121: train loss: 0.014066175557672977\n",
            "Epoch 15122: train loss: 0.014064603485167027\n",
            "Epoch 15123: train loss: 0.014063030481338501\n",
            "Epoch 15124: train loss: 0.014061464928090572\n",
            "Epoch 15125: train loss: 0.014059903100132942\n",
            "Epoch 15126: train loss: 0.014058327302336693\n",
            "Epoch 15127: train loss: 0.01405675895512104\n",
            "Epoch 15128: train loss: 0.014055189676582813\n",
            "Epoch 15129: train loss: 0.014053627848625183\n",
            "Epoch 15130: train loss: 0.01405205950140953\n",
            "Epoch 15131: train loss: 0.014050492085516453\n",
            "Epoch 15132: train loss: 0.014048924669623375\n",
            "Epoch 15133: train loss: 0.014047360979020596\n",
            "Epoch 15134: train loss: 0.014045794494450092\n",
            "Epoch 15135: train loss: 0.014044231735169888\n",
            "Epoch 15136: train loss: 0.01404266431927681\n",
            "Epoch 15137: train loss: 0.014041105285286903\n",
            "Epoch 15138: train loss: 0.014039546251296997\n",
            "Epoch 15139: train loss: 0.014037983492016792\n",
            "Epoch 15140: train loss: 0.01403642538934946\n",
            "Epoch 15141: train loss: 0.014034859836101532\n",
            "Epoch 15142: train loss: 0.01403330359607935\n",
            "Epoch 15143: train loss: 0.014031745493412018\n",
            "Epoch 15144: train loss: 0.014030184596776962\n",
            "Epoch 15145: train loss: 0.014028623700141907\n",
            "Epoch 15146: train loss: 0.014027070254087448\n",
            "Epoch 15147: train loss: 0.014025500975549221\n",
            "Epoch 15148: train loss: 0.014023951254785061\n",
            "Epoch 15149: train loss: 0.014022381976246834\n",
            "Epoch 15150: train loss: 0.014020833186805248\n",
            "Epoch 15151: train loss: 0.014019276015460491\n",
            "Epoch 15152: train loss: 0.014017711393535137\n",
            "Epoch 15153: train loss: 0.014016160741448402\n",
            "Epoch 15154: train loss: 0.01401460263878107\n",
            "Epoch 15155: train loss: 0.014013041742146015\n",
            "Epoch 15156: train loss: 0.014011483639478683\n",
            "Epoch 15157: train loss: 0.0140099311247468\n",
            "Epoch 15158: train loss: 0.014008382335305214\n",
            "Epoch 15159: train loss: 0.01400681771337986\n",
            "Epoch 15160: train loss: 0.014005261473357677\n",
            "Epoch 15161: train loss: 0.014003714546561241\n",
            "Epoch 15162: train loss: 0.014002150855958462\n",
            "Epoch 15163: train loss: 0.014000596478581429\n",
            "Epoch 15164: train loss: 0.01399904303252697\n",
            "Epoch 15165: train loss: 0.013997489586472511\n",
            "Epoch 15166: train loss: 0.013995934277772903\n",
            "Epoch 15167: train loss: 0.013994376175105572\n",
            "Epoch 15168: train loss: 0.013992823660373688\n",
            "Epoch 15169: train loss: 0.013991271145641804\n",
            "Epoch 15170: train loss: 0.013989717699587345\n",
            "Epoch 15171: train loss: 0.01398816704750061\n",
            "Epoch 15172: train loss: 0.013986612670123577\n",
            "Epoch 15173: train loss: 0.01398506574332714\n",
            "Epoch 15174: train loss: 0.013983513228595257\n",
            "Epoch 15175: train loss: 0.013981962576508522\n",
            "Epoch 15176: train loss: 0.013980410993099213\n",
            "Epoch 15177: train loss: 0.013978864066302776\n",
            "Epoch 15178: train loss: 0.013977313414216042\n",
            "Epoch 15179: train loss: 0.013975761830806732\n",
            "Epoch 15180: train loss: 0.013974212110042572\n",
            "Epoch 15181: train loss: 0.013972669839859009\n",
            "Epoch 15182: train loss: 0.013971120119094849\n",
            "Epoch 15183: train loss: 0.013969572260975838\n",
            "Epoch 15184: train loss: 0.0139680290594697\n",
            "Epoch 15185: train loss: 0.01396647933870554\n",
            "Epoch 15186: train loss: 0.0139649398624897\n",
            "Epoch 15187: train loss: 0.013963398523628712\n",
            "Epoch 15188: train loss: 0.013961848802864552\n",
            "Epoch 15189: train loss: 0.013960306532680988\n",
            "Epoch 15190: train loss: 0.013958764262497425\n",
            "Epoch 15191: train loss: 0.01395721547305584\n",
            "Epoch 15192: train loss: 0.013955670408904552\n",
            "Epoch 15193: train loss: 0.013954129070043564\n",
            "Epoch 15194: train loss: 0.013952587731182575\n",
            "Epoch 15195: train loss: 0.013951045460999012\n",
            "Epoch 15196: train loss: 0.013949502259492874\n",
            "Epoch 15197: train loss: 0.013947953470051289\n",
            "Epoch 15198: train loss: 0.013946413062512875\n",
            "Epoch 15199: train loss: 0.01394487265497446\n",
            "Epoch 15200: train loss: 0.01394332479685545\n",
            "Epoch 15201: train loss: 0.013941782526671886\n",
            "Epoch 15202: train loss: 0.013940244913101196\n",
            "Epoch 15203: train loss: 0.01393869798630476\n",
            "Epoch 15204: train loss: 0.013937163166701794\n",
            "Epoch 15205: train loss: 0.013935624621808529\n",
            "Epoch 15206: train loss: 0.013934082351624966\n",
            "Epoch 15207: train loss: 0.013932536356151104\n",
            "Epoch 15208: train loss: 0.013930998742580414\n",
            "Epoch 15209: train loss: 0.013929464854300022\n",
            "Epoch 15210: train loss: 0.013927917927503586\n",
            "Epoch 15211: train loss: 0.013926379382610321\n",
            "Epoch 15212: train loss: 0.013924840837717056\n",
            "Epoch 15213: train loss: 0.01392330601811409\n",
            "Epoch 15214: train loss: 0.0139217684045434\n",
            "Epoch 15215: train loss: 0.013920229859650135\n",
            "Epoch 15216: train loss: 0.013918700627982616\n",
            "Epoch 15217: train loss: 0.013917160220444202\n",
            "Epoch 15218: train loss: 0.01391562633216381\n",
            "Epoch 15219: train loss: 0.01391408871859312\n",
            "Epoch 15220: train loss: 0.013912553898990154\n",
            "Epoch 15221: train loss: 0.013911021873354912\n",
            "Epoch 15222: train loss: 0.013909486122429371\n",
            "Epoch 15223: train loss: 0.013907953165471554\n",
            "Epoch 15224: train loss: 0.013906413689255714\n",
            "Epoch 15225: train loss: 0.013904884457588196\n",
            "Epoch 15226: train loss: 0.013903350569307804\n",
            "Epoch 15227: train loss: 0.013901809230446815\n",
            "Epoch 15228: train loss: 0.013900283724069595\n",
            "Epoch 15229: train loss: 0.013898750767111778\n",
            "Epoch 15230: train loss: 0.013897217810153961\n",
            "Epoch 15231: train loss: 0.013895691372454166\n",
            "Epoch 15232: train loss: 0.013894157484173775\n",
            "Epoch 15233: train loss: 0.013892621733248234\n",
            "Epoch 15234: train loss: 0.013891098089516163\n",
            "Epoch 15235: train loss: 0.013889568857848644\n",
            "Epoch 15236: train loss: 0.0138880405575037\n",
            "Epoch 15237: train loss: 0.013886503875255585\n",
            "Epoch 15238: train loss: 0.013884982094168663\n",
            "Epoch 15239: train loss: 0.013883448205888271\n",
            "Epoch 15240: train loss: 0.013881921768188477\n",
            "Epoch 15241: train loss: 0.013880395330488682\n",
            "Epoch 15242: train loss: 0.013878867030143738\n",
            "Epoch 15243: train loss: 0.013877341523766518\n",
            "Epoch 15244: train loss: 0.013875809498131275\n",
            "Epoch 15245: train loss: 0.013874286785721779\n",
            "Epoch 15246: train loss: 0.013872758485376835\n",
            "Epoch 15247: train loss: 0.013871235772967339\n",
            "Epoch 15248: train loss: 0.013869709335267544\n",
            "Epoch 15249: train loss: 0.013868177309632301\n",
            "Epoch 15250: train loss: 0.013866648077964783\n",
            "Epoch 15251: train loss: 0.013865129090845585\n",
            "Epoch 15252: train loss: 0.013863603584468365\n",
            "Epoch 15253: train loss: 0.013862076215445995\n",
            "Epoch 15254: train loss: 0.013860558159649372\n",
            "Epoch 15255: train loss: 0.013859031721949577\n",
            "Epoch 15256: train loss: 0.013857509940862656\n",
            "Epoch 15257: train loss: 0.013855986297130585\n",
            "Epoch 15258: train loss: 0.013854464516043663\n",
            "Epoch 15259: train loss: 0.013852941803634167\n",
            "Epoch 15260: train loss: 0.013851425610482693\n",
            "Epoch 15261: train loss: 0.013849903829395771\n",
            "Epoch 15262: train loss: 0.013848373666405678\n",
            "Epoch 15263: train loss: 0.013846851885318756\n",
            "Epoch 15264: train loss: 0.01384533941745758\n",
            "Epoch 15265: train loss: 0.01384381577372551\n",
            "Epoch 15266: train loss: 0.013842293992638588\n",
            "Epoch 15267: train loss: 0.013840775936841965\n",
            "Epoch 15268: train loss: 0.013839255087077618\n",
            "Epoch 15269: train loss: 0.013837737962603569\n",
            "Epoch 15270: train loss: 0.013836216181516647\n",
            "Epoch 15271: train loss: 0.013834696263074875\n",
            "Epoch 15272: train loss: 0.013833179138600826\n",
            "Epoch 15273: train loss: 0.013831663876771927\n",
            "Epoch 15274: train loss: 0.013830144889652729\n",
            "Epoch 15275: train loss: 0.013828626833856106\n",
            "Epoch 15276: train loss: 0.013827107846736908\n",
            "Epoch 15277: train loss: 0.013825598172843456\n",
            "Epoch 15278: train loss: 0.013824081048369408\n",
            "Epoch 15279: train loss: 0.013822564855217934\n",
            "Epoch 15280: train loss: 0.013821050524711609\n",
            "Epoch 15281: train loss: 0.01381953340023756\n",
            "Epoch 15282: train loss: 0.013818017207086086\n",
            "Epoch 15283: train loss: 0.013816505670547485\n",
            "Epoch 15284: train loss: 0.013814989477396011\n",
            "Epoch 15285: train loss: 0.01381347794085741\n",
            "Epoch 15286: train loss: 0.013811962679028511\n",
            "Epoch 15287: train loss: 0.013810454867780209\n",
            "Epoch 15288: train loss: 0.01380893774330616\n",
            "Epoch 15289: train loss: 0.013807417824864388\n",
            "Epoch 15290: train loss: 0.013805915601551533\n",
            "Epoch 15291: train loss: 0.013804402202367783\n",
            "Epoch 15292: train loss: 0.013802886009216309\n",
            "Epoch 15293: train loss: 0.01380138099193573\n",
            "Epoch 15294: train loss: 0.01379986573010683\n",
            "Epoch 15295: train loss: 0.013798357918858528\n",
            "Epoch 15296: train loss: 0.013796846382319927\n",
            "Epoch 15297: train loss: 0.01379533763974905\n",
            "Epoch 15298: train loss: 0.013793835416436195\n",
            "Epoch 15299: train loss: 0.013792325742542744\n",
            "Epoch 15300: train loss: 0.013790816999971867\n",
            "Epoch 15301: train loss: 0.01378930825740099\n",
            "Epoch 15302: train loss: 0.01378780510276556\n",
            "Epoch 15303: train loss: 0.013786285184323788\n",
            "Epoch 15304: train loss: 0.013784780167043209\n",
            "Epoch 15305: train loss: 0.01378327701240778\n",
            "Epoch 15306: train loss: 0.013781771995127201\n",
            "Epoch 15307: train loss: 0.013780257664620876\n",
            "Epoch 15308: train loss: 0.013778756372630596\n",
            "Epoch 15309: train loss: 0.01377724576741457\n",
            "Epoch 15310: train loss: 0.013775738887488842\n",
            "Epoch 15311: train loss: 0.013774238526821136\n",
            "Epoch 15312: train loss: 0.01377272978425026\n",
            "Epoch 15313: train loss: 0.01377122476696968\n",
            "Epoch 15314: train loss: 0.013769718818366528\n",
            "Epoch 15315: train loss: 0.013768215663731098\n",
            "Epoch 15316: train loss: 0.013766709715127945\n",
            "Epoch 15317: train loss: 0.013765205629169941\n",
            "Epoch 15318: train loss: 0.01376369409263134\n",
            "Epoch 15319: train loss: 0.013762194663286209\n",
            "Epoch 15320: train loss: 0.013760693371295929\n",
            "Epoch 15321: train loss: 0.013759189285337925\n",
            "Epoch 15322: train loss: 0.013757682405412197\n",
            "Epoch 15323: train loss: 0.013756176456809044\n",
            "Epoch 15324: train loss: 0.01375468447804451\n",
            "Epoch 15325: train loss: 0.013753179460763931\n",
            "Epoch 15326: train loss: 0.013751682825386524\n",
            "Epoch 15327: train loss: 0.013750183396041393\n",
            "Epoch 15328: train loss: 0.013748683035373688\n",
            "Epoch 15329: train loss: 0.013747181743383408\n",
            "Epoch 15330: train loss: 0.013745682314038277\n",
            "Epoch 15331: train loss: 0.01374418567866087\n",
            "Epoch 15332: train loss: 0.013742689043283463\n",
            "Epoch 15333: train loss: 0.01374119147658348\n",
            "Epoch 15334: train loss: 0.013739692978560925\n",
            "Epoch 15335: train loss: 0.01373820099979639\n",
            "Epoch 15336: train loss: 0.013736702501773834\n",
            "Epoch 15337: train loss: 0.013735204935073853\n",
            "Epoch 15338: train loss: 0.01373371109366417\n",
            "Epoch 15339: train loss: 0.013732213526964188\n",
            "Epoch 15340: train loss: 0.013730715960264206\n",
            "Epoch 15341: train loss: 0.013729225844144821\n",
            "Epoch 15342: train loss: 0.013727731071412563\n",
            "Epoch 15343: train loss: 0.013726236298680305\n",
            "Epoch 15344: train loss: 0.013724739663302898\n",
            "Epoch 15345: train loss: 0.013723251409828663\n",
            "Epoch 15346: train loss: 0.01372175570577383\n",
            "Epoch 15347: train loss: 0.013720263727009296\n",
            "Epoch 15348: train loss: 0.013718764297664165\n",
            "Epoch 15349: train loss: 0.013717267662286758\n",
            "Epoch 15350: train loss: 0.013715779408812523\n",
            "Epoch 15351: train loss: 0.01371428370475769\n",
            "Epoch 15352: train loss: 0.01371279638260603\n",
            "Epoch 15353: train loss: 0.013711303472518921\n",
            "Epoch 15354: train loss: 0.013709810562431812\n",
            "Epoch 15355: train loss: 0.013708318583667278\n",
            "Epoch 15356: train loss: 0.013706829398870468\n",
            "Epoch 15357: train loss: 0.013705339282751083\n",
            "Epoch 15358: train loss: 0.013703848235309124\n",
            "Epoch 15359: train loss: 0.013702357187867165\n",
            "Epoch 15360: train loss: 0.01370086520910263\n",
            "Epoch 15361: train loss: 0.013699375092983246\n",
            "Epoch 15362: train loss: 0.013697884976863861\n",
            "Epoch 15363: train loss: 0.013696392066776752\n",
            "Epoch 15364: train loss: 0.01369490660727024\n",
            "Epoch 15365: train loss: 0.013693416491150856\n",
            "Epoch 15366: train loss: 0.013691924512386322\n",
            "Epoch 15367: train loss: 0.01369044091552496\n",
            "Epoch 15368: train loss: 0.013688950799405575\n",
            "Epoch 15369: train loss: 0.013687464408576488\n",
            "Epoch 15370: train loss: 0.01368597149848938\n",
            "Epoch 15371: train loss: 0.013684489764273167\n",
            "Epoch 15372: train loss: 0.013683000579476357\n",
            "Epoch 15373: train loss: 0.013681513257324696\n",
            "Epoch 15374: train loss: 0.013680031523108482\n",
            "Epoch 15375: train loss: 0.01367854792624712\n",
            "Epoch 15376: train loss: 0.01367706898599863\n",
            "Epoch 15377: train loss: 0.013675578869879246\n",
            "Epoch 15378: train loss: 0.013674101792275906\n",
            "Epoch 15379: train loss: 0.013672608882188797\n",
            "Epoch 15380: train loss: 0.01367113646119833\n",
            "Epoch 15381: train loss: 0.013669658452272415\n",
            "Epoch 15382: train loss: 0.013668167404830456\n",
            "Epoch 15383: train loss: 0.013666692189872265\n",
            "Epoch 15384: train loss: 0.0136652123183012\n",
            "Epoch 15385: train loss: 0.013663727790117264\n",
            "Epoch 15386: train loss: 0.013662248849868774\n",
            "Epoch 15387: train loss: 0.013660768046975136\n",
            "Epoch 15388: train loss: 0.01365929190069437\n",
            "Epoch 15389: train loss: 0.013657813891768456\n",
            "Epoch 15390: train loss: 0.013656333088874817\n",
            "Epoch 15391: train loss: 0.01365484856069088\n",
            "Epoch 15392: train loss: 0.013653373345732689\n",
            "Epoch 15393: train loss: 0.013651890680193901\n",
            "Epoch 15394: train loss: 0.013650414533913136\n",
            "Epoch 15395: train loss: 0.013648929074406624\n",
            "Epoch 15396: train loss: 0.013647454790771008\n",
            "Epoch 15397: train loss: 0.013645976781845093\n",
            "Epoch 15398: train loss: 0.013644502498209476\n",
            "Epoch 15399: train loss: 0.013643022626638412\n",
            "Epoch 15400: train loss: 0.013641546480357647\n",
            "Epoch 15401: train loss: 0.013640067540109158\n",
            "Epoch 15402: train loss: 0.013638589531183243\n",
            "Epoch 15403: train loss: 0.01363710779696703\n",
            "Epoch 15404: train loss: 0.013635635375976562\n",
            "Epoch 15405: train loss: 0.01363415364176035\n",
            "Epoch 15406: train loss: 0.013632677495479584\n",
            "Epoch 15407: train loss: 0.013631199486553669\n",
            "Epoch 15408: train loss: 0.013629723340272903\n",
            "Epoch 15409: train loss: 0.013628249056637287\n",
            "Epoch 15410: train loss: 0.013626769185066223\n",
            "Epoch 15411: train loss: 0.013625294901430607\n",
            "Epoch 15412: train loss: 0.013623826205730438\n",
            "Epoch 15413: train loss: 0.013622351922094822\n",
            "Epoch 15414: train loss: 0.013620881363749504\n",
            "Epoch 15415: train loss: 0.013619408942759037\n",
            "Epoch 15416: train loss: 0.01361793465912342\n",
            "Epoch 15417: train loss: 0.01361645758152008\n",
            "Epoch 15418: train loss: 0.013614987954497337\n",
            "Epoch 15419: train loss: 0.013613522052764893\n",
            "Epoch 15420: train loss: 0.013612048700451851\n",
            "Epoch 15421: train loss: 0.013610581867396832\n",
            "Epoch 15422: train loss: 0.013609110377728939\n",
            "Epoch 15423: train loss: 0.013607647269964218\n",
            "Epoch 15424: train loss: 0.013606182299554348\n",
            "Epoch 15425: train loss: 0.01360471174120903\n",
            "Epoch 15426: train loss: 0.013603243045508862\n",
            "Epoch 15427: train loss: 0.013601773418486118\n",
            "Epoch 15428: train loss: 0.013600309379398823\n",
            "Epoch 15429: train loss: 0.013598844408988953\n",
            "Epoch 15430: train loss: 0.01359737478196621\n",
            "Epoch 15431: train loss: 0.013595910742878914\n",
            "Epoch 15432: train loss: 0.01359444111585617\n",
            "Epoch 15433: train loss: 0.013592975214123726\n",
            "Epoch 15434: train loss: 0.013591512106359005\n",
            "Epoch 15435: train loss: 0.013590042479336262\n",
            "Epoch 15436: train loss: 0.01358858123421669\n",
            "Epoch 15437: train loss: 0.013587106950581074\n",
            "Epoch 15438: train loss: 0.013585648499429226\n",
            "Epoch 15439: train loss: 0.013584182597696781\n",
            "Epoch 15440: train loss: 0.013582714833319187\n",
            "Epoch 15441: train loss: 0.013581250794231892\n",
            "Epoch 15442: train loss: 0.013579788617789745\n",
            "Epoch 15443: train loss: 0.013578320853412151\n",
            "Epoch 15444: train loss: 0.013576856814324856\n",
            "Epoch 15445: train loss: 0.013575398363173008\n",
            "Epoch 15446: train loss: 0.013573931530117989\n",
            "Epoch 15447: train loss: 0.013572468422353268\n",
            "Epoch 15448: train loss: 0.013571006245911121\n",
            "Epoch 15449: train loss: 0.013569537550210953\n",
            "Epoch 15450: train loss: 0.01356807816773653\n",
            "Epoch 15451: train loss: 0.013566619716584682\n",
            "Epoch 15452: train loss: 0.013565159402787685\n",
            "Epoch 15453: train loss: 0.01356369536370039\n",
            "Epoch 15454: train loss: 0.013562241569161415\n",
            "Epoch 15455: train loss: 0.013560776598751545\n",
            "Epoch 15456: train loss: 0.013559318147599697\n",
            "Epoch 15457: train loss: 0.01355785597115755\n",
            "Epoch 15458: train loss: 0.01355640310794115\n",
            "Epoch 15459: train loss: 0.013554946519434452\n",
            "Epoch 15460: train loss: 0.013553486205637455\n",
            "Epoch 15461: train loss: 0.01355202216655016\n",
            "Epoch 15462: train loss: 0.013550562784075737\n",
            "Epoch 15463: train loss: 0.01354911271482706\n",
            "Epoch 15464: train loss: 0.01354764774441719\n",
            "Epoch 15465: train loss: 0.013546198606491089\n",
            "Epoch 15466: train loss: 0.013544739224016666\n",
            "Epoch 15467: train loss: 0.013543283566832542\n",
            "Epoch 15468: train loss: 0.013541823253035545\n",
            "Epoch 15469: train loss: 0.013540367595851421\n",
            "Epoch 15470: train loss: 0.013538911938667297\n",
            "Epoch 15471: train loss: 0.013537467457354069\n",
            "Epoch 15472: train loss: 0.013536006212234497\n",
            "Epoch 15473: train loss: 0.013534552417695522\n",
            "Epoch 15474: train loss: 0.013533102348446846\n",
            "Epoch 15475: train loss: 0.013531643897294998\n",
            "Epoch 15476: train loss: 0.013530195690691471\n",
            "Epoch 15477: train loss: 0.013528737239539623\n",
            "Epoch 15478: train loss: 0.013527285307645798\n",
            "Epoch 15479: train loss: 0.013525838032364845\n",
            "Epoch 15480: train loss: 0.01352438610047102\n",
            "Epoch 15481: train loss: 0.013522934168577194\n",
            "Epoch 15482: train loss: 0.013521482236683369\n",
            "Epoch 15483: train loss: 0.01352002751082182\n",
            "Epoch 15484: train loss: 0.013518571853637695\n",
            "Epoch 15485: train loss: 0.013517131097614765\n",
            "Epoch 15486: train loss: 0.013515670783817768\n",
            "Epoch 15487: train loss: 0.01351422630250454\n",
            "Epoch 15488: train loss: 0.013512775301933289\n",
            "Epoch 15489: train loss: 0.013511325232684612\n",
            "Epoch 15490: train loss: 0.01350987795740366\n",
            "Epoch 15491: train loss: 0.013508418574929237\n",
            "Epoch 15492: train loss: 0.013506967574357986\n",
            "Epoch 15493: train loss: 0.013505524955689907\n",
            "Epoch 15494: train loss: 0.01350407674908638\n",
            "Epoch 15495: train loss: 0.013502629473805428\n",
            "Epoch 15496: train loss: 0.0135011812672019\n",
            "Epoch 15497: train loss: 0.013499733060598373\n",
            "Epoch 15498: train loss: 0.01349828951060772\n",
            "Epoch 15499: train loss: 0.013496842235326767\n",
            "Epoch 15500: train loss: 0.013495395891368389\n",
            "Epoch 15501: train loss: 0.013493952341377735\n",
            "Epoch 15502: train loss: 0.013492499478161335\n",
            "Epoch 15503: train loss: 0.013491062447428703\n",
            "Epoch 15504: train loss: 0.013489614240825176\n",
            "Epoch 15505: train loss: 0.013488172553479671\n",
            "Epoch 15506: train loss: 0.01348672341555357\n",
            "Epoch 15507: train loss: 0.01348528265953064\n",
            "Epoch 15508: train loss: 0.013483840972185135\n",
            "Epoch 15509: train loss: 0.013482394628226757\n",
            "Epoch 15510: train loss: 0.013480952009558678\n",
            "Epoch 15511: train loss: 0.013479511253535748\n",
            "Epoch 15512: train loss: 0.013478065840899944\n",
            "Epoch 15513: train loss: 0.013476621359586716\n",
            "Epoch 15514: train loss: 0.013475177809596062\n",
            "Epoch 15515: train loss: 0.01347373891621828\n",
            "Epoch 15516: train loss: 0.013472293503582478\n",
            "Epoch 15517: train loss: 0.013470854610204697\n",
            "Epoch 15518: train loss: 0.013469415716826916\n",
            "Epoch 15519: train loss: 0.013467971235513687\n",
            "Epoch 15520: train loss: 0.01346653699874878\n",
            "Epoch 15521: train loss: 0.0134650943800807\n",
            "Epoch 15522: train loss: 0.013463657349348068\n",
            "Epoch 15523: train loss: 0.013462217524647713\n",
            "Epoch 15524: train loss: 0.013460775837302208\n",
            "Epoch 15525: train loss: 0.013459339737892151\n",
            "Epoch 15526: train loss: 0.013457908295094967\n",
            "Epoch 15527: train loss: 0.013456465676426888\n",
            "Epoch 15528: train loss: 0.013455024920403957\n",
            "Epoch 15529: train loss: 0.013453583233058453\n",
            "Epoch 15530: train loss: 0.013452152721583843\n",
            "Epoch 15531: train loss: 0.013450714759528637\n",
            "Epoch 15532: train loss: 0.013449274934828281\n",
            "Epoch 15533: train loss: 0.013447835110127926\n",
            "Epoch 15534: train loss: 0.013446401804685593\n",
            "Epoch 15535: train loss: 0.013444971293210983\n",
            "Epoch 15536: train loss: 0.01344353798776865\n",
            "Epoch 15537: train loss: 0.013442105613648891\n",
            "Epoch 15538: train loss: 0.01344067882746458\n",
            "Epoch 15539: train loss: 0.013439234346151352\n",
            "Epoch 15540: train loss: 0.01343779917806387\n",
            "Epoch 15541: train loss: 0.013436371460556984\n",
            "Epoch 15542: train loss: 0.013434939086437225\n",
            "Epoch 15543: train loss: 0.013433510437607765\n",
            "Epoch 15544: train loss: 0.013432076200842857\n",
            "Epoch 15545: train loss: 0.013430642895400524\n",
            "Epoch 15546: train loss: 0.013429207727313042\n",
            "Epoch 15547: train loss: 0.013427781872451305\n",
            "Epoch 15548: train loss: 0.013426351360976696\n",
            "Epoch 15549: train loss: 0.013424916192889214\n",
            "Epoch 15550: train loss: 0.013423486612737179\n",
            "Epoch 15551: train loss: 0.013422049582004547\n",
            "Epoch 15552: train loss: 0.01342062558978796\n",
            "Epoch 15553: train loss: 0.013419196009635925\n",
            "Epoch 15554: train loss: 0.013417762704193592\n",
            "Epoch 15555: train loss: 0.013416331261396408\n",
            "Epoch 15556: train loss: 0.013414902612566948\n",
            "Epoch 15557: train loss: 0.013413467444479465\n",
            "Epoch 15558: train loss: 0.01341203972697258\n",
            "Epoch 15559: train loss: 0.013410613872110844\n",
            "Epoch 15560: train loss: 0.013409184291958809\n",
            "Epoch 15561: train loss: 0.013407750055193901\n",
            "Epoch 15562: train loss: 0.01340632326900959\n",
            "Epoch 15563: train loss: 0.01340489648282528\n",
            "Epoch 15564: train loss: 0.01340346597135067\n",
            "Epoch 15565: train loss: 0.013402038253843784\n",
            "Epoch 15566: train loss: 0.013400617055594921\n",
            "Epoch 15567: train loss: 0.013399188406765461\n",
            "Epoch 15568: train loss: 0.013397758826613426\n",
            "Epoch 15569: train loss: 0.01339633297175169\n",
            "Epoch 15570: train loss: 0.013394908048212528\n",
            "Epoch 15571: train loss: 0.013393484055995941\n",
            "Epoch 15572: train loss: 0.013392061926424503\n",
            "Epoch 15573: train loss: 0.013390636071562767\n",
            "Epoch 15574: train loss: 0.01338921207934618\n",
            "Epoch 15575: train loss: 0.013387789949774742\n",
            "Epoch 15576: train loss: 0.013386370614171028\n",
            "Epoch 15577: train loss: 0.013384945690631866\n",
            "Epoch 15578: train loss: 0.013383523561060429\n",
            "Epoch 15579: train loss: 0.013382104225456715\n",
            "Epoch 15580: train loss: 0.013380682095885277\n",
            "Epoch 15581: train loss: 0.013379260897636414\n",
            "Epoch 15582: train loss: 0.013377836905419827\n",
            "Epoch 15583: train loss: 0.013376419432461262\n",
            "Epoch 15584: train loss: 0.013374998234212399\n",
            "Epoch 15585: train loss: 0.01337357610464096\n",
            "Epoch 15586: train loss: 0.01337215956300497\n",
            "Epoch 15587: train loss: 0.01337074488401413\n",
            "Epoch 15588: train loss: 0.013369323685765266\n",
            "Epoch 15589: train loss: 0.013367896899580956\n",
            "Epoch 15590: train loss: 0.013366480357944965\n",
            "Epoch 15591: train loss: 0.0133650628849864\n",
            "Epoch 15592: train loss: 0.013363642618060112\n",
            "Epoch 15593: train loss: 0.013362221419811249\n",
            "Epoch 15594: train loss: 0.013360797427594662\n",
            "Epoch 15595: train loss: 0.013359392993152142\n",
            "Epoch 15596: train loss: 0.013357969000935555\n",
            "Epoch 15597: train loss: 0.013356559909880161\n",
            "Epoch 15598: train loss: 0.013355134055018425\n",
            "Epoch 15599: train loss: 0.013353722169995308\n",
            "Epoch 15600: train loss: 0.013352305628359318\n",
            "Epoch 15601: train loss: 0.013350891880691051\n",
            "Epoch 15602: train loss: 0.013349472545087337\n",
            "Epoch 15603: train loss: 0.013348054140806198\n",
            "Epoch 15604: train loss: 0.013346643187105656\n",
            "Epoch 15605: train loss: 0.013345221057534218\n",
            "Epoch 15606: train loss: 0.013343806378543377\n",
            "Epoch 15607: train loss: 0.013342401012778282\n",
            "Epoch 15608: train loss: 0.013340981677174568\n",
            "Epoch 15609: train loss: 0.013339566066861153\n",
            "Epoch 15610: train loss: 0.013338149525225163\n",
            "Epoch 15611: train loss: 0.013336741365492344\n",
            "Epoch 15612: train loss: 0.013335327617824078\n",
            "Epoch 15613: train loss: 0.013333912007510662\n",
            "Epoch 15614: train loss: 0.013332496397197247\n",
            "Epoch 15615: train loss: 0.013331079855561256\n",
            "Epoch 15616: train loss: 0.013329671695828438\n",
            "Epoch 15617: train loss: 0.01332826167345047\n",
            "Epoch 15618: train loss: 0.013326847925782204\n",
            "Epoch 15619: train loss: 0.013325436972081661\n",
            "Epoch 15620: train loss: 0.01332402415573597\n",
            "Epoch 15621: train loss: 0.013322623446583748\n",
            "Epoch 15622: train loss: 0.013321217149496078\n",
            "Epoch 15623: train loss: 0.013319802470505238\n",
            "Epoch 15624: train loss: 0.01331839244812727\n",
            "Epoch 15625: train loss: 0.013316982425749302\n",
            "Epoch 15626: train loss: 0.013315578922629356\n",
            "Epoch 15627: train loss: 0.013314174488186836\n",
            "Epoch 15628: train loss: 0.013312768191099167\n",
            "Epoch 15629: train loss: 0.013311360031366348\n",
            "Epoch 15630: train loss: 0.013309956528246403\n",
            "Epoch 15631: train loss: 0.013308548368513584\n",
            "Epoch 15632: train loss: 0.013307139277458191\n",
            "Epoch 15633: train loss: 0.013305739499628544\n",
            "Epoch 15634: train loss: 0.01330433040857315\n",
            "Epoch 15635: train loss: 0.013302922248840332\n",
            "Epoch 15636: train loss: 0.013301518745720387\n",
            "Epoch 15637: train loss: 0.013300115242600441\n",
            "Epoch 15638: train loss: 0.01329871267080307\n",
            "Epoch 15639: train loss: 0.013297307305037975\n",
            "Epoch 15640: train loss: 0.013295906595885754\n",
            "Epoch 15641: train loss: 0.013294496573507786\n",
            "Epoch 15642: train loss: 0.013293099589645863\n",
            "Epoch 15643: train loss: 0.013291693292558193\n",
            "Epoch 15644: train loss: 0.01329028233885765\n",
            "Epoch 15645: train loss: 0.013288883492350578\n",
            "Epoch 15646: train loss: 0.013287478126585484\n",
            "Epoch 15647: train loss: 0.013286072760820389\n",
            "Epoch 15648: train loss: 0.013284672051668167\n",
            "Epoch 15649: train loss: 0.013283275067806244\n",
            "Epoch 15650: train loss: 0.013281867839396\n",
            "Epoch 15651: train loss: 0.013280465267598629\n",
            "Epoch 15652: train loss: 0.013279064558446407\n",
            "Epoch 15653: train loss: 0.013277663849294186\n",
            "Epoch 15654: train loss: 0.013276263140141964\n",
            "Epoch 15655: train loss: 0.013274858705699444\n",
            "Epoch 15656: train loss: 0.01327346172183752\n",
            "Epoch 15657: train loss: 0.013272061012685299\n",
            "Epoch 15658: train loss: 0.013270656578242779\n",
            "Epoch 15659: train loss: 0.013269258663058281\n",
            "Epoch 15660: train loss: 0.013267857022583485\n",
            "Epoch 15661: train loss: 0.013266462832689285\n",
            "Epoch 15662: train loss: 0.013265064917504787\n",
            "Epoch 15663: train loss: 0.013263668864965439\n",
            "Epoch 15664: train loss: 0.013262270949780941\n",
            "Epoch 15665: train loss: 0.013260869309306145\n",
            "Epoch 15666: train loss: 0.01325947791337967\n",
            "Epoch 15667: train loss: 0.013258074410259724\n",
            "Epoch 15668: train loss: 0.013256682083010674\n",
            "Epoch 15669: train loss: 0.013255281373858452\n",
            "Epoch 15670: train loss: 0.013253890909254551\n",
            "Epoch 15671: train loss: 0.01325249020010233\n",
            "Epoch 15672: train loss: 0.013251098804175854\n",
            "Epoch 15673: train loss: 0.01324970368295908\n",
            "Epoch 15674: train loss: 0.013248306699097157\n",
            "Epoch 15675: train loss: 0.013246922753751278\n",
            "Epoch 15676: train loss: 0.013245520181953907\n",
            "Epoch 15677: train loss: 0.013244136236608028\n",
            "Epoch 15678: train loss: 0.013242739252746105\n",
            "Epoch 15679: train loss: 0.013241349719464779\n",
            "Epoch 15680: train loss: 0.013239950872957706\n",
            "Epoch 15681: train loss: 0.013238558545708656\n",
            "Epoch 15682: train loss: 0.013237168081104755\n",
            "Epoch 15683: train loss: 0.013235780410468578\n",
            "Epoch 15684: train loss: 0.013234386220574379\n",
            "Epoch 15685: train loss: 0.013232992962002754\n",
            "Epoch 15686: train loss: 0.013231602497398853\n",
            "Epoch 15687: train loss: 0.01323021575808525\n",
            "Epoch 15688: train loss: 0.01322882529348135\n",
            "Epoch 15689: train loss: 0.013227429240942001\n",
            "Epoch 15690: train loss: 0.013226041570305824\n",
            "Epoch 15691: train loss: 0.013224654830992222\n",
            "Epoch 15692: train loss: 0.013223262503743172\n",
            "Epoch 15693: train loss: 0.013221869245171547\n",
            "Epoch 15694: train loss: 0.013220489956438541\n",
            "Epoch 15695: train loss: 0.01321910135447979\n",
            "Epoch 15696: train loss: 0.01321770902723074\n",
            "Epoch 15697: train loss: 0.01321631669998169\n",
            "Epoch 15698: train loss: 0.013214931823313236\n",
            "Epoch 15699: train loss: 0.013213541358709335\n",
            "Epoch 15700: train loss: 0.013212154619395733\n",
            "Epoch 15701: train loss: 0.013210764154791832\n",
            "Epoch 15702: train loss: 0.01320937555283308\n",
            "Epoch 15703: train loss: 0.0132079953327775\n",
            "Epoch 15704: train loss: 0.013206608593463898\n",
            "Epoch 15705: train loss: 0.013205224648118019\n",
            "Epoch 15706: train loss: 0.013203837908804417\n",
            "Epoch 15707: train loss: 0.013202452100813389\n",
            "Epoch 15708: train loss: 0.013201066292822361\n",
            "Epoch 15709: train loss: 0.013199685141444206\n",
            "Epoch 15710: train loss: 0.013198291882872581\n",
            "Epoch 15711: train loss: 0.013196911662817001\n",
            "Epoch 15712: train loss: 0.013195529580116272\n",
            "Epoch 15713: train loss: 0.013194144703447819\n",
            "Epoch 15714: train loss: 0.013192761689424515\n",
            "Epoch 15715: train loss: 0.013191381469368935\n",
            "Epoch 15716: train loss: 0.013189993798732758\n",
            "Epoch 15717: train loss: 0.01318860799074173\n",
            "Epoch 15718: train loss: 0.01318722777068615\n",
            "Epoch 15719: train loss: 0.013185846619307995\n",
            "Epoch 15720: train loss: 0.013184461742639542\n",
            "Epoch 15721: train loss: 0.013183082453906536\n",
            "Epoch 15722: train loss: 0.013181697577238083\n",
            "Epoch 15723: train loss: 0.01318032294511795\n",
            "Epoch 15724: train loss: 0.013178945519030094\n",
            "Epoch 15725: train loss: 0.013177563436329365\n",
            "Epoch 15726: train loss: 0.013176188804209232\n",
            "Epoch 15727: train loss: 0.013174815103411674\n",
            "Epoch 15728: train loss: 0.013173429295420647\n",
            "Epoch 15729: train loss: 0.013172047212719917\n",
            "Epoch 15730: train loss: 0.013170670717954636\n",
            "Epoch 15731: train loss: 0.013169296085834503\n",
            "Epoch 15732: train loss: 0.013167920522391796\n",
            "Epoch 15733: train loss: 0.01316654123365879\n",
            "Epoch 15734: train loss: 0.01316516101360321\n",
            "Epoch 15735: train loss: 0.013163785450160503\n",
            "Epoch 15736: train loss: 0.013162415474653244\n",
            "Epoch 15737: train loss: 0.01316103059798479\n",
            "Epoch 15738: train loss: 0.013159657828509808\n",
            "Epoch 15739: train loss: 0.013158285990357399\n",
            "Epoch 15740: train loss: 0.013156906701624393\n",
            "Epoch 15741: train loss: 0.013155530206859112\n",
            "Epoch 15742: train loss: 0.013154157437384129\n",
            "Epoch 15743: train loss: 0.013152782805263996\n",
            "Epoch 15744: train loss: 0.013151402585208416\n",
            "Epoch 15745: train loss: 0.013150029815733433\n",
            "Epoch 15746: train loss: 0.013148653320968151\n",
            "Epoch 15747: train loss: 0.013147277757525444\n",
            "Epoch 15748: train loss: 0.013145909644663334\n",
            "Epoch 15749: train loss: 0.013144535943865776\n",
            "Epoch 15750: train loss: 0.013143162243068218\n",
            "Epoch 15751: train loss: 0.01314178854227066\n",
            "Epoch 15752: train loss: 0.013140416704118252\n",
            "Epoch 15753: train loss: 0.013139043934643269\n",
            "Epoch 15754: train loss: 0.013137677684426308\n",
            "Epoch 15755: train loss: 0.0131363021209836\n",
            "Epoch 15756: train loss: 0.013134933076798916\n",
            "Epoch 15757: train loss: 0.013133550994098186\n",
            "Epoch 15758: train loss: 0.013132194988429546\n",
            "Epoch 15759: train loss: 0.01313081942498684\n",
            "Epoch 15760: train loss: 0.01312944944947958\n",
            "Epoch 15761: train loss: 0.013128076680004597\n",
            "Epoch 15762: train loss: 0.013126703910529613\n",
            "Epoch 15763: train loss: 0.013125337660312653\n",
            "Epoch 15764: train loss: 0.013123970478773117\n",
            "Epoch 15765: train loss: 0.013122593984007835\n",
            "Epoch 15766: train loss: 0.013121228665113449\n",
            "Epoch 15767: train loss: 0.01311985682696104\n",
            "Epoch 15768: train loss: 0.013118493370711803\n",
            "Epoch 15769: train loss: 0.013117124326527119\n",
            "Epoch 15770: train loss: 0.013115756213665009\n",
            "Epoch 15771: train loss: 0.013114385306835175\n",
            "Epoch 15772: train loss: 0.013113019987940788\n",
            "Epoch 15773: train loss: 0.013111649081110954\n",
            "Epoch 15774: train loss: 0.013110285624861717\n",
            "Epoch 15775: train loss: 0.013108919374644756\n",
            "Epoch 15776: train loss: 0.01310755591839552\n",
            "Epoch 15777: train loss: 0.013106189668178558\n",
            "Epoch 15778: train loss: 0.013104824349284172\n",
            "Epoch 15779: train loss: 0.01310345996171236\n",
            "Epoch 15780: train loss: 0.013102096505463123\n",
            "Epoch 15781: train loss: 0.013100727461278439\n",
            "Epoch 15782: train loss: 0.013099364005029202\n",
            "Epoch 15783: train loss: 0.013098003342747688\n",
            "Epoch 15784: train loss: 0.0130966417491436\n",
            "Epoch 15785: train loss: 0.013095277361571789\n",
            "Epoch 15786: train loss: 0.01309391763061285\n",
            "Epoch 15787: train loss: 0.01309254951775074\n",
            "Epoch 15788: train loss: 0.013091186061501503\n",
            "Epoch 15789: train loss: 0.013089824467897415\n",
            "Epoch 15790: train loss: 0.013088462874293327\n",
            "Epoch 15791: train loss: 0.013087104074656963\n",
            "Epoch 15792: train loss: 0.013085744343698025\n",
            "Epoch 15793: train loss: 0.013084381818771362\n",
            "Epoch 15794: train loss: 0.013083027675747871\n",
            "Epoch 15795: train loss: 0.013081665150821209\n",
            "Epoch 15796: train loss: 0.01308030728250742\n",
            "Epoch 15797: train loss: 0.013078943826258183\n",
            "Epoch 15798: train loss: 0.01307759340852499\n",
            "Epoch 15799: train loss: 0.013076231814920902\n",
            "Epoch 15800: train loss: 0.013074872083961964\n",
            "Epoch 15801: train loss: 0.01307351142168045\n",
            "Epoch 15802: train loss: 0.013072147965431213\n",
            "Epoch 15803: train loss: 0.013070796616375446\n",
            "Epoch 15804: train loss: 0.013069446198642254\n",
            "Epoch 15805: train loss: 0.01306807529181242\n",
            "Epoch 15806: train loss: 0.013066712766885757\n",
            "Epoch 15807: train loss: 0.01306536328047514\n",
            "Epoch 15808: train loss: 0.013064006343483925\n",
            "Epoch 15809: train loss: 0.01306265126913786\n",
            "Epoch 15810: train loss: 0.013061296194791794\n",
            "Epoch 15811: train loss: 0.013059939257800579\n",
            "Epoch 15812: train loss: 0.01305857952684164\n",
            "Epoch 15813: train loss: 0.013057228177785873\n",
            "Epoch 15814: train loss: 0.013055866584181786\n",
            "Epoch 15815: train loss: 0.013054517097771168\n",
            "Epoch 15816: train loss: 0.01305315550416708\n",
            "Epoch 15817: train loss: 0.013051804155111313\n",
            "Epoch 15818: train loss: 0.01305045373737812\n",
            "Epoch 15819: train loss: 0.013049095869064331\n",
            "Epoch 15820: train loss: 0.013047744520008564\n",
            "Epoch 15821: train loss: 0.013046395033597946\n",
            "Epoch 15822: train loss: 0.013045033439993858\n",
            "Epoch 15823: train loss: 0.013043680228292942\n",
            "Epoch 15824: train loss: 0.013042323291301727\n",
            "Epoch 15825: train loss: 0.013040968216955662\n",
            "Epoch 15826: train loss: 0.013039618730545044\n",
            "Epoch 15827: train loss: 0.013038262724876404\n",
            "Epoch 15828: train loss: 0.013036916963756084\n",
            "Epoch 15829: train loss: 0.01303557213395834\n",
            "Epoch 15830: train loss: 0.013034213334321976\n",
            "Epoch 15831: train loss: 0.013032861985266209\n",
            "Epoch 15832: train loss: 0.01303151622414589\n",
            "Epoch 15833: train loss: 0.013030162081122398\n",
            "Epoch 15834: train loss: 0.013028811663389206\n",
            "Epoch 15835: train loss: 0.01302747055888176\n",
            "Epoch 15836: train loss: 0.013026117347180843\n",
            "Epoch 15837: train loss: 0.013024771586060524\n",
            "Epoch 15838: train loss: 0.01302342303097248\n",
            "Epoch 15839: train loss: 0.013022080063819885\n",
            "Epoch 15840: train loss: 0.01302072312682867\n",
            "Epoch 15841: train loss: 0.013019382022321224\n",
            "Epoch 15842: train loss: 0.013018031604588032\n",
            "Epoch 15843: train loss: 0.013016690500080585\n",
            "Epoch 15844: train loss: 0.013015341013669968\n",
            "Epoch 15845: train loss: 0.013013998046517372\n",
            "Epoch 15846: train loss: 0.013012643903493881\n",
            "Epoch 15847: train loss: 0.013011307455599308\n",
            "Epoch 15848: train loss: 0.013009962625801563\n",
            "Epoch 15849: train loss: 0.013008618727326393\n",
            "Epoch 15850: train loss: 0.01300727017223835\n",
            "Epoch 15851: train loss: 0.013005922548472881\n",
            "Epoch 15852: train loss: 0.013004579581320286\n",
            "Epoch 15853: train loss: 0.013003239408135414\n",
            "Epoch 15854: train loss: 0.013001895509660244\n",
            "Epoch 15855: train loss: 0.013000547885894775\n",
            "Epoch 15856: train loss: 0.012999210506677628\n",
            "Epoch 15857: train loss: 0.012997862882912159\n",
            "Epoch 15858: train loss: 0.012996520847082138\n",
            "Epoch 15859: train loss: 0.01299517322331667\n",
            "Epoch 15860: train loss: 0.012993831187486649\n",
            "Epoch 15861: train loss: 0.012992491014301777\n",
            "Epoch 15862: train loss: 0.012991145253181458\n",
            "Epoch 15863: train loss: 0.012989804148674011\n",
            "Epoch 15864: train loss: 0.01298846211284399\n",
            "Epoch 15865: train loss: 0.012987119145691395\n",
            "Epoch 15866: train loss: 0.012985777109861374\n",
            "Epoch 15867: train loss: 0.012984443455934525\n",
            "Epoch 15868: train loss: 0.012983094900846481\n",
            "Epoch 15869: train loss: 0.012981755658984184\n",
            "Epoch 15870: train loss: 0.01298041082918644\n",
            "Epoch 15871: train loss: 0.012979067862033844\n",
            "Epoch 15872: train loss: 0.012977729551494122\n",
            "Epoch 15873: train loss: 0.01297638937830925\n",
            "Epoch 15874: train loss: 0.012975050136446953\n",
            "Epoch 15875: train loss: 0.012973706237971783\n",
            "Epoch 15876: train loss: 0.012972371652722359\n",
            "Epoch 15877: train loss: 0.012971028685569763\n",
            "Epoch 15878: train loss: 0.012969695031642914\n",
            "Epoch 15879: train loss: 0.012968356721103191\n",
            "Epoch 15880: train loss: 0.012967019341886044\n",
            "Epoch 15881: train loss: 0.012965688481926918\n",
            "Epoch 15882: train loss: 0.01296435296535492\n",
            "Epoch 15883: train loss: 0.012963013723492622\n",
            "Epoch 15884: train loss: 0.012961680069565773\n",
            "Epoch 15885: train loss: 0.012960344552993774\n",
            "Epoch 15886: train loss: 0.012959009036421776\n",
            "Epoch 15887: train loss: 0.01295767817646265\n",
            "Epoch 15888: train loss: 0.012956349179148674\n",
            "Epoch 15889: train loss: 0.012955006211996078\n",
            "Epoch 15890: train loss: 0.012953671626746655\n",
            "Epoch 15891: train loss: 0.012952339835464954\n",
            "Epoch 15892: train loss: 0.012951010838150978\n",
            "Epoch 15893: train loss: 0.012949681840837002\n",
            "Epoch 15894: train loss: 0.012948347255587578\n",
            "Epoch 15895: train loss: 0.01294701173901558\n",
            "Epoch 15896: train loss: 0.012945685535669327\n",
            "Epoch 15897: train loss: 0.012944351881742477\n",
            "Epoch 15898: train loss: 0.012943017296493053\n",
            "Epoch 15899: train loss: 0.012941687367856503\n",
            "Epoch 15900: train loss: 0.01294035091996193\n",
            "Epoch 15901: train loss: 0.012939018197357655\n",
            "Epoch 15902: train loss: 0.012937689200043678\n",
            "Epoch 15903: train loss: 0.012936362065374851\n",
            "Epoch 15904: train loss: 0.012935028411448002\n",
            "Epoch 15905: train loss: 0.012933693826198578\n",
            "Epoch 15906: train loss: 0.01293236669152975\n",
            "Epoch 15907: train loss: 0.012931035831570625\n",
            "Epoch 15908: train loss: 0.012929708696901798\n",
            "Epoch 15909: train loss: 0.012928379699587822\n",
            "Epoch 15910: train loss: 0.012927055358886719\n",
            "Epoch 15911: train loss: 0.012925722636282444\n",
            "Epoch 15912: train loss: 0.012924391776323318\n",
            "Epoch 15913: train loss: 0.012923064641654491\n",
            "Epoch 15914: train loss: 0.012921733781695366\n",
            "Epoch 15915: train loss: 0.01292040292173624\n",
            "Epoch 15916: train loss: 0.012919073924422264\n",
            "Epoch 15917: train loss: 0.012917746789753437\n",
            "Epoch 15918: train loss: 0.012916414998471737\n",
            "Epoch 15919: train loss: 0.012915097177028656\n",
            "Epoch 15920: train loss: 0.012913764454424381\n",
            "Epoch 15921: train loss: 0.012912439182400703\n",
            "Epoch 15922: train loss: 0.012911113910377026\n",
            "Epoch 15923: train loss: 0.012909788638353348\n",
            "Epoch 15924: train loss: 0.012908458709716797\n",
            "Epoch 15925: train loss: 0.012907136231660843\n",
            "Epoch 15926: train loss: 0.012905810959637165\n",
            "Epoch 15927: train loss: 0.012904489412903786\n",
            "Epoch 15928: train loss: 0.012903162278234959\n",
            "Epoch 15929: train loss: 0.012901834212243557\n",
            "Epoch 15930: train loss: 0.012900511734187603\n",
            "Epoch 15931: train loss: 0.012899189256131649\n",
            "Epoch 15932: train loss: 0.012897870503365993\n",
            "Epoch 15933: train loss: 0.012896545231342316\n",
            "Epoch 15934: train loss: 0.01289522647857666\n",
            "Epoch 15935: train loss: 0.01289390493184328\n",
            "Epoch 15936: train loss: 0.012892590835690498\n",
            "Epoch 15937: train loss: 0.012891259975731373\n",
            "Epoch 15938: train loss: 0.012889942154288292\n",
            "Epoch 15939: train loss: 0.01288861595094204\n",
            "Epoch 15940: train loss: 0.012887297198176384\n",
            "Epoch 15941: train loss: 0.012885970994830132\n",
            "Epoch 15942: train loss: 0.012884663417935371\n",
            "Epoch 15943: train loss: 0.012883339077234268\n",
            "Epoch 15944: train loss: 0.012882021255791187\n",
            "Epoch 15945: train loss: 0.012880700640380383\n",
            "Epoch 15946: train loss: 0.012879379093647003\n",
            "Epoch 15947: train loss: 0.01287805661559105\n",
            "Epoch 15948: train loss: 0.012876740656793118\n",
            "Epoch 15949: train loss: 0.012875421904027462\n",
            "Epoch 15950: train loss: 0.012874108739197254\n",
            "Epoch 15951: train loss: 0.01287278812378645\n",
            "Epoch 15952: train loss: 0.012871469371020794\n",
            "Epoch 15953: train loss: 0.012870152480900288\n",
            "Epoch 15954: train loss: 0.012868826277554035\n",
            "Epoch 15955: train loss: 0.0128675177693367\n",
            "Epoch 15956: train loss: 0.012866203673183918\n",
            "Epoch 15957: train loss: 0.012864884920418262\n",
            "Epoch 15958: train loss: 0.012863566167652607\n",
            "Epoch 15959: train loss: 0.01286225114017725\n",
            "Epoch 15960: train loss: 0.012860936112701893\n",
            "Epoch 15961: train loss: 0.012859613634645939\n",
            "Epoch 15962: train loss: 0.012858301401138306\n",
            "Epoch 15963: train loss: 0.012856989167630672\n",
            "Epoch 15964: train loss: 0.012855670414865017\n",
            "Epoch 15965: train loss: 0.012854350730776787\n",
            "Epoch 15966: train loss: 0.012853042222559452\n",
            "Epoch 15967: train loss: 0.012851724401116371\n",
            "Epoch 15968: train loss: 0.012850413098931313\n",
            "Epoch 15969: train loss: 0.012849102728068829\n",
            "Epoch 15970: train loss: 0.012847783043980598\n",
            "Epoch 15971: train loss: 0.012846478261053562\n",
            "Epoch 15972: train loss: 0.01284516230225563\n",
            "Epoch 15973: train loss: 0.012843849137425423\n",
            "Epoch 15974: train loss: 0.012842535972595215\n",
            "Epoch 15975: train loss: 0.012841224670410156\n",
            "Epoch 15976: train loss: 0.012839916162192822\n",
            "Epoch 15977: train loss: 0.012838601134717464\n",
            "Epoch 15978: train loss: 0.012837296351790428\n",
            "Epoch 15979: train loss: 0.012835982255637646\n",
            "Epoch 15980: train loss: 0.012834668159484863\n",
            "Epoch 15981: train loss: 0.012833363376557827\n",
            "Epoch 15982: train loss: 0.01283204648643732\n",
            "Epoch 15983: train loss: 0.012830741703510284\n",
            "Epoch 15984: train loss: 0.012829432263970375\n",
            "Epoch 15985: train loss: 0.012828119099140167\n",
            "Epoch 15986: train loss: 0.01282680593430996\n",
            "Epoch 15987: train loss: 0.012825507670640945\n",
            "Epoch 15988: train loss: 0.01282420102506876\n",
            "Epoch 15989: train loss: 0.01282289158552885\n",
            "Epoch 15990: train loss: 0.012821580283343792\n",
            "Epoch 15991: train loss: 0.01282027643173933\n",
            "Epoch 15992: train loss: 0.012818966060876846\n",
            "Epoch 15993: train loss: 0.012817658483982086\n",
            "Epoch 15994: train loss: 0.012816350907087326\n",
            "Epoch 15995: train loss: 0.012815050780773163\n",
            "Epoch 15996: train loss: 0.012813740409910679\n",
            "Epoch 15997: train loss: 0.01281243097037077\n",
            "Epoch 15998: train loss: 0.012811131775379181\n",
            "Epoch 15999: train loss: 0.01280982419848442\n",
            "Epoch 16000: train loss: 0.012808517552912235\n",
            "Epoch 16001: train loss: 0.012807212769985199\n",
            "Epoch 16002: train loss: 0.012805907987058163\n",
            "Epoch 16003: train loss: 0.012804606929421425\n",
            "Epoch 16004: train loss: 0.012803299352526665\n",
            "Epoch 16005: train loss: 0.012801993638277054\n",
            "Epoch 16006: train loss: 0.012800690717995167\n",
            "Epoch 16007: train loss: 0.012799386866390705\n",
            "Epoch 16008: train loss: 0.012798081152141094\n",
            "Epoch 16009: train loss: 0.012796778231859207\n",
            "Epoch 16010: train loss: 0.012795473448932171\n",
            "Epoch 16011: train loss: 0.012794176116585732\n",
            "Epoch 16012: train loss: 0.01279287040233612\n",
            "Epoch 16013: train loss: 0.012791567482054234\n",
            "Epoch 16014: train loss: 0.012790268287062645\n",
            "Epoch 16015: train loss: 0.012788967229425907\n",
            "Epoch 16016: train loss: 0.01278766430914402\n",
            "Epoch 16017: train loss: 0.01278636883944273\n",
            "Epoch 16018: train loss: 0.012785065919160843\n",
            "Epoch 16019: train loss: 0.012783772312104702\n",
            "Epoch 16020: train loss: 0.01278247032314539\n",
            "Epoch 16021: train loss: 0.012781169265508652\n",
            "Epoch 16022: train loss: 0.012779874727129936\n",
            "Epoch 16023: train loss: 0.01277858018875122\n",
            "Epoch 16024: train loss: 0.012777275405824184\n",
            "Epoch 16025: train loss: 0.012775976210832596\n",
            "Epoch 16026: train loss: 0.012774679809808731\n",
            "Epoch 16027: train loss: 0.012773379683494568\n",
            "Epoch 16028: train loss: 0.012772075831890106\n",
            "Epoch 16029: train loss: 0.012770785950124264\n",
            "Epoch 16030: train loss: 0.012769483961164951\n",
            "Epoch 16031: train loss: 0.012768186628818512\n",
            "Epoch 16032: train loss: 0.012766887433826923\n",
            "Epoch 16033: train loss: 0.012765591032803059\n",
            "Epoch 16034: train loss: 0.012764294631779194\n",
            "Epoch 16035: train loss: 0.01276299450546503\n",
            "Epoch 16036: train loss: 0.012761693447828293\n",
            "Epoch 16037: train loss: 0.012760400772094727\n",
            "Epoch 16038: train loss: 0.012759104371070862\n",
            "Epoch 16039: train loss: 0.01275781262665987\n",
            "Epoch 16040: train loss: 0.012756512500345707\n",
            "Epoch 16041: train loss: 0.012755228206515312\n",
            "Epoch 16042: train loss: 0.012753927148878574\n",
            "Epoch 16043: train loss: 0.012752636335790157\n",
            "Epoch 16044: train loss: 0.012751339934766293\n",
            "Epoch 16045: train loss: 0.012750046327710152\n",
            "Epoch 16046: train loss: 0.012748749926686287\n",
            "Epoch 16047: train loss: 0.012747451663017273\n",
            "Epoch 16048: train loss: 0.01274616364389658\n",
            "Epoch 16049: train loss: 0.012744873762130737\n",
            "Epoch 16050: train loss: 0.012743576429784298\n",
            "Epoch 16051: train loss: 0.012742288410663605\n",
            "Epoch 16052: train loss: 0.01274099200963974\n",
            "Epoch 16053: train loss: 0.012739707715809345\n",
            "Epoch 16054: train loss: 0.012738416902720928\n",
            "Epoch 16055: train loss: 0.012737130746245384\n",
            "Epoch 16056: train loss: 0.012735830619931221\n",
            "Epoch 16057: train loss: 0.012734544463455677\n",
            "Epoch 16058: train loss: 0.012733250856399536\n",
            "Epoch 16059: train loss: 0.012731960043311119\n",
            "Epoch 16060: train loss: 0.012730671092867851\n",
            "Epoch 16061: train loss: 0.012729390524327755\n",
            "Epoch 16062: train loss: 0.012728102505207062\n",
            "Epoch 16063: train loss: 0.012726805172860622\n",
            "Epoch 16064: train loss: 0.012725524604320526\n",
            "Epoch 16065: train loss: 0.012724232859909534\n",
            "Epoch 16066: train loss: 0.01272294856607914\n",
            "Epoch 16067: train loss: 0.012721660546958447\n",
            "Epoch 16068: train loss: 0.012720376253128052\n",
            "Epoch 16069: train loss: 0.01271908637136221\n",
            "Epoch 16070: train loss: 0.012717798352241516\n",
            "Epoch 16071: train loss: 0.012716510333120823\n",
            "Epoch 16072: train loss: 0.012715219520032406\n",
            "Epoch 16073: train loss: 0.012713943608105183\n",
            "Epoch 16074: train loss: 0.012712651863694191\n",
            "Epoch 16075: train loss: 0.012711367569863796\n",
            "Epoch 16076: train loss: 0.012710082344710827\n",
            "Epoch 16077: train loss: 0.012708796188235283\n",
            "Epoch 16078: train loss: 0.012707507237792015\n",
            "Epoch 16079: train loss: 0.01270622294396162\n",
            "Epoch 16080: train loss: 0.012704947963356972\n",
            "Epoch 16081: train loss: 0.012703659944236279\n",
            "Epoch 16082: train loss: 0.012702369131147861\n",
            "Epoch 16083: train loss: 0.012701083905994892\n",
            "Epoch 16084: train loss: 0.01269980613142252\n",
            "Epoch 16085: train loss: 0.01269852090626955\n",
            "Epoch 16086: train loss: 0.012697235681116581\n",
            "Epoch 16087: train loss: 0.012695950455963612\n",
            "Epoch 16088: train loss: 0.012694667093455791\n",
            "Epoch 16089: train loss: 0.012693380005657673\n",
            "Epoch 16090: train loss: 0.012692097574472427\n",
            "Epoch 16091: train loss: 0.012690817937254906\n",
            "Epoch 16092: train loss: 0.012689531780779362\n",
            "Epoch 16093: train loss: 0.01268825214356184\n",
            "Epoch 16094: train loss: 0.012686969712376595\n",
            "Epoch 16095: train loss: 0.0126856854185462\n",
            "Epoch 16096: train loss: 0.012684411369264126\n",
            "Epoch 16097: train loss: 0.012683126144111156\n",
            "Epoch 16098: train loss: 0.012681853957474232\n",
            "Epoch 16099: train loss: 0.012680570594966412\n",
            "Epoch 16100: train loss: 0.012679291889071465\n",
            "Epoch 16101: train loss: 0.012678009457886219\n",
            "Epoch 16102: train loss: 0.012676731683313847\n",
            "Epoch 16103: train loss: 0.012675449252128601\n",
            "Epoch 16104: train loss: 0.012674175202846527\n",
            "Epoch 16105: train loss: 0.012672890909016132\n",
            "Epoch 16106: train loss: 0.012671619653701782\n",
            "Epoch 16107: train loss: 0.012670342810451984\n",
            "Epoch 16108: train loss: 0.01266906876116991\n",
            "Epoch 16109: train loss: 0.012667793780565262\n",
            "Epoch 16110: train loss: 0.012666513212025166\n",
            "Epoch 16111: train loss: 0.012665241956710815\n",
            "Epoch 16112: train loss: 0.012663960456848145\n",
            "Epoch 16113: train loss: 0.012662683613598347\n",
            "Epoch 16114: train loss: 0.01266141515225172\n",
            "Epoch 16115: train loss: 0.012660139240324497\n",
            "Epoch 16116: train loss: 0.012658865191042423\n",
            "Epoch 16117: train loss: 0.012657584622502327\n",
            "Epoch 16118: train loss: 0.012656314298510551\n",
            "Epoch 16119: train loss: 0.012655040249228477\n",
            "Epoch 16120: train loss: 0.012653767131268978\n",
            "Epoch 16121: train loss: 0.012652494944632053\n",
            "Epoch 16122: train loss: 0.01265121903270483\n",
            "Epoch 16123: train loss: 0.012649954296648502\n",
            "Epoch 16124: train loss: 0.01264867652207613\n",
            "Epoch 16125: train loss: 0.01264740340411663\n",
            "Epoch 16126: train loss: 0.012646126560866833\n",
            "Epoch 16127: train loss: 0.012644852511584759\n",
            "Epoch 16128: train loss: 0.01264357939362526\n",
            "Epoch 16129: train loss: 0.012642309069633484\n",
            "Epoch 16130: train loss: 0.012641040608286858\n",
            "Epoch 16131: train loss: 0.012639766559004784\n",
            "Epoch 16132: train loss: 0.01263849064707756\n",
            "Epoch 16133: train loss: 0.012637225911021233\n",
            "Epoch 16134: train loss: 0.012635944411158562\n",
            "Epoch 16135: train loss: 0.01263467501848936\n",
            "Epoch 16136: train loss: 0.012633406557142735\n",
            "Epoch 16137: train loss: 0.012632136233150959\n",
            "Epoch 16138: train loss: 0.01263086311519146\n",
            "Epoch 16139: train loss: 0.012629595585167408\n",
            "Epoch 16140: train loss: 0.012628323398530483\n",
            "Epoch 16141: train loss: 0.012627050280570984\n",
            "Epoch 16142: train loss: 0.012625783681869507\n",
            "Epoch 16143: train loss: 0.012624513357877731\n",
            "Epoch 16144: train loss: 0.01262324582785368\n",
            "Epoch 16145: train loss: 0.012621969915926456\n",
            "Epoch 16146: train loss: 0.01262070331722498\n",
            "Epoch 16147: train loss: 0.012619437649846077\n",
            "Epoch 16148: train loss: 0.012618165463209152\n",
            "Epoch 16149: train loss: 0.0126168979331851\n",
            "Epoch 16150: train loss: 0.01261562667787075\n",
            "Epoch 16151: train loss: 0.012614361941814423\n",
            "Epoch 16152: train loss: 0.012613097205758095\n",
            "Epoch 16153: train loss: 0.012611832469701767\n",
            "Epoch 16154: train loss: 0.012610568664968014\n",
            "Epoch 16155: train loss: 0.012609307654201984\n",
            "Epoch 16156: train loss: 0.012608043849468231\n",
            "Epoch 16157: train loss: 0.012606779113411903\n",
            "Epoch 16158: train loss: 0.012605514377355576\n",
            "Epoch 16159: train loss: 0.012604248709976673\n",
            "Epoch 16160: train loss: 0.012602994218468666\n",
            "Epoch 16161: train loss: 0.01260172389447689\n",
            "Epoch 16162: train loss: 0.012600461021065712\n",
            "Epoch 16163: train loss: 0.012599200010299683\n",
            "Epoch 16164: train loss: 0.012597940862178802\n",
            "Epoch 16165: train loss: 0.012596677988767624\n",
            "Epoch 16166: train loss: 0.012595409527420998\n",
            "Epoch 16167: train loss: 0.01259415689855814\n",
            "Epoch 16168: train loss: 0.012592894025146961\n",
            "Epoch 16169: train loss: 0.012591630220413208\n",
            "Epoch 16170: train loss: 0.01259036734700203\n",
            "Epoch 16171: train loss: 0.012589103542268276\n",
            "Epoch 16172: train loss: 0.012587842531502247\n",
            "Epoch 16173: train loss: 0.012586582452058792\n",
            "Epoch 16174: train loss: 0.012585317716002464\n",
            "Epoch 16175: train loss: 0.01258405763655901\n",
            "Epoch 16176: train loss: 0.012582801282405853\n",
            "Epoch 16177: train loss: 0.012581534683704376\n",
            "Epoch 16178: train loss: 0.012580277398228645\n",
            "Epoch 16179: train loss: 0.012579022906720638\n",
            "Epoch 16180: train loss: 0.012577757239341736\n",
            "Epoch 16181: train loss: 0.012576499953866005\n",
            "Epoch 16182: train loss: 0.012575238943099976\n",
            "Epoch 16183: train loss: 0.012573977001011372\n",
            "Epoch 16184: train loss: 0.01257272157818079\n",
            "Epoch 16185: train loss: 0.012571461498737335\n",
            "Epoch 16186: train loss: 0.012570207938551903\n",
            "Epoch 16187: train loss: 0.01256894413381815\n",
            "Epoch 16188: train loss: 0.012567686848342419\n",
            "Epoch 16189: train loss: 0.012566431425511837\n",
            "Epoch 16190: train loss: 0.012565168552100658\n",
            "Epoch 16191: train loss: 0.012563918717205524\n",
            "Epoch 16192: train loss: 0.012562659569084644\n",
            "Epoch 16193: train loss: 0.012561402283608913\n",
            "Epoch 16194: train loss: 0.012560144998133183\n",
            "Epoch 16195: train loss: 0.012558890506625175\n",
            "Epoch 16196: train loss: 0.012557633221149445\n",
            "Epoch 16197: train loss: 0.012556375935673714\n",
            "Epoch 16198: train loss: 0.012555116787552834\n",
            "Epoch 16199: train loss: 0.012553852051496506\n",
            "Epoch 16200: train loss: 0.012552608735859394\n",
            "Epoch 16201: train loss: 0.012551353313028812\n",
            "Epoch 16202: train loss: 0.01255009975284338\n",
            "Epoch 16203: train loss: 0.012548844330012798\n",
            "Epoch 16204: train loss: 0.012547596357762814\n",
            "Epoch 16205: train loss: 0.012546336278319359\n",
            "Epoch 16206: train loss: 0.0125450873747468\n",
            "Epoch 16207: train loss: 0.012543837539851665\n",
            "Epoch 16208: train loss: 0.012542584910988808\n",
            "Epoch 16209: train loss: 0.012541331350803375\n",
            "Epoch 16210: train loss: 0.012540087103843689\n",
            "Epoch 16211: train loss: 0.012538829818367958\n",
            "Epoch 16212: train loss: 0.012537579983472824\n",
            "Epoch 16213: train loss: 0.01253633201122284\n",
            "Epoch 16214: train loss: 0.012535079382359982\n",
            "Epoch 16215: train loss: 0.012533833272755146\n",
            "Epoch 16216: train loss: 0.012532582506537437\n",
            "Epoch 16217: train loss: 0.012531327083706856\n",
            "Epoch 16218: train loss: 0.012530081905424595\n",
            "Epoch 16219: train loss: 0.012528828345239162\n",
            "Epoch 16220: train loss: 0.012527581304311752\n",
            "Epoch 16221: train loss: 0.012526334263384342\n",
            "Epoch 16222: train loss: 0.012525086291134357\n",
            "Epoch 16223: train loss: 0.012523836456239223\n",
            "Epoch 16224: train loss: 0.012522588483989239\n",
            "Epoch 16225: train loss: 0.01252133958041668\n",
            "Epoch 16226: train loss: 0.01252009253948927\n",
            "Epoch 16227: train loss: 0.012518839910626411\n",
            "Epoch 16228: train loss: 0.0125175965949893\n",
            "Epoch 16229: train loss: 0.01251634955406189\n",
            "Epoch 16230: train loss: 0.012515099719166756\n",
            "Epoch 16231: train loss: 0.012513852678239346\n",
            "Epoch 16232: train loss: 0.012512609362602234\n",
            "Epoch 16233: train loss: 0.012511360459029675\n",
            "Epoch 16234: train loss: 0.012510111555457115\n",
            "Epoch 16235: train loss: 0.012508869171142578\n",
            "Epoch 16236: train loss: 0.012507621198892593\n",
            "Epoch 16237: train loss: 0.012506376020610332\n",
            "Epoch 16238: train loss: 0.012505128979682922\n",
            "Epoch 16239: train loss: 0.012503881007432938\n",
            "Epoch 16240: train loss: 0.0125026386231184\n",
            "Epoch 16241: train loss: 0.01250139344483614\n",
            "Epoch 16242: train loss: 0.012500147335231304\n",
            "Epoch 16243: train loss: 0.012498905882239342\n",
            "Epoch 16244: train loss: 0.012497656047344208\n",
            "Epoch 16245: train loss: 0.012496414594352245\n",
            "Epoch 16246: train loss: 0.012495167553424835\n",
            "Epoch 16247: train loss: 0.012493927031755447\n",
            "Epoch 16248: train loss: 0.012492687441408634\n",
            "Epoch 16249: train loss: 0.012491435743868351\n",
            "Epoch 16250: train loss: 0.012490198016166687\n",
            "Epoch 16251: train loss: 0.012488958425819874\n",
            "Epoch 16252: train loss: 0.012487716041505337\n",
            "Epoch 16253: train loss: 0.012486472725868225\n",
            "Epoch 16254: train loss: 0.012485231272876263\n",
            "Epoch 16255: train loss: 0.012483986094594002\n",
            "Epoch 16256: train loss: 0.012482751160860062\n",
            "Epoch 16257: train loss: 0.012481508776545525\n",
            "Epoch 16258: train loss: 0.012480273842811584\n",
            "Epoch 16259: train loss: 0.012479030527174473\n",
            "Epoch 16260: train loss: 0.012477787211537361\n",
            "Epoch 16261: train loss: 0.012476553209125996\n",
            "Epoch 16262: train loss: 0.01247530709952116\n",
            "Epoch 16263: train loss: 0.01247407030314207\n",
            "Epoch 16264: train loss: 0.012472833506762981\n",
            "Epoch 16265: train loss: 0.012471594847738743\n",
            "Epoch 16266: train loss: 0.012470360845327377\n",
            "Epoch 16267: train loss: 0.012469122186303139\n",
            "Epoch 16268: train loss: 0.012467887252569199\n",
            "Epoch 16269: train loss: 0.012466653250157833\n",
            "Epoch 16270: train loss: 0.012465410865843296\n",
            "Epoch 16271: train loss: 0.012464175000786781\n",
            "Epoch 16272: train loss: 0.01246294379234314\n",
            "Epoch 16273: train loss: 0.012461704201996326\n",
            "Epoch 16274: train loss: 0.012460466474294662\n",
            "Epoch 16275: train loss: 0.012459231540560722\n",
            "Epoch 16276: train loss: 0.012457995675504208\n",
            "Epoch 16277: train loss: 0.012456764467060566\n",
            "Epoch 16278: train loss: 0.012455525808036327\n",
            "Epoch 16279: train loss: 0.012454282492399216\n",
            "Epoch 16280: train loss: 0.012453050352633\n",
            "Epoch 16281: train loss: 0.012451816350221634\n",
            "Epoch 16282: train loss: 0.012450586073100567\n",
            "Epoch 16283: train loss: 0.012449349276721478\n",
            "Epoch 16284: train loss: 0.012448116205632687\n",
            "Epoch 16285: train loss: 0.012446878477931023\n",
            "Epoch 16286: train loss: 0.012445641681551933\n",
            "Epoch 16287: train loss: 0.012444407679140568\n",
            "Epoch 16288: train loss: 0.012443175539374352\n",
            "Epoch 16289: train loss: 0.01244194433093071\n",
            "Epoch 16290: train loss: 0.012440706603229046\n",
            "Epoch 16291: train loss: 0.012439475394785404\n",
            "Epoch 16292: train loss: 0.01243823766708374\n",
            "Epoch 16293: train loss: 0.012437010183930397\n",
            "Epoch 16294: train loss: 0.01243577804416418\n",
            "Epoch 16295: train loss: 0.012434548698365688\n",
            "Epoch 16296: train loss: 0.012433315627276897\n",
            "Epoch 16297: train loss: 0.012432088144123554\n",
            "Epoch 16298: train loss: 0.012430853210389614\n",
            "Epoch 16299: train loss: 0.012429624795913696\n",
            "Epoch 16300: train loss: 0.012428399175405502\n",
            "Epoch 16301: train loss: 0.012427167035639286\n",
            "Epoch 16302: train loss: 0.012425940483808517\n",
            "Epoch 16303: train loss: 0.012424708344042301\n",
            "Epoch 16304: train loss: 0.012423480860888958\n",
            "Epoch 16305: train loss: 0.012422248721122742\n",
            "Epoch 16306: train loss: 0.012421022169291973\n",
            "Epoch 16307: train loss: 0.012419791892170906\n",
            "Epoch 16308: train loss: 0.012418565340340137\n",
            "Epoch 16309: train loss: 0.012417337857186794\n",
            "Epoch 16310: train loss: 0.012416107580065727\n",
            "Epoch 16311: train loss: 0.012414878234267235\n",
            "Epoch 16312: train loss: 0.012413647957146168\n",
            "Epoch 16313: train loss: 0.0124124214053154\n",
            "Epoch 16314: train loss: 0.012411198578774929\n",
            "Epoch 16315: train loss: 0.012409972958266735\n",
            "Epoch 16316: train loss: 0.012408742681145668\n",
            "Epoch 16317: train loss: 0.012407513335347176\n",
            "Epoch 16318: train loss: 0.012406288646161556\n",
            "Epoch 16319: train loss: 0.012405059300363064\n",
            "Epoch 16320: train loss: 0.012403834611177444\n",
            "Epoch 16321: train loss: 0.012402611784636974\n",
            "Epoch 16322: train loss: 0.012401393614709377\n",
            "Epoch 16323: train loss: 0.012400164268910885\n",
            "Epoch 16324: train loss: 0.012398938648402691\n",
            "Epoch 16325: train loss: 0.012397716753184795\n",
            "Epoch 16326: train loss: 0.01239649299532175\n",
            "Epoch 16327: train loss: 0.012395267374813557\n",
            "Epoch 16328: train loss: 0.01239404920488596\n",
            "Epoch 16329: train loss: 0.012392823584377766\n",
            "Epoch 16330: train loss: 0.01239160168915987\n",
            "Epoch 16331: train loss: 0.012390377931296825\n",
            "Epoch 16332: train loss: 0.012389158830046654\n",
            "Epoch 16333: train loss: 0.012387930415570736\n",
            "Epoch 16334: train loss: 0.012386709451675415\n",
            "Epoch 16335: train loss: 0.012385488487780094\n",
            "Epoch 16336: train loss: 0.012384263798594475\n",
            "Epoch 16337: train loss: 0.012383048422634602\n",
            "Epoch 16338: train loss: 0.012381820939481258\n",
            "Epoch 16339: train loss: 0.012380603700876236\n",
            "Epoch 16340: train loss: 0.012379378080368042\n",
            "Epoch 16341: train loss: 0.012378156185150146\n",
            "Epoch 16342: train loss: 0.012376943603157997\n",
            "Epoch 16343: train loss: 0.012375717982649803\n",
            "Epoch 16344: train loss: 0.012374498881399632\n",
            "Epoch 16345: train loss: 0.012373276986181736\n",
            "Epoch 16346: train loss: 0.012372058816254139\n",
            "Epoch 16347: train loss: 0.012370841577649117\n",
            "Epoch 16348: train loss: 0.012369617819786072\n",
            "Epoch 16349: train loss: 0.012368403375148773\n",
            "Epoch 16350: train loss: 0.01236718613654375\n",
            "Epoch 16351: train loss: 0.012365967035293579\n",
            "Epoch 16352: train loss: 0.012364749796688557\n",
            "Epoch 16353: train loss: 0.012363536283373833\n",
            "Epoch 16354: train loss: 0.01236230693757534\n",
            "Epoch 16355: train loss: 0.012361101806163788\n",
            "Epoch 16356: train loss: 0.012359879910945892\n",
            "Epoch 16357: train loss: 0.01235866267234087\n",
            "Epoch 16358: train loss: 0.012357449159026146\n",
            "Epoch 16359: train loss: 0.012356230989098549\n",
            "Epoch 16360: train loss: 0.012355017475783825\n",
            "Epoch 16361: train loss: 0.012353803962469101\n",
            "Epoch 16362: train loss: 0.012352589517831802\n",
            "Epoch 16363: train loss: 0.01235137414187193\n",
            "Epoch 16364: train loss: 0.01235015969723463\n",
            "Epoch 16365: train loss: 0.012348943389952183\n",
            "Epoch 16366: train loss: 0.012347729876637459\n",
            "Epoch 16367: train loss: 0.012346516363322735\n",
            "Epoch 16368: train loss: 0.012345303781330585\n",
            "Epoch 16369: train loss: 0.012344086542725563\n",
            "Epoch 16370: train loss: 0.012342876754701138\n",
            "Epoch 16371: train loss: 0.01234165858477354\n",
            "Epoch 16372: train loss: 0.012340446002781391\n",
            "Epoch 16373: train loss: 0.012339235283434391\n",
            "Epoch 16374: train loss: 0.01233801431953907\n",
            "Epoch 16375: train loss: 0.01233680360019207\n",
            "Epoch 16376: train loss: 0.012335594743490219\n",
            "Epoch 16377: train loss: 0.012334375642240047\n",
            "Epoch 16378: train loss: 0.012333166785538197\n",
            "Epoch 16379: train loss: 0.012331959791481495\n",
            "Epoch 16380: train loss: 0.012330742552876472\n",
            "Epoch 16381: train loss: 0.012329529970884323\n",
            "Epoch 16382: train loss: 0.012328320182859898\n",
            "Epoch 16383: train loss: 0.012327116914093494\n",
            "Epoch 16384: train loss: 0.012325901538133621\n",
            "Epoch 16385: train loss: 0.012324689887464046\n",
            "Epoch 16386: train loss: 0.012323474511504173\n",
            "Epoch 16387: train loss: 0.01232226938009262\n",
            "Epoch 16388: train loss: 0.012321057729423046\n",
            "Epoch 16389: train loss: 0.012319847010076046\n",
            "Epoch 16390: train loss: 0.012318640016019344\n",
            "Epoch 16391: train loss: 0.012317432090640068\n",
            "Epoch 16392: train loss: 0.012316224165260792\n",
            "Epoch 16393: train loss: 0.01231500692665577\n",
            "Epoch 16394: train loss: 0.01231380645185709\n",
            "Epoch 16395: train loss: 0.012312602251768112\n",
            "Epoch 16396: train loss: 0.012311388738453388\n",
            "Epoch 16397: train loss: 0.01231018640100956\n",
            "Epoch 16398: train loss: 0.012308977544307709\n",
            "Epoch 16399: train loss: 0.012307774275541306\n",
            "Epoch 16400: train loss: 0.01230656448751688\n",
            "Epoch 16401: train loss: 0.012305363081395626\n",
            "Epoch 16402: train loss: 0.01230415515601635\n",
            "Epoch 16403: train loss: 0.012302952818572521\n",
            "Epoch 16404: train loss: 0.012301743030548096\n",
            "Epoch 16405: train loss: 0.012300544418394566\n",
            "Epoch 16406: train loss: 0.012299342080950737\n",
            "Epoch 16407: train loss: 0.012298139743506908\n",
            "Epoch 16408: train loss: 0.012296926230192184\n",
            "Epoch 16409: train loss: 0.012295727618038654\n",
            "Epoch 16410: train loss: 0.012294517830014229\n",
            "Epoch 16411: train loss: 0.012293312698602676\n",
            "Epoch 16412: train loss: 0.012292105704545975\n",
            "Epoch 16413: train loss: 0.01229090616106987\n",
            "Epoch 16414: train loss: 0.01228970568627119\n",
            "Epoch 16415: train loss: 0.012288502417504787\n",
            "Epoch 16416: train loss: 0.012287299148738384\n",
            "Epoch 16417: train loss: 0.012286096811294556\n",
            "Epoch 16418: train loss: 0.012284886091947556\n",
            "Epoch 16419: train loss: 0.0122836884111166\n",
            "Epoch 16420: train loss: 0.012282487004995346\n",
            "Epoch 16421: train loss: 0.012281281873583794\n",
            "Epoch 16422: train loss: 0.01228008046746254\n",
            "Epoch 16423: train loss: 0.012278878130018711\n",
            "Epoch 16424: train loss: 0.012277673929929733\n",
            "Epoch 16425: train loss: 0.012276469729840755\n",
            "Epoch 16426: train loss: 0.012275275774300098\n",
            "Epoch 16427: train loss: 0.01227407343685627\n",
            "Epoch 16428: train loss: 0.012272864580154419\n",
            "Epoch 16429: train loss: 0.012271665036678314\n",
            "Epoch 16430: train loss: 0.012270462699234486\n",
            "Epoch 16431: train loss: 0.01226926688104868\n",
            "Epoch 16432: train loss: 0.01226806640625\n",
            "Epoch 16433: train loss: 0.012266864068806171\n",
            "Epoch 16434: train loss: 0.012265663594007492\n",
            "Epoch 16435: train loss: 0.012264464050531387\n",
            "Epoch 16436: train loss: 0.012263268232345581\n",
            "Epoch 16437: train loss: 0.012262067757546902\n",
            "Epoch 16438: train loss: 0.012260866351425648\n",
            "Epoch 16439: train loss: 0.012259670533239841\n",
            "Epoch 16440: train loss: 0.01225847378373146\n",
            "Epoch 16441: train loss: 0.012257274240255356\n",
            "Epoch 16442: train loss: 0.01225607842206955\n",
            "Epoch 16443: train loss: 0.012254882603883743\n",
            "Epoch 16444: train loss: 0.012253688648343086\n",
            "Epoch 16445: train loss: 0.012252495624125004\n",
            "Epoch 16446: train loss: 0.012251298874616623\n",
            "Epoch 16447: train loss: 0.012250108644366264\n",
            "Epoch 16448: train loss: 0.012248906306922436\n",
            "Epoch 16449: train loss: 0.012247717007994652\n",
            "Epoch 16450: train loss: 0.012246527709066868\n",
            "Epoch 16451: train loss: 0.012245330959558487\n",
            "Epoch 16452: train loss: 0.012244137935340405\n",
            "Epoch 16453: train loss: 0.012242943048477173\n",
            "Epoch 16454: train loss: 0.012241750955581665\n",
            "Epoch 16455: train loss: 0.01224056351929903\n",
            "Epoch 16456: train loss: 0.012239361181855202\n",
            "Epoch 16457: train loss: 0.012238174676895142\n",
            "Epoch 16458: train loss: 0.012236980721354485\n",
            "Epoch 16459: train loss: 0.012235789559781551\n",
            "Epoch 16460: train loss: 0.012234597466886044\n",
            "Epoch 16461: train loss: 0.012233405373990536\n",
            "Epoch 16462: train loss: 0.012232211418449879\n",
            "Epoch 16463: train loss: 0.012231022119522095\n",
            "Epoch 16464: train loss: 0.012229829095304012\n",
            "Epoch 16465: train loss: 0.012228637933731079\n",
            "Epoch 16466: train loss: 0.012227444909512997\n",
            "Epoch 16467: train loss: 0.012226257473230362\n",
            "Epoch 16468: train loss: 0.012225056998431683\n",
            "Epoch 16469: train loss: 0.012223868630826473\n",
            "Epoch 16470: train loss: 0.01222267746925354\n",
            "Epoch 16471: train loss: 0.012221486307680607\n",
            "Epoch 16472: train loss: 0.012220297940075397\n",
            "Epoch 16473: train loss: 0.012219108641147614\n",
            "Epoch 16474: train loss: 0.012217916548252106\n",
            "Epoch 16475: train loss: 0.012216730043292046\n",
            "Epoch 16476: train loss: 0.012215539813041687\n",
            "Epoch 16477: train loss: 0.012214346788823605\n",
            "Epoch 16478: train loss: 0.01221315748989582\n",
            "Epoch 16479: train loss: 0.012211968190968037\n",
            "Epoch 16480: train loss: 0.012210776098072529\n",
            "Epoch 16481: train loss: 0.012209591455757618\n",
            "Epoch 16482: train loss: 0.012208398431539536\n",
            "Epoch 16483: train loss: 0.0122072109952569\n",
            "Epoch 16484: train loss: 0.01220602635294199\n",
            "Epoch 16485: train loss: 0.012204834260046482\n",
            "Epoch 16486: train loss: 0.012203649617731571\n",
            "Epoch 16487: train loss: 0.012202460318803787\n",
            "Epoch 16488: train loss: 0.012201272882521152\n",
            "Epoch 16489: train loss: 0.012200080789625645\n",
            "Epoch 16490: train loss: 0.01219889149069786\n",
            "Epoch 16491: train loss: 0.012197711504995823\n",
            "Epoch 16492: train loss: 0.012196521274745464\n",
            "Epoch 16493: train loss: 0.012195334769785404\n",
            "Epoch 16494: train loss: 0.012194150127470493\n",
            "Epoch 16495: train loss: 0.012192962691187859\n",
            "Epoch 16496: train loss: 0.012191783636808395\n",
            "Epoch 16497: train loss: 0.01219060830771923\n",
            "Epoch 16498: train loss: 0.012189417146146297\n",
            "Epoch 16499: train loss: 0.012188236229121685\n",
            "Epoch 16500: train loss: 0.012187056243419647\n",
            "Epoch 16501: train loss: 0.012185872532427311\n",
            "Epoch 16502: train loss: 0.012184690684080124\n",
            "Epoch 16503: train loss: 0.012183509767055511\n",
            "Epoch 16504: train loss: 0.012182330712676048\n",
            "Epoch 16505: train loss: 0.012181145139038563\n",
            "Epoch 16506: train loss: 0.012179960496723652\n",
            "Epoch 16507: train loss: 0.012178782373666763\n",
            "Epoch 16508: train loss: 0.012177597731351852\n",
            "Epoch 16509: train loss: 0.012176424264907837\n",
            "Epoch 16510: train loss: 0.0121752405539155\n",
            "Epoch 16511: train loss: 0.012174061499536037\n",
            "Epoch 16512: train loss: 0.012172882445156574\n",
            "Epoch 16513: train loss: 0.012171700596809387\n",
            "Epoch 16514: train loss: 0.0121705187484622\n",
            "Epoch 16515: train loss: 0.012169339694082737\n",
            "Epoch 16516: train loss: 0.012168165296316147\n",
            "Epoch 16517: train loss: 0.012166975997388363\n",
            "Epoch 16518: train loss: 0.0121657969430089\n",
            "Epoch 16519: train loss: 0.01216462254524231\n",
            "Epoch 16520: train loss: 0.012163447216153145\n",
            "Epoch 16521: train loss: 0.012162269093096256\n",
            "Epoch 16522: train loss: 0.012161081656813622\n",
            "Epoch 16523: train loss: 0.012159906327724457\n",
            "Epoch 16524: train loss: 0.012158728204667568\n",
            "Epoch 16525: train loss: 0.012157549150288105\n",
            "Epoch 16526: train loss: 0.012156365439295769\n",
            "Epoch 16527: train loss: 0.012155190110206604\n",
            "Epoch 16528: train loss: 0.012154011987149715\n",
            "Epoch 16529: train loss: 0.01215283665806055\n",
            "Epoch 16530: train loss: 0.012151655741035938\n",
            "Epoch 16531: train loss: 0.012150479480624199\n",
            "Epoch 16532: train loss: 0.01214930135756731\n",
            "Epoch 16533: train loss: 0.012148123234510422\n",
            "Epoch 16534: train loss: 0.012146944180130959\n",
            "Epoch 16535: train loss: 0.01214576605707407\n",
            "Epoch 16536: train loss: 0.012144588865339756\n",
            "Epoch 16537: train loss: 0.012143408879637718\n",
            "Epoch 16538: train loss: 0.012142232619225979\n",
            "Epoch 16539: train loss: 0.012141059152781963\n",
            "Epoch 16540: train loss: 0.012139884755015373\n",
            "Epoch 16541: train loss: 0.012138702906668186\n",
            "Epoch 16542: train loss: 0.012137526646256447\n",
            "Epoch 16543: train loss: 0.012136355973780155\n",
            "Epoch 16544: train loss: 0.01213518250733614\n",
            "Epoch 16545: train loss: 0.012134009040892124\n",
            "Epoch 16546: train loss: 0.012132834643125534\n",
            "Epoch 16547: train loss: 0.012131662108004093\n",
            "Epoch 16548: train loss: 0.012130486778914928\n",
            "Epoch 16549: train loss: 0.012129317037761211\n",
            "Epoch 16550: train loss: 0.01212814636528492\n",
            "Epoch 16551: train loss: 0.012126974761486053\n",
            "Epoch 16552: train loss: 0.012125803157687187\n",
            "Epoch 16553: train loss: 0.012124629691243172\n",
            "Epoch 16554: train loss: 0.012123458087444305\n",
            "Epoch 16555: train loss: 0.012122291140258312\n",
            "Epoch 16556: train loss: 0.012121124193072319\n",
            "Epoch 16557: train loss: 0.012119955383241177\n",
            "Epoch 16558: train loss: 0.012118781916797161\n",
            "Epoch 16559: train loss: 0.012117612175643444\n",
            "Epoch 16560: train loss: 0.012116439640522003\n",
            "Epoch 16561: train loss: 0.012115279212594032\n",
            "Epoch 16562: train loss: 0.012114106677472591\n",
            "Epoch 16563: train loss: 0.012112937867641449\n",
            "Epoch 16564: train loss: 0.012111770920455456\n",
            "Epoch 16565: train loss: 0.012110604904592037\n",
            "Epoch 16566: train loss: 0.012109437957406044\n",
            "Epoch 16567: train loss: 0.01210827101022005\n",
            "Epoch 16568: train loss: 0.01210709661245346\n",
            "Epoch 16569: train loss: 0.012105927802622318\n",
            "Epoch 16570: train loss: 0.01210475992411375\n",
            "Epoch 16571: train loss: 0.012103589251637459\n",
            "Epoch 16572: train loss: 0.012102429755032063\n",
            "Epoch 16573: train loss: 0.012101261876523495\n",
            "Epoch 16574: train loss: 0.012100093066692352\n",
            "Epoch 16575: train loss: 0.01209892239421606\n",
            "Epoch 16576: train loss: 0.012097756378352642\n",
            "Epoch 16577: train loss: 0.012096586637198925\n",
            "Epoch 16578: train loss: 0.012095427140593529\n",
            "Epoch 16579: train loss: 0.012094255536794662\n",
            "Epoch 16580: train loss: 0.012093085795640945\n",
            "Epoch 16581: train loss: 0.012091919779777527\n",
            "Epoch 16582: train loss: 0.012090755626559258\n",
            "Epoch 16583: train loss: 0.012089590542018414\n",
            "Epoch 16584: train loss: 0.012088427320122719\n",
            "Epoch 16585: train loss: 0.012087268754839897\n",
            "Epoch 16586: train loss: 0.012086094357073307\n",
            "Epoch 16587: train loss: 0.012084925547242165\n",
            "Epoch 16588: train loss: 0.012083761394023895\n",
            "Epoch 16589: train loss: 0.01208260003477335\n",
            "Epoch 16590: train loss: 0.012081442400813103\n",
            "Epoch 16591: train loss: 0.012080268934369087\n",
            "Epoch 16592: train loss: 0.012079107575118542\n",
            "Epoch 16593: train loss: 0.012077942490577698\n",
            "Epoch 16594: train loss: 0.012076777406036854\n",
            "Epoch 16595: train loss: 0.012075618840754032\n",
            "Epoch 16596: train loss: 0.012074461206793785\n",
            "Epoch 16597: train loss: 0.012073301710188389\n",
            "Epoch 16598: train loss: 0.012072131037712097\n",
            "Epoch 16599: train loss: 0.012070976197719574\n",
            "Epoch 16600: train loss: 0.01206980925053358\n",
            "Epoch 16601: train loss: 0.012068649753928185\n",
            "Epoch 16602: train loss: 0.012067493982613087\n",
            "Epoch 16603: train loss: 0.012066333554685116\n",
            "Epoch 16604: train loss: 0.012065171264111996\n",
            "Epoch 16605: train loss: 0.01206400990486145\n",
            "Epoch 16606: train loss: 0.01206284575164318\n",
            "Epoch 16607: train loss: 0.012061688117682934\n",
            "Epoch 16608: train loss: 0.012060527689754963\n",
            "Epoch 16609: train loss: 0.01205937284976244\n",
            "Epoch 16610: train loss: 0.012058211490511894\n",
            "Epoch 16611: train loss: 0.01205704640597105\n",
            "Epoch 16612: train loss: 0.012055887840688229\n",
            "Epoch 16613: train loss: 0.012054728344082832\n",
            "Epoch 16614: train loss: 0.012053570710122585\n",
            "Epoch 16615: train loss: 0.012052418664097786\n",
            "Epoch 16616: train loss: 0.012051254510879517\n",
            "Epoch 16617: train loss: 0.012050102464854717\n",
            "Epoch 16618: train loss: 0.012048941105604172\n",
            "Epoch 16619: train loss: 0.012047783471643925\n",
            "Epoch 16620: train loss: 0.0120466323569417\n",
            "Epoch 16621: train loss: 0.012045472860336304\n",
            "Epoch 16622: train loss: 0.012044316157698631\n",
            "Epoch 16623: train loss: 0.012043161317706108\n",
            "Epoch 16624: train loss: 0.012042006477713585\n",
            "Epoch 16625: train loss: 0.012040849775075912\n",
            "Epoch 16626: train loss: 0.012039690278470516\n",
            "Epoch 16627: train loss: 0.012038540095090866\n",
            "Epoch 16628: train loss: 0.012037385255098343\n",
            "Epoch 16629: train loss: 0.01203623041510582\n",
            "Epoch 16630: train loss: 0.012035077437758446\n",
            "Epoch 16631: train loss: 0.012033917009830475\n",
            "Epoch 16632: train loss: 0.012032763101160526\n",
            "Epoch 16633: train loss: 0.012031607329845428\n",
            "Epoch 16634: train loss: 0.012030452489852905\n",
            "Epoch 16635: train loss: 0.012029297649860382\n",
            "Epoch 16636: train loss: 0.012028144672513008\n",
            "Epoch 16637: train loss: 0.01202699076384306\n",
            "Epoch 16638: train loss: 0.01202583871781826\n",
            "Epoch 16639: train loss: 0.012024683877825737\n",
            "Epoch 16640: train loss: 0.01202352624386549\n",
            "Epoch 16641: train loss: 0.012022373266518116\n",
            "Epoch 16642: train loss: 0.012021221220493317\n",
            "Epoch 16643: train loss: 0.012020068243145943\n",
            "Epoch 16644: train loss: 0.012018917128443718\n",
            "Epoch 16645: train loss: 0.012017762288451195\n",
            "Epoch 16646: train loss: 0.012016620486974716\n",
            "Epoch 16647: train loss: 0.012015468440949917\n",
            "Epoch 16648: train loss: 0.012014314532279968\n",
            "Epoch 16649: train loss: 0.012013166211545467\n",
            "Epoch 16650: train loss: 0.012012014165520668\n",
            "Epoch 16651: train loss: 0.012010862119495869\n",
            "Epoch 16652: train loss: 0.012009713798761368\n",
            "Epoch 16653: train loss: 0.012008564546704292\n",
            "Epoch 16654: train loss: 0.012007411569356918\n",
            "Epoch 16655: train loss: 0.012006259523332119\n",
            "Epoch 16656: train loss: 0.012005115859210491\n",
            "Epoch 16657: train loss: 0.012003965675830841\n",
            "Epoch 16658: train loss: 0.012002816423773766\n",
            "Epoch 16659: train loss: 0.012001668103039265\n",
            "Epoch 16660: train loss: 0.012000523507595062\n",
            "Epoch 16661: train loss: 0.011999370530247688\n",
            "Epoch 16662: train loss: 0.011998227797448635\n",
            "Epoch 16663: train loss: 0.011997077614068985\n",
            "Epoch 16664: train loss: 0.011995927430689335\n",
            "Epoch 16665: train loss: 0.011994781903922558\n",
            "Epoch 16666: train loss: 0.011993632651865482\n",
            "Epoch 16667: train loss: 0.011992488987743855\n",
            "Epoch 16668: train loss: 0.011991338804364204\n",
            "Epoch 16669: train loss: 0.01199018768966198\n",
            "Epoch 16670: train loss: 0.011989043094217777\n",
            "Epoch 16671: train loss: 0.011987894773483276\n",
            "Epoch 16672: train loss: 0.011986753903329372\n",
            "Epoch 16673: train loss: 0.011985603719949722\n",
            "Epoch 16674: train loss: 0.011984460987150669\n",
            "Epoch 16675: train loss: 0.01198331918567419\n",
            "Epoch 16676: train loss: 0.011982168070971966\n",
            "Epoch 16677: train loss: 0.011981027200818062\n",
            "Epoch 16678: train loss: 0.01197988074272871\n",
            "Epoch 16679: train loss: 0.011978741735219955\n",
            "Epoch 16680: train loss: 0.01197759434580803\n",
            "Epoch 16681: train loss: 0.011976450681686401\n",
            "Epoch 16682: train loss: 0.011975310742855072\n",
            "Epoch 16683: train loss: 0.011974168941378593\n",
            "Epoch 16684: train loss: 0.01197302620857954\n",
            "Epoch 16685: train loss: 0.011971880681812763\n",
            "Epoch 16686: train loss: 0.011970735155045986\n",
            "Epoch 16687: train loss: 0.01196958962827921\n",
            "Epoch 16688: train loss: 0.011968446895480156\n",
            "Epoch 16689: train loss: 0.01196730975061655\n",
            "Epoch 16690: train loss: 0.011966165155172348\n",
            "Epoch 16691: train loss: 0.011965026147663593\n",
            "Epoch 16692: train loss: 0.011963884346187115\n",
            "Epoch 16693: train loss: 0.011962741613388062\n",
            "Epoch 16694: train loss: 0.011961599811911583\n",
            "Epoch 16695: train loss: 0.011960458941757679\n",
            "Epoch 16696: train loss: 0.011959316208958626\n",
            "Epoch 16697: train loss: 0.011958175338804722\n",
            "Epoch 16698: train loss: 0.011957031674683094\n",
            "Epoch 16699: train loss: 0.011955893598496914\n",
            "Epoch 16700: train loss: 0.01195475459098816\n",
            "Epoch 16701: train loss: 0.011953622102737427\n",
            "Epoch 16702: train loss: 0.011952484957873821\n",
            "Epoch 16703: train loss: 0.011951345950365067\n",
            "Epoch 16704: train loss: 0.011950206011533737\n",
            "Epoch 16705: train loss: 0.011949071660637856\n",
            "Epoch 16706: train loss: 0.011947931721806526\n",
            "Epoch 16707: train loss: 0.011946793645620346\n",
            "Epoch 16708: train loss: 0.011945657432079315\n",
            "Epoch 16709: train loss: 0.01194451842457056\n",
            "Epoch 16710: train loss: 0.011943386867642403\n",
            "Epoch 16711: train loss: 0.011942238546907902\n",
            "Epoch 16712: train loss: 0.011941105127334595\n",
            "Epoch 16713: train loss: 0.011939969845116138\n",
            "Epoch 16714: train loss: 0.011938832700252533\n",
            "Epoch 16715: train loss: 0.011937699280679226\n",
            "Epoch 16716: train loss: 0.011936554685235023\n",
            "Epoch 16717: train loss: 0.011935421265661716\n",
            "Epoch 16718: train loss: 0.01193428784608841\n",
            "Epoch 16719: train loss: 0.011933146975934505\n",
            "Epoch 16720: train loss: 0.011932019144296646\n",
            "Epoch 16721: train loss: 0.011930880136787891\n",
            "Epoch 16722: train loss: 0.011929744854569435\n",
            "Epoch 16723: train loss: 0.01192860584706068\n",
            "Epoch 16724: train loss: 0.011927471496164799\n",
            "Epoch 16725: train loss: 0.011926337145268917\n",
            "Epoch 16726: train loss: 0.011925202794373035\n",
            "Epoch 16727: train loss: 0.01192406751215458\n",
            "Epoch 16728: train loss: 0.011922926642000675\n",
            "Epoch 16729: train loss: 0.011921797879040241\n",
            "Epoch 16730: train loss: 0.011920660734176636\n",
            "Epoch 16731: train loss: 0.011919531039893627\n",
            "Epoch 16732: train loss: 0.011918395757675171\n",
            "Epoch 16733: train loss: 0.011917256750166416\n",
            "Epoch 16734: train loss: 0.011916128918528557\n",
            "Epoch 16735: train loss: 0.01191499549895525\n",
            "Epoch 16736: train loss: 0.011913852766156197\n",
            "Epoch 16737: train loss: 0.011912722140550613\n",
            "Epoch 16738: train loss: 0.011911595240235329\n",
            "Epoch 16739: train loss: 0.011910463683307171\n",
            "Epoch 16740: train loss: 0.011909333989024162\n",
            "Epoch 16741: train loss: 0.011908198706805706\n",
            "Epoch 16742: train loss: 0.011907068081200123\n",
            "Epoch 16743: train loss: 0.011905940249562263\n",
            "Epoch 16744: train loss: 0.011904808692634106\n",
            "Epoch 16745: train loss: 0.011903680860996246\n",
            "Epoch 16746: train loss: 0.011902549304068089\n",
            "Epoch 16747: train loss: 0.01190141774713993\n",
            "Epoch 16748: train loss: 0.011900285258889198\n",
            "Epoch 16749: train loss: 0.01189915556460619\n",
            "Epoch 16750: train loss: 0.011898030526936054\n",
            "Epoch 16751: train loss: 0.01189690362662077\n",
            "Epoch 16752: train loss: 0.011895773932337761\n",
            "Epoch 16753: train loss: 0.011894644238054752\n",
            "Epoch 16754: train loss: 0.011893521063029766\n",
            "Epoch 16755: train loss: 0.011892387643456459\n",
            "Epoch 16756: train loss: 0.011891261674463749\n",
            "Epoch 16757: train loss: 0.011890137568116188\n",
            "Epoch 16758: train loss: 0.011889013461768627\n",
            "Epoch 16759: train loss: 0.011887888424098492\n",
            "Epoch 16760: train loss: 0.011886757798492908\n",
            "Epoch 16761: train loss: 0.011885632760822773\n",
            "Epoch 16762: train loss: 0.01188450213521719\n",
            "Epoch 16763: train loss: 0.011883369646966457\n",
            "Epoch 16764: train loss: 0.011882257647812366\n",
            "Epoch 16765: train loss: 0.011881130747497082\n",
            "Epoch 16766: train loss: 0.011880002915859222\n",
            "Epoch 16767: train loss: 0.011878876946866512\n",
            "Epoch 16768: train loss: 0.011877748183906078\n",
            "Epoch 16769: train loss: 0.011876629665493965\n",
            "Epoch 16770: train loss: 0.011875500902533531\n",
            "Epoch 16771: train loss: 0.011874369345605373\n",
            "Epoch 16772: train loss: 0.01187325082719326\n",
            "Epoch 16773: train loss: 0.011872123926877975\n",
            "Epoch 16774: train loss: 0.011870999820530415\n",
            "Epoch 16775: train loss: 0.01186987292021513\n",
            "Epoch 16776: train loss: 0.011868753470480442\n",
            "Epoch 16777: train loss: 0.011867634020745754\n",
            "Epoch 16778: train loss: 0.01186650525778532\n",
            "Epoch 16779: train loss: 0.011865383945405483\n",
            "Epoch 16780: train loss: 0.011864260770380497\n",
            "Epoch 16781: train loss: 0.011863134801387787\n",
            "Epoch 16782: train loss: 0.0118620116263628\n",
            "Epoch 16783: train loss: 0.011860894039273262\n",
            "Epoch 16784: train loss: 0.011859766207635403\n",
            "Epoch 16785: train loss: 0.011858646757900715\n",
            "Epoch 16786: train loss: 0.011857523582875729\n",
            "Epoch 16787: train loss: 0.011856402270495892\n",
            "Epoch 16788: train loss: 0.01185528188943863\n",
            "Epoch 16789: train loss: 0.01185416430234909\n",
            "Epoch 16790: train loss: 0.011853036470711231\n",
            "Epoch 16791: train loss: 0.01185191422700882\n",
            "Epoch 16792: train loss: 0.011850796639919281\n",
            "Epoch 16793: train loss: 0.01184967253357172\n",
            "Epoch 16794: train loss: 0.011848552152514458\n",
            "Epoch 16795: train loss: 0.011847428046166897\n",
            "Epoch 16796: train loss: 0.011846309527754784\n",
            "Epoch 16797: train loss: 0.011845187284052372\n",
            "Epoch 16798: train loss: 0.011844061315059662\n",
            "Epoch 16799: train loss: 0.011842944659292698\n",
            "Epoch 16800: train loss: 0.01184182520955801\n",
            "Epoch 16801: train loss: 0.011840712279081345\n",
            "Epoch 16802: train loss: 0.01183958537876606\n",
            "Epoch 16803: train loss: 0.011838467791676521\n",
            "Epoch 16804: train loss: 0.011837358586490154\n",
            "Epoch 16805: train loss: 0.011836240999400616\n",
            "Epoch 16806: train loss: 0.011835123412311077\n",
            "Epoch 16807: train loss: 0.011834008619189262\n",
            "Epoch 16808: train loss: 0.011832891963422298\n",
            "Epoch 16809: train loss: 0.01183177251368761\n",
            "Epoch 16810: train loss: 0.011830661445856094\n",
            "Epoch 16811: train loss: 0.011829541996121407\n",
            "Epoch 16812: train loss: 0.011828426271677017\n",
            "Epoch 16813: train loss: 0.011827303096652031\n",
            "Epoch 16814: train loss: 0.01182619296014309\n",
            "Epoch 16815: train loss: 0.011825084686279297\n",
            "Epoch 16816: train loss: 0.011823968030512333\n",
            "Epoch 16817: train loss: 0.011822855100035667\n",
            "Epoch 16818: train loss: 0.011821742169559002\n",
            "Epoch 16819: train loss: 0.011820631101727486\n",
            "Epoch 16820: train loss: 0.011819515377283096\n",
            "Epoch 16821: train loss: 0.011818400584161282\n",
            "Epoch 16822: train loss: 0.011817284859716892\n",
            "Epoch 16823: train loss: 0.011816167272627354\n",
            "Epoch 16824: train loss: 0.011815061792731285\n",
            "Epoch 16825: train loss: 0.011813949793577194\n",
            "Epoch 16826: train loss: 0.011812837794423103\n",
            "Epoch 16827: train loss: 0.01181172113865614\n",
            "Epoch 16828: train loss: 0.011810606345534325\n",
            "Epoch 16829: train loss: 0.01180949341505766\n",
            "Epoch 16830: train loss: 0.011808381415903568\n",
            "Epoch 16831: train loss: 0.011807266622781754\n",
            "Epoch 16832: train loss: 0.011806159280240536\n",
            "Epoch 16833: train loss: 0.011805045418441296\n",
            "Epoch 16834: train loss: 0.01180393435060978\n",
            "Epoch 16835: train loss: 0.011802827939391136\n",
            "Epoch 16836: train loss: 0.011801712214946747\n",
            "Epoch 16837: train loss: 0.011800599284470081\n",
            "Epoch 16838: train loss: 0.011799486353993416\n",
            "Epoch 16839: train loss: 0.011798379011452198\n",
            "Epoch 16840: train loss: 0.011797262355685234\n",
            "Epoch 16841: train loss: 0.011796153150498867\n",
            "Epoch 16842: train loss: 0.0117950439453125\n",
            "Epoch 16843: train loss: 0.011793931946158409\n",
            "Epoch 16844: train loss: 0.011792820878326893\n",
            "Epoch 16845: train loss: 0.011791713535785675\n",
            "Epoch 16846: train loss: 0.011790601536631584\n",
            "Epoch 16847: train loss: 0.011789493262767792\n",
            "Epoch 16848: train loss: 0.011788380332291126\n",
            "Epoch 16849: train loss: 0.011787272058427334\n",
            "Epoch 16850: train loss: 0.011786160990595818\n",
            "Epoch 16851: train loss: 0.011785049922764301\n",
            "Epoch 16852: train loss: 0.011783941648900509\n",
            "Epoch 16853: train loss: 0.011782830581068993\n",
            "Epoch 16854: train loss: 0.011781723238527775\n",
            "Epoch 16855: train loss: 0.011780604720115662\n",
            "Epoch 16856: train loss: 0.01177950855344534\n",
            "Epoch 16857: train loss: 0.011778395622968674\n",
            "Epoch 16858: train loss: 0.011777285486459732\n",
            "Epoch 16859: train loss: 0.011776184663176537\n",
            "Epoch 16860: train loss: 0.01177507545799017\n",
            "Epoch 16861: train loss: 0.011773969978094101\n",
            "Epoch 16862: train loss: 0.011772860772907734\n",
            "Epoch 16863: train loss: 0.011771757155656815\n",
            "Epoch 16864: train loss: 0.011770651675760746\n",
            "Epoch 16865: train loss: 0.011769548989832401\n",
            "Epoch 16866: train loss: 0.011768448166549206\n",
            "Epoch 16867: train loss: 0.011767340824007988\n",
            "Epoch 16868: train loss: 0.011766240932047367\n",
            "Epoch 16869: train loss: 0.01176513358950615\n",
            "Epoch 16870: train loss: 0.011764033697545528\n",
            "Epoch 16871: train loss: 0.011762927286326885\n",
            "Epoch 16872: train loss: 0.011761821806430817\n",
            "Epoch 16873: train loss: 0.011760720983147621\n",
            "Epoch 16874: train loss: 0.011759617365896702\n",
            "Epoch 16875: train loss: 0.011758517473936081\n",
            "Epoch 16876: train loss: 0.011757411994040012\n",
            "Epoch 16877: train loss: 0.011756312102079391\n",
            "Epoch 16878: train loss: 0.011755210347473621\n",
            "Epoch 16879: train loss: 0.011754105798900127\n",
            "Epoch 16880: train loss: 0.01175301056355238\n",
            "Epoch 16881: train loss: 0.011751901358366013\n",
            "Epoch 16882: train loss: 0.011750795878469944\n",
            "Epoch 16883: train loss: 0.01174970157444477\n",
            "Epoch 16884: train loss: 0.011748598888516426\n",
            "Epoch 16885: train loss: 0.011747503653168678\n",
            "Epoch 16886: train loss: 0.011746400035917759\n",
            "Epoch 16887: train loss: 0.011745298281311989\n",
            "Epoch 16888: train loss: 0.011744197458028793\n",
            "Epoch 16889: train loss: 0.011743100360035896\n",
            "Epoch 16890: train loss: 0.011741996742784977\n",
            "Epoch 16891: train loss: 0.011740896850824356\n",
            "Epoch 16892: train loss: 0.011739792302250862\n",
            "Epoch 16893: train loss: 0.01173869427293539\n",
            "Epoch 16894: train loss: 0.011737598106265068\n",
            "Epoch 16895: train loss: 0.011736499145627022\n",
            "Epoch 16896: train loss: 0.011735402047634125\n",
            "Epoch 16897: train loss: 0.011734291911125183\n",
            "Epoch 16898: train loss: 0.011733193881809711\n",
            "Epoch 16899: train loss: 0.011732098646461964\n",
            "Epoch 16900: train loss: 0.011730998754501343\n",
            "Epoch 16901: train loss: 0.011729905381798744\n",
            "Epoch 16902: train loss: 0.011728808283805847\n",
            "Epoch 16903: train loss: 0.011727703735232353\n",
            "Epoch 16904: train loss: 0.01172661129385233\n",
            "Epoch 16905: train loss: 0.01172550767660141\n",
            "Epoch 16906: train loss: 0.011724414303898811\n",
            "Epoch 16907: train loss: 0.011723309755325317\n",
            "Epoch 16908: train loss: 0.011722215451300144\n",
            "Epoch 16909: train loss: 0.011721121147274971\n",
            "Epoch 16910: train loss: 0.01172002125531435\n",
            "Epoch 16911: train loss: 0.0117189297452569\n",
            "Epoch 16912: train loss: 0.011717830784618855\n",
            "Epoch 16913: train loss: 0.011716733686625957\n",
            "Epoch 16914: train loss: 0.011715641245245934\n",
            "Epoch 16915: train loss: 0.011714543215930462\n",
            "Epoch 16916: train loss: 0.011713450774550438\n",
            "Epoch 16917: train loss: 0.011712353676557541\n",
            "Epoch 16918: train loss: 0.011711257509887218\n",
            "Epoch 16919: train loss: 0.011710160411894321\n",
            "Epoch 16920: train loss: 0.011709064245223999\n",
            "Epoch 16921: train loss: 0.011707974597811699\n",
            "Epoch 16922: train loss: 0.011706878431141376\n",
            "Epoch 16923: train loss: 0.011705788783729076\n",
            "Epoch 16924: train loss: 0.01170469168573618\n",
            "Epoch 16925: train loss: 0.011703599244356155\n",
            "Epoch 16926: train loss: 0.011702504009008408\n",
            "Epoch 16927: train loss: 0.011701416224241257\n",
            "Epoch 16928: train loss: 0.011700320057570934\n",
            "Epoch 16929: train loss: 0.011699235066771507\n",
            "Epoch 16930: train loss: 0.011698141694068909\n",
            "Epoch 16931: train loss: 0.011697045527398586\n",
            "Epoch 16932: train loss: 0.01169596053659916\n",
            "Epoch 16933: train loss: 0.011694861575961113\n",
            "Epoch 16934: train loss: 0.011693774722516537\n",
            "Epoch 16935: train loss: 0.01169268973171711\n",
            "Epoch 16936: train loss: 0.011691591702401638\n",
            "Epoch 16937: train loss: 0.011690507642924786\n",
            "Epoch 16938: train loss: 0.011689414270222187\n",
            "Epoch 16939: train loss: 0.011688324622809887\n",
            "Epoch 16940: train loss: 0.011687240563333035\n",
            "Epoch 16941: train loss: 0.011686146259307861\n",
            "Epoch 16942: train loss: 0.011685053817927837\n",
            "Epoch 16943: train loss: 0.01168396882712841\n",
            "Epoch 16944: train loss: 0.011682873591780663\n",
            "Epoch 16945: train loss: 0.011681790463626385\n",
            "Epoch 16946: train loss: 0.01168070174753666\n",
            "Epoch 16947: train loss: 0.01167960837483406\n",
            "Epoch 16948: train loss: 0.011678528040647507\n",
            "Epoch 16949: train loss: 0.011677436530590057\n",
            "Epoch 16950: train loss: 0.011676348745822906\n",
            "Epoch 16951: train loss: 0.011675260961055756\n",
            "Epoch 16952: train loss: 0.011674170382320881\n",
            "Epoch 16953: train loss: 0.011673081666231155\n",
            "Epoch 16954: train loss: 0.011671993881464005\n",
            "Epoch 16955: train loss: 0.011670908890664577\n",
            "Epoch 16956: train loss: 0.011669817380607128\n",
            "Epoch 16957: train loss: 0.011668731458485126\n",
            "Epoch 16958: train loss: 0.0116676464676857\n",
            "Epoch 16959: train loss: 0.011666557751595974\n",
            "Epoch 16960: train loss: 0.0116654671728611\n",
            "Epoch 16961: train loss: 0.011664382182061672\n",
            "Epoch 16962: train loss: 0.011663295328617096\n",
            "Epoch 16963: train loss: 0.011662202887237072\n",
            "Epoch 16964: train loss: 0.011661117896437645\n",
            "Epoch 16965: train loss: 0.01166002918034792\n",
            "Epoch 16966: train loss: 0.011658952571451664\n",
            "Epoch 16967: train loss: 0.011657866649329662\n",
            "Epoch 16968: train loss: 0.01165678258985281\n",
            "Epoch 16969: train loss: 0.011655702255666256\n",
            "Epoch 16970: train loss: 0.011654611676931381\n",
            "Epoch 16971: train loss: 0.011653527617454529\n",
            "Epoch 16972: train loss: 0.011652443557977676\n",
            "Epoch 16973: train loss: 0.01165135856717825\n",
            "Epoch 16974: train loss: 0.011650271713733673\n",
            "Epoch 16975: train loss: 0.011649190448224545\n",
            "Epoch 16976: train loss: 0.011648111045360565\n",
            "Epoch 16977: train loss: 0.011647026985883713\n",
            "Epoch 16978: train loss: 0.011645941063761711\n",
            "Epoch 16979: train loss: 0.011644859798252583\n",
            "Epoch 16980: train loss: 0.011643783189356327\n",
            "Epoch 16981: train loss: 0.011642691679298878\n",
            "Epoch 16982: train loss: 0.011641614139080048\n",
            "Epoch 16983: train loss: 0.011640531942248344\n",
            "Epoch 16984: train loss: 0.011639456264674664\n",
            "Epoch 16985: train loss: 0.011638370342552662\n",
            "Epoch 16986: train loss: 0.011637283489108086\n",
            "Epoch 16987: train loss: 0.011636204086244106\n",
            "Epoch 16988: train loss: 0.011635124683380127\n",
            "Epoch 16989: train loss: 0.01163404993712902\n",
            "Epoch 16990: train loss: 0.011632963083684444\n",
            "Epoch 16991: train loss: 0.01163188461214304\n",
            "Epoch 16992: train loss: 0.011630799621343613\n",
            "Epoch 16993: train loss: 0.011629734188318253\n",
            "Epoch 16994: train loss: 0.011628651060163975\n",
            "Epoch 16995: train loss: 0.011627573519945145\n",
            "Epoch 16996: train loss: 0.011626492254436016\n",
            "Epoch 16997: train loss: 0.011625410057604313\n",
            "Epoch 16998: train loss: 0.01162433996796608\n",
            "Epoch 16999: train loss: 0.011623257771134377\n",
            "Epoch 17000: train loss: 0.011622180230915546\n",
            "Epoch 17001: train loss: 0.011621096171438694\n",
            "Epoch 17002: train loss: 0.011620027013123035\n",
            "Epoch 17003: train loss: 0.011618942022323608\n",
            "Epoch 17004: train loss: 0.011617869138717651\n",
            "Epoch 17005: train loss: 0.011616790667176247\n",
            "Epoch 17006: train loss: 0.011615713126957417\n",
            "Epoch 17007: train loss: 0.011614636518061161\n",
            "Epoch 17008: train loss: 0.011613558977842331\n",
            "Epoch 17009: train loss: 0.011612484231591225\n",
            "Epoch 17010: train loss: 0.011611394584178925\n",
            "Epoch 17011: train loss: 0.011610321700572968\n",
            "Epoch 17012: train loss: 0.011609247885644436\n",
            "Epoch 17013: train loss: 0.011608175002038479\n",
            "Epoch 17014: train loss: 0.011607090011239052\n",
            "Epoch 17015: train loss: 0.011606020852923393\n",
            "Epoch 17016: train loss: 0.011604943312704563\n",
            "Epoch 17017: train loss: 0.011603862047195435\n",
            "Epoch 17018: train loss: 0.011602792888879776\n",
            "Epoch 17019: train loss: 0.01160171627998352\n",
            "Epoch 17020: train loss: 0.011600646190345287\n",
            "Epoch 17021: train loss: 0.011599570512771606\n",
            "Epoch 17022: train loss: 0.011598492972552776\n",
            "Epoch 17023: train loss: 0.011597424745559692\n",
            "Epoch 17024: train loss: 0.011596351861953735\n",
            "Epoch 17025: train loss: 0.01159527525305748\n",
            "Epoch 17026: train loss: 0.01159420795738697\n",
            "Epoch 17027: train loss: 0.011593133211135864\n",
            "Epoch 17028: train loss: 0.011592056602239609\n",
            "Epoch 17029: train loss: 0.01159098744392395\n",
            "Epoch 17030: train loss: 0.011589917354285717\n",
            "Epoch 17031: train loss: 0.01158884260803461\n",
            "Epoch 17032: train loss: 0.011587770655751228\n",
            "Epoch 17033: train loss: 0.01158670149743557\n",
            "Epoch 17034: train loss: 0.011585630476474762\n",
            "Epoch 17035: train loss: 0.011584559455513954\n",
            "Epoch 17036: train loss: 0.011583487503230572\n",
            "Epoch 17037: train loss: 0.011582416482269764\n",
            "Epoch 17038: train loss: 0.011581344529986382\n",
            "Epoch 17039: train loss: 0.011580277234315872\n",
            "Epoch 17040: train loss: 0.01157920528203249\n",
            "Epoch 17041: train loss: 0.011578133329749107\n",
            "Epoch 17042: train loss: 0.011577069759368896\n",
            "Epoch 17043: train loss: 0.011575999669730663\n",
            "Epoch 17044: train loss: 0.011574926786124706\n",
            "Epoch 17045: train loss: 0.01157385390251875\n",
            "Epoch 17046: train loss: 0.011572781018912792\n",
            "Epoch 17047: train loss: 0.011571718379855156\n",
            "Epoch 17048: train loss: 0.011570649221539497\n",
            "Epoch 17049: train loss: 0.011569579131901264\n",
            "Epoch 17050: train loss: 0.011568507179617882\n",
            "Epoch 17051: train loss: 0.0115674352273345\n",
            "Epoch 17052: train loss: 0.011566372588276863\n",
            "Epoch 17053: train loss: 0.011565297842025757\n",
            "Epoch 17054: train loss: 0.011564228683710098\n",
            "Epoch 17055: train loss: 0.011563164182007313\n",
            "Epoch 17056: train loss: 0.011562093161046505\n",
            "Epoch 17057: train loss: 0.01156102865934372\n",
            "Epoch 17058: train loss: 0.011559960432350636\n",
            "Epoch 17059: train loss: 0.011558898724615574\n",
            "Epoch 17060: train loss: 0.011557824909687042\n",
            "Epoch 17061: train loss: 0.011556761339306831\n",
            "Epoch 17062: train loss: 0.011555698700249195\n",
            "Epoch 17063: train loss: 0.011554628610610962\n",
            "Epoch 17064: train loss: 0.011553570628166199\n",
            "Epoch 17065: train loss: 0.011552495881915092\n",
            "Epoch 17066: train loss: 0.011551428586244583\n",
            "Epoch 17067: train loss: 0.011550369672477245\n",
            "Epoch 17068: train loss: 0.011549300514161587\n",
            "Epoch 17069: train loss: 0.011548236012458801\n",
            "Epoch 17070: train loss: 0.011547170579433441\n",
            "Epoch 17071: train loss: 0.011546102352440357\n",
            "Epoch 17072: train loss: 0.011545038782060146\n",
            "Epoch 17073: train loss: 0.01154397614300251\n",
            "Epoch 17074: train loss: 0.011542913503944874\n",
            "Epoch 17075: train loss: 0.011541850864887238\n",
            "Epoch 17076: train loss: 0.011540782637894154\n",
            "Epoch 17077: train loss: 0.011539722792804241\n",
            "Epoch 17078: train loss: 0.011538662016391754\n",
            "Epoch 17079: train loss: 0.011537597514688969\n",
            "Epoch 17080: train loss: 0.011536535806953907\n",
            "Epoch 17081: train loss: 0.01153547689318657\n",
            "Epoch 17082: train loss: 0.011534415185451508\n",
            "Epoch 17083: train loss: 0.01153335440903902\n",
            "Epoch 17084: train loss: 0.011532286182045937\n",
            "Epoch 17085: train loss: 0.011531226336956024\n",
            "Epoch 17086: train loss: 0.011530166491866112\n",
            "Epoch 17087: train loss: 0.011529101058840752\n",
            "Epoch 17088: train loss: 0.011528044939041138\n",
            "Epoch 17089: train loss: 0.011526986956596375\n",
            "Epoch 17090: train loss: 0.01152592059224844\n",
            "Epoch 17091: train loss: 0.011524863541126251\n",
            "Epoch 17092: train loss: 0.011523805558681488\n",
            "Epoch 17093: train loss: 0.011522740125656128\n",
            "Epoch 17094: train loss: 0.011521687731146812\n",
            "Epoch 17095: train loss: 0.011520622298121452\n",
            "Epoch 17096: train loss: 0.011519561521708965\n",
            "Epoch 17097: train loss: 0.011518503539264202\n",
            "Epoch 17098: train loss: 0.011517440900206566\n",
            "Epoch 17099: train loss: 0.011516382917761803\n",
            "Epoch 17100: train loss: 0.011515325866639614\n",
            "Epoch 17101: train loss: 0.01151426974684\n",
            "Epoch 17102: train loss: 0.011513211764395237\n",
            "Epoch 17103: train loss: 0.011512150056660175\n",
            "Epoch 17104: train loss: 0.011511090211570263\n",
            "Epoch 17105: train loss: 0.011510031297802925\n",
            "Epoch 17106: train loss: 0.01150897890329361\n",
            "Epoch 17107: train loss: 0.011507919989526272\n",
            "Epoch 17108: train loss: 0.011506859213113785\n",
            "Epoch 17109: train loss: 0.011505804024636745\n",
            "Epoch 17110: train loss: 0.011504745110869408\n",
            "Epoch 17111: train loss: 0.01150368619710207\n",
            "Epoch 17112: train loss: 0.01150263100862503\n",
            "Epoch 17113: train loss: 0.011501573957502842\n",
            "Epoch 17114: train loss: 0.011500517837703228\n",
            "Epoch 17115: train loss: 0.011499455198645592\n",
            "Epoch 17116: train loss: 0.011498400941491127\n",
            "Epoch 17117: train loss: 0.011497339233756065\n",
            "Epoch 17118: train loss: 0.011496282182633877\n",
            "Epoch 17119: train loss: 0.011495229788124561\n",
            "Epoch 17120: train loss: 0.011494170874357224\n",
            "Epoch 17121: train loss: 0.011493117548525333\n",
            "Epoch 17122: train loss: 0.011492063291370869\n",
            "Epoch 17123: train loss: 0.011491009034216404\n",
            "Epoch 17124: train loss: 0.011489957571029663\n",
            "Epoch 17125: train loss: 0.011488895863294601\n",
            "Epoch 17126: train loss: 0.011487843468785286\n",
            "Epoch 17127: train loss: 0.011486788280308247\n",
            "Epoch 17128: train loss: 0.011485734023153782\n",
            "Epoch 17129: train loss: 0.011484683491289616\n",
            "Epoch 17130: train loss: 0.011483628302812576\n",
            "Epoch 17131: train loss: 0.011482572183012962\n",
            "Epoch 17132: train loss: 0.01148152444511652\n",
            "Epoch 17133: train loss: 0.011480468325316906\n",
            "Epoch 17134: train loss: 0.011479422450065613\n",
            "Epoch 17135: train loss: 0.011478368192911148\n",
            "Epoch 17136: train loss: 0.011477317661046982\n",
            "Epoch 17137: train loss: 0.011476271785795689\n",
            "Epoch 17138: train loss: 0.0114752147346735\n",
            "Epoch 17139: train loss: 0.011474166996777058\n",
            "Epoch 17140: train loss: 0.011473113670945168\n",
            "Epoch 17141: train loss: 0.011472064070403576\n",
            "Epoch 17142: train loss: 0.011471018195152283\n",
            "Epoch 17143: train loss: 0.011469961144030094\n",
            "Epoch 17144: train loss: 0.011468914337456226\n",
            "Epoch 17145: train loss: 0.01146786380559206\n",
            "Epoch 17146: train loss: 0.011466815136373043\n",
            "Epoch 17147: train loss: 0.011465771123766899\n",
            "Epoch 17148: train loss: 0.01146471593528986\n",
            "Epoch 17149: train loss: 0.011463666334748268\n",
            "Epoch 17150: train loss: 0.011462613008916378\n",
            "Epoch 17151: train loss: 0.011461570858955383\n",
            "Epoch 17152: train loss: 0.011460523121058941\n",
            "Epoch 17153: train loss: 0.011459470726549625\n",
            "Epoch 17154: train loss: 0.011458422057330608\n",
            "Epoch 17155: train loss: 0.01145737711340189\n",
            "Epoch 17156: train loss: 0.011456327512860298\n",
            "Epoch 17157: train loss: 0.01145528070628643\n",
            "Epoch 17158: train loss: 0.011454228311777115\n",
            "Epoch 17159: train loss: 0.011453189887106419\n",
            "Epoch 17160: train loss: 0.011452140286564827\n",
            "Epoch 17161: train loss: 0.01145108975470066\n",
            "Epoch 17162: train loss: 0.011450041085481644\n",
            "Epoch 17163: train loss: 0.011448993347585201\n",
            "Epoch 17164: train loss: 0.011447947472333908\n",
            "Epoch 17165: train loss: 0.011446903459727764\n",
            "Epoch 17166: train loss: 0.011445856653153896\n",
            "Epoch 17167: train loss: 0.011444808915257454\n",
            "Epoch 17168: train loss: 0.01144376304000616\n",
            "Epoch 17169: train loss: 0.01144271157681942\n",
            "Epoch 17170: train loss: 0.011441665701568127\n",
            "Epoch 17171: train loss: 0.011440622620284557\n",
            "Epoch 17172: train loss: 0.011439577676355839\n",
            "Epoch 17173: train loss: 0.011438528075814247\n",
            "Epoch 17174: train loss: 0.01143747940659523\n",
            "Epoch 17175: train loss: 0.011436435393989086\n",
            "Epoch 17176: train loss: 0.011435394175350666\n",
            "Epoch 17177: train loss: 0.0114343436434865\n",
            "Epoch 17178: train loss: 0.011433294974267483\n",
            "Epoch 17179: train loss: 0.011432251892983913\n",
            "Epoch 17180: train loss: 0.011431210674345493\n",
            "Epoch 17181: train loss: 0.01143016666173935\n",
            "Epoch 17182: train loss: 0.011429117061197758\n",
            "Epoch 17183: train loss: 0.011428071185946465\n",
            "Epoch 17184: train loss: 0.011427028104662895\n",
            "Epoch 17185: train loss: 0.011425981298089027\n",
            "Epoch 17186: train loss: 0.011424937285482883\n",
            "Epoch 17187: train loss: 0.011423897929489613\n",
            "Epoch 17188: train loss: 0.011422860436141491\n",
            "Epoch 17189: train loss: 0.011421822011470795\n",
            "Epoch 17190: train loss: 0.011420777067542076\n",
            "Epoch 17191: train loss: 0.01141973864287138\n",
            "Epoch 17192: train loss: 0.01141869556158781\n",
            "Epoch 17193: train loss: 0.011417662724852562\n",
            "Epoch 17194: train loss: 0.01141661498695612\n",
            "Epoch 17195: train loss: 0.0114155737683177\n",
            "Epoch 17196: train loss: 0.011414538137614727\n",
            "Epoch 17197: train loss: 0.011413504369556904\n",
            "Epoch 17198: train loss: 0.011412457562983036\n",
            "Epoch 17199: train loss: 0.011411421932280064\n",
            "Epoch 17200: train loss: 0.011410380713641644\n",
            "Epoch 17201: train loss: 0.011409345082938671\n",
            "Epoch 17202: train loss: 0.011408302932977676\n",
            "Epoch 17203: train loss: 0.011407261714339256\n",
            "Epoch 17204: train loss: 0.011406226083636284\n",
            "Epoch 17205: train loss: 0.01140518393367529\n",
            "Epoch 17206: train loss: 0.011404147371649742\n",
            "Epoch 17207: train loss: 0.011403107084333897\n",
            "Epoch 17208: train loss: 0.01140207052230835\n",
            "Epoch 17209: train loss: 0.011401033960282803\n",
            "Epoch 17210: train loss: 0.011399989947676659\n",
            "Epoch 17211: train loss: 0.011398952454328537\n",
            "Epoch 17212: train loss: 0.01139791775494814\n",
            "Epoch 17213: train loss: 0.011396877467632294\n",
            "Epoch 17214: train loss: 0.01139584369957447\n",
            "Epoch 17215: train loss: 0.011394803412258625\n",
            "Epoch 17216: train loss: 0.011393764056265354\n",
            "Epoch 17217: train loss: 0.011392731219530106\n",
            "Epoch 17218: train loss: 0.011391696520149708\n",
            "Epoch 17219: train loss: 0.011390654370188713\n",
            "Epoch 17220: train loss: 0.011389622464776039\n",
            "Epoch 17221: train loss: 0.011388585902750492\n",
            "Epoch 17222: train loss: 0.011387549340724945\n",
            "Epoch 17223: train loss: 0.011386511847376823\n",
            "Epoch 17224: train loss: 0.011385472491383553\n",
            "Epoch 17225: train loss: 0.011384437792003155\n",
            "Epoch 17226: train loss: 0.011383402161300182\n",
            "Epoch 17227: train loss: 0.011382367461919785\n",
            "Epoch 17228: train loss: 0.011381330899894238\n",
            "Epoch 17229: train loss: 0.011380298994481564\n",
            "Epoch 17230: train loss: 0.011379257775843143\n",
            "Epoch 17231: train loss: 0.011378228664398193\n",
            "Epoch 17232: train loss: 0.011377192102372646\n",
            "Epoch 17233: train loss: 0.011376154609024525\n",
            "Epoch 17234: train loss: 0.011375121772289276\n",
            "Epoch 17235: train loss: 0.011374086141586304\n",
            "Epoch 17236: train loss: 0.011373053304851055\n",
            "Epoch 17237: train loss: 0.011372019536793232\n",
            "Epoch 17238: train loss: 0.011370982974767685\n",
            "Epoch 17239: train loss: 0.011369953863322735\n",
            "Epoch 17240: train loss: 0.011368920095264912\n",
            "Epoch 17241: train loss: 0.011367887258529663\n",
            "Epoch 17242: train loss: 0.01136685162782669\n",
            "Epoch 17243: train loss: 0.011365820653736591\n",
            "Epoch 17244: train loss: 0.011364787817001343\n",
            "Epoch 17245: train loss: 0.011363762430846691\n",
            "Epoch 17246: train loss: 0.011362716555595398\n",
            "Epoch 17247: train loss: 0.011361688375473022\n",
            "Epoch 17248: train loss: 0.011360662057995796\n",
            "Epoch 17249: train loss: 0.011359629221260548\n",
            "Epoch 17250: train loss: 0.011358599178493023\n",
            "Epoch 17251: train loss: 0.011357570067048073\n",
            "Epoch 17252: train loss: 0.011356542818248272\n",
            "Epoch 17253: train loss: 0.011355509050190449\n",
            "Epoch 17254: train loss: 0.011354477144777775\n",
            "Epoch 17255: train loss: 0.01135344710201025\n",
            "Epoch 17256: train loss: 0.01135241612792015\n",
            "Epoch 17257: train loss: 0.011351397261023521\n",
            "Epoch 17258: train loss: 0.011350363492965698\n",
            "Epoch 17259: train loss: 0.011349336244165897\n",
            "Epoch 17260: train loss: 0.01134831178933382\n",
            "Epoch 17261: train loss: 0.011347281746566296\n",
            "Epoch 17262: train loss: 0.011346254497766495\n",
            "Epoch 17263: train loss: 0.011345220729708672\n",
            "Epoch 17264: train loss: 0.01134419348090887\n",
            "Epoch 17265: train loss: 0.011343169957399368\n",
            "Epoch 17266: train loss: 0.011342145502567291\n",
            "Epoch 17267: train loss: 0.01134112011641264\n",
            "Epoch 17268: train loss: 0.011340086348354816\n",
            "Epoch 17269: train loss: 0.011339059099555016\n",
            "Epoch 17270: train loss: 0.011338029988110065\n",
            "Epoch 17271: train loss: 0.011337000876665115\n",
            "Epoch 17272: train loss: 0.01133597269654274\n",
            "Epoch 17273: train loss: 0.011334952898323536\n",
            "Epoch 17274: train loss: 0.01133392471820116\n",
            "Epoch 17275: train loss: 0.011332899332046509\n",
            "Epoch 17276: train loss: 0.011331875808537006\n",
            "Epoch 17277: train loss: 0.011330853216350079\n",
            "Epoch 17278: train loss: 0.011329826898872852\n",
            "Epoch 17279: train loss: 0.01132880337536335\n",
            "Epoch 17280: train loss: 0.011327769607305527\n",
            "Epoch 17281: train loss: 0.011326751671731472\n",
            "Epoch 17282: train loss: 0.01132572628557682\n",
            "Epoch 17283: train loss: 0.011324698105454445\n",
            "Epoch 17284: train loss: 0.01132366992533207\n",
            "Epoch 17285: train loss: 0.01132264919579029\n",
            "Epoch 17286: train loss: 0.011321624740958214\n",
            "Epoch 17287: train loss: 0.011320600286126137\n",
            "Epoch 17288: train loss: 0.01131957583129406\n",
            "Epoch 17289: train loss: 0.011318554170429707\n",
            "Epoch 17290: train loss: 0.01131752971559763\n",
            "Epoch 17291: train loss: 0.011316508986055851\n",
            "Epoch 17292: train loss: 0.011315484531223774\n",
            "Epoch 17293: train loss: 0.011314456351101398\n",
            "Epoch 17294: train loss: 0.01131343748420477\n",
            "Epoch 17295: train loss: 0.011312408372759819\n",
            "Epoch 17296: train loss: 0.011311388574540615\n",
            "Epoch 17297: train loss: 0.011310365982353687\n",
            "Epoch 17298: train loss: 0.011309342458844185\n",
            "Epoch 17299: train loss: 0.011308320797979832\n",
            "Epoch 17300: train loss: 0.01130729541182518\n",
            "Epoch 17301: train loss: 0.011306275613605976\n",
            "Epoch 17302: train loss: 0.0113052474334836\n",
            "Epoch 17303: train loss: 0.011304227635264397\n",
            "Epoch 17304: train loss: 0.011303205974400043\n",
            "Epoch 17305: train loss: 0.011302189901471138\n",
            "Epoch 17306: train loss: 0.011301170103251934\n",
            "Epoch 17307: train loss: 0.011300146579742432\n",
            "Epoch 17308: train loss: 0.011299128644168377\n",
            "Epoch 17309: train loss: 0.011298109777271748\n",
            "Epoch 17310: train loss: 0.01129708532243967\n",
            "Epoch 17311: train loss: 0.01129606831818819\n",
            "Epoch 17312: train loss: 0.011295046657323837\n",
            "Epoch 17313: train loss: 0.011294028721749783\n",
            "Epoch 17314: train loss: 0.011293012648820877\n",
            "Epoch 17315: train loss: 0.011291990987956524\n",
            "Epoch 17316: train loss: 0.011290976777672768\n",
            "Epoch 17317: train loss: 0.01128995418548584\n",
            "Epoch 17318: train loss: 0.01128893718123436\n",
            "Epoch 17319: train loss: 0.011287910863757133\n",
            "Epoch 17320: train loss: 0.011286900378763676\n",
            "Epoch 17321: train loss: 0.011285880580544472\n",
            "Epoch 17322: train loss: 0.011284862644970417\n",
            "Epoch 17323: train loss: 0.011283846572041512\n",
            "Epoch 17324: train loss: 0.011282827705144882\n",
            "Epoch 17325: train loss: 0.0112818144261837\n",
            "Epoch 17326: train loss: 0.011280793696641922\n",
            "Epoch 17327: train loss: 0.011279781349003315\n",
            "Epoch 17328: train loss: 0.011278768070042133\n",
            "Epoch 17329: train loss: 0.01127774827182293\n",
            "Epoch 17330: train loss: 0.01127673126757145\n",
            "Epoch 17331: train loss: 0.01127571240067482\n",
            "Epoch 17332: train loss: 0.011274698190391064\n",
            "Epoch 17333: train loss: 0.011273684911429882\n",
            "Epoch 17334: train loss: 0.011272666975855827\n",
            "Epoch 17335: train loss: 0.01127165649086237\n",
            "Epoch 17336: train loss: 0.011270638555288315\n",
            "Epoch 17337: train loss: 0.011269623413681984\n",
            "Epoch 17338: train loss: 0.01126860547810793\n",
            "Epoch 17339: train loss: 0.01126759871840477\n",
            "Epoch 17340: train loss: 0.01126658171415329\n",
            "Epoch 17341: train loss: 0.011265565641224384\n",
            "Epoch 17342: train loss: 0.011264554224908352\n",
            "Epoch 17343: train loss: 0.011263540014624596\n",
            "Epoch 17344: train loss: 0.011262527666985989\n",
            "Epoch 17345: train loss: 0.011261509731411934\n",
            "Epoch 17346: train loss: 0.011260502971708775\n",
            "Epoch 17347: train loss: 0.011259484104812145\n",
            "Epoch 17348: train loss: 0.011258468963205814\n",
            "Epoch 17349: train loss: 0.011257462203502655\n",
            "Epoch 17350: train loss: 0.011256445199251175\n",
            "Epoch 17351: train loss: 0.011255432851612568\n",
            "Epoch 17352: train loss: 0.011254417710006237\n",
            "Epoch 17353: train loss: 0.011253406293690205\n",
            "Epoch 17354: train loss: 0.011252392083406448\n",
            "Epoch 17355: train loss: 0.011251382529735565\n",
            "Epoch 17356: train loss: 0.011250368319451809\n",
            "Epoch 17357: train loss: 0.011249360628426075\n",
            "Epoch 17358: train loss: 0.01124834269285202\n",
            "Epoch 17359: train loss: 0.011247334070503712\n",
            "Epoch 17360: train loss: 0.011246319860219955\n",
            "Epoch 17361: train loss: 0.011245312169194221\n",
            "Epoch 17362: train loss: 0.011244297958910465\n",
            "Epoch 17363: train loss: 0.01124329399317503\n",
            "Epoch 17364: train loss: 0.011242282576858997\n",
            "Epoch 17365: train loss: 0.01124127022922039\n",
            "Epoch 17366: train loss: 0.01124026719480753\n",
            "Epoch 17367: train loss: 0.011239257641136646\n",
            "Epoch 17368: train loss: 0.011238250881433487\n",
            "Epoch 17369: train loss: 0.011237245984375477\n",
            "Epoch 17370: train loss: 0.011236228048801422\n",
            "Epoch 17371: train loss: 0.01123521663248539\n",
            "Epoch 17372: train loss: 0.011234207078814507\n",
            "Epoch 17373: train loss: 0.011233198456466198\n",
            "Epoch 17374: train loss: 0.011232188902795315\n",
            "Epoch 17375: train loss: 0.011231190524995327\n",
            "Epoch 17376: train loss: 0.01123017817735672\n",
            "Epoch 17377: train loss: 0.011229166761040688\n",
            "Epoch 17378: train loss: 0.011228160932660103\n",
            "Epoch 17379: train loss: 0.011227152310311794\n",
            "Epoch 17380: train loss: 0.01122614461928606\n",
            "Epoch 17381: train loss: 0.011225135065615177\n",
            "Epoch 17382: train loss: 0.011224129237234592\n",
            "Epoch 17383: train loss: 0.01122311968356371\n",
            "Epoch 17384: train loss: 0.011222115717828274\n",
            "Epoch 17385: train loss: 0.011221108958125114\n",
            "Epoch 17386: train loss: 0.011220104061067104\n",
            "Epoch 17387: train loss: 0.011219090782105923\n",
            "Epoch 17388: train loss: 0.011218084022402763\n",
            "Epoch 17389: train loss: 0.011217073537409306\n",
            "Epoch 17390: train loss: 0.011216073296964169\n",
            "Epoch 17391: train loss: 0.01121506281197071\n",
            "Epoch 17392: train loss: 0.011214063502848148\n",
            "Epoch 17393: train loss: 0.011213057674467564\n",
            "Epoch 17394: train loss: 0.01121204998344183\n",
            "Epoch 17395: train loss: 0.011211050674319267\n",
            "Epoch 17396: train loss: 0.011210044845938683\n",
            "Epoch 17397: train loss: 0.011209044605493546\n",
            "Epoch 17398: train loss: 0.011208039708435535\n",
            "Epoch 17399: train loss: 0.01120703760534525\n",
            "Epoch 17400: train loss: 0.011206031776964664\n",
            "Epoch 17401: train loss: 0.011205027811229229\n",
            "Epoch 17402: train loss: 0.011204026639461517\n",
            "Epoch 17403: train loss: 0.011203024536371231\n",
            "Epoch 17404: train loss: 0.011202024295926094\n",
            "Epoch 17405: train loss: 0.011201022192835808\n",
            "Epoch 17406: train loss: 0.011200015433132648\n",
            "Epoch 17407: train loss: 0.011199011467397213\n",
            "Epoch 17408: train loss: 0.0111980140209198\n",
            "Epoch 17409: train loss: 0.011197010055184364\n",
            "Epoch 17410: train loss: 0.011196007020771503\n",
            "Epoch 17411: train loss: 0.011195003986358643\n",
            "Epoch 17412: train loss: 0.01119400467723608\n",
            "Epoch 17413: train loss: 0.01119299978017807\n",
            "Epoch 17414: train loss: 0.011192000471055508\n",
            "Epoch 17415: train loss: 0.01119100209325552\n",
            "Epoch 17416: train loss: 0.011189999058842659\n",
            "Epoch 17417: train loss: 0.011188997887074947\n",
            "Epoch 17418: train loss: 0.011187994852662086\n",
            "Epoch 17419: train loss: 0.011186990886926651\n",
            "Epoch 17420: train loss: 0.011185994371771812\n",
            "Epoch 17421: train loss: 0.011184992268681526\n",
            "Epoch 17422: train loss: 0.011184001341462135\n",
            "Epoch 17423: train loss: 0.0111829973757267\n",
            "Epoch 17424: train loss: 0.011181998997926712\n",
            "Epoch 17425: train loss: 0.011180998757481575\n",
            "Epoch 17426: train loss: 0.011180000379681587\n",
            "Epoch 17427: train loss: 0.011178996413946152\n",
            "Epoch 17428: train loss: 0.011177998967468739\n",
            "Epoch 17429: train loss: 0.0111770024523139\n",
            "Epoch 17430: train loss: 0.011176013387739658\n",
            "Epoch 17431: train loss: 0.011175008490681648\n",
            "Epoch 17432: train loss: 0.01117401197552681\n",
            "Epoch 17433: train loss: 0.011173010803759098\n",
            "Epoch 17434: train loss: 0.01117201242595911\n",
            "Epoch 17435: train loss: 0.011171015910804272\n",
            "Epoch 17436: train loss: 0.011170021258294582\n",
            "Epoch 17437: train loss: 0.011169017292559147\n",
            "Epoch 17438: train loss: 0.011168018914759159\n",
            "Epoch 17439: train loss: 0.011167025193572044\n",
            "Epoch 17440: train loss: 0.011166026815772057\n",
            "Epoch 17441: train loss: 0.011165032163262367\n",
            "Epoch 17442: train loss: 0.011164029128849506\n",
            "Epoch 17443: train loss: 0.011163032613694668\n",
            "Epoch 17444: train loss: 0.011162039823830128\n",
            "Epoch 17445: train loss: 0.011161042377352715\n",
            "Epoch 17446: train loss: 0.011160047724843025\n",
            "Epoch 17447: train loss: 0.011159047484397888\n",
            "Epoch 17448: train loss: 0.011158046312630177\n",
            "Epoch 17449: train loss: 0.011157055385410786\n",
            "Epoch 17450: train loss: 0.011156061664223671\n",
            "Epoch 17451: train loss: 0.011155059561133385\n",
            "Epoch 17452: train loss: 0.011154069565236568\n",
            "Epoch 17453: train loss: 0.011153075844049454\n",
            "Epoch 17454: train loss: 0.01115207839757204\n",
            "Epoch 17455: train loss: 0.011151080951094627\n",
            "Epoch 17456: train loss: 0.011150090023875237\n",
            "Epoch 17457: train loss: 0.0111490897834301\n",
            "Epoch 17458: train loss: 0.011148099787533283\n",
            "Epoch 17459: train loss: 0.011147106997668743\n",
            "Epoch 17460: train loss: 0.011146108619868755\n",
            "Epoch 17461: train loss: 0.011145118623971939\n",
            "Epoch 17462: train loss: 0.011144126765429974\n",
            "Epoch 17463: train loss: 0.01114313118159771\n",
            "Epoch 17464: train loss: 0.011142142117023468\n",
            "Epoch 17465: train loss: 0.011141153983771801\n",
            "Epoch 17466: train loss: 0.01114016305655241\n",
            "Epoch 17467: train loss: 0.01113916840404272\n",
            "Epoch 17468: train loss: 0.011138170957565308\n",
            "Epoch 17469: train loss: 0.011137190274894238\n",
            "Epoch 17470: train loss: 0.011136193759739399\n",
            "Epoch 17471: train loss: 0.011135200038552284\n",
            "Epoch 17472: train loss: 0.011134212836623192\n",
            "Epoch 17473: train loss: 0.011133219115436077\n",
            "Epoch 17474: train loss: 0.01113223284482956\n",
            "Epoch 17475: train loss: 0.01113123632967472\n",
            "Epoch 17476: train loss: 0.011130252853035927\n",
            "Epoch 17477: train loss: 0.011129259131848812\n",
            "Epoch 17478: train loss: 0.01112827192991972\n",
            "Epoch 17479: train loss: 0.011127278208732605\n",
            "Epoch 17480: train loss: 0.011126288212835789\n",
            "Epoch 17481: train loss: 0.011125302873551846\n",
            "Epoch 17482: train loss: 0.011124316602945328\n",
            "Epoch 17483: train loss: 0.011123329401016235\n",
            "Epoch 17484: train loss: 0.01112233754247427\n",
            "Epoch 17485: train loss: 0.011121354065835476\n",
            "Epoch 17486: train loss: 0.011120360344648361\n",
            "Epoch 17487: train loss: 0.011119379661977291\n",
            "Epoch 17488: train loss: 0.011118385009467602\n",
            "Epoch 17489: train loss: 0.011117404326796532\n",
            "Epoch 17490: train loss: 0.01111641526222229\n",
            "Epoch 17491: train loss: 0.011115430854260921\n",
            "Epoch 17492: train loss: 0.011114442721009254\n",
            "Epoch 17493: train loss: 0.011113451793789864\n",
            "Epoch 17494: train loss: 0.011112461797893047\n",
            "Epoch 17495: train loss: 0.01111147552728653\n",
            "Epoch 17496: train loss: 0.011110489256680012\n",
            "Epoch 17497: train loss: 0.011109508574008942\n",
            "Epoch 17498: train loss: 0.011108514852821827\n",
            "Epoch 17499: train loss: 0.011107534170150757\n",
            "Epoch 17500: train loss: 0.011106549762189388\n",
            "Epoch 17501: train loss: 0.011105562560260296\n",
            "Epoch 17502: train loss: 0.011104573495686054\n",
            "Epoch 17503: train loss: 0.011103589087724686\n",
            "Epoch 17504: train loss: 0.011102602817118168\n",
            "Epoch 17505: train loss: 0.0111016184091568\n",
            "Epoch 17506: train loss: 0.011100632138550282\n",
            "Epoch 17507: train loss: 0.011099647730588913\n",
            "Epoch 17508: train loss: 0.01109866052865982\n",
            "Epoch 17509: train loss: 0.011097678914666176\n",
            "Epoch 17510: train loss: 0.011096689850091934\n",
            "Epoch 17511: train loss: 0.011095701716840267\n",
            "Epoch 17512: train loss: 0.011094714514911175\n",
            "Epoch 17513: train loss: 0.011093735694885254\n",
            "Epoch 17514: train loss: 0.01109275035560131\n",
            "Epoch 17515: train loss: 0.011091764084994793\n",
            "Epoch 17516: train loss: 0.011090783402323723\n",
            "Epoch 17517: train loss: 0.011089799925684929\n",
            "Epoch 17518: train loss: 0.011088807135820389\n",
            "Epoch 17519: train loss: 0.011087828315794468\n",
            "Epoch 17520: train loss: 0.011086845770478249\n",
            "Epoch 17521: train loss: 0.011085862293839455\n",
            "Epoch 17522: train loss: 0.01108488067984581\n",
            "Epoch 17523: train loss: 0.011083895340561867\n",
            "Epoch 17524: train loss: 0.011082911863923073\n",
            "Epoch 17525: train loss: 0.011081928387284279\n",
            "Epoch 17526: train loss: 0.011080948635935783\n",
            "Epoch 17527: train loss: 0.011079958640038967\n",
            "Epoch 17528: train loss: 0.011078976094722748\n",
            "Epoch 17529: train loss: 0.01107800006866455\n",
            "Epoch 17530: train loss: 0.011077016592025757\n",
            "Epoch 17531: train loss: 0.011076035909354687\n",
            "Epoch 17532: train loss: 0.011075051501393318\n",
            "Epoch 17533: train loss: 0.011074073612689972\n",
            "Epoch 17534: train loss: 0.011073091998696327\n",
            "Epoch 17535: train loss: 0.011072112247347832\n",
            "Epoch 17536: train loss: 0.01107113528996706\n",
            "Epoch 17537: train loss: 0.011070149019360542\n",
            "Epoch 17538: train loss: 0.011069169268012047\n",
            "Epoch 17539: train loss: 0.011068194173276424\n",
            "Epoch 17540: train loss: 0.011067215353250504\n",
            "Epoch 17541: train loss: 0.011066232807934284\n",
            "Epoch 17542: train loss: 0.011065255850553513\n",
            "Epoch 17543: train loss: 0.01106428075581789\n",
            "Epoch 17544: train loss: 0.011063302867114544\n",
            "Epoch 17545: train loss: 0.01106231939047575\n",
            "Epoch 17546: train loss: 0.011061346158385277\n",
            "Epoch 17547: train loss: 0.011060365475714207\n",
            "Epoch 17548: train loss: 0.011059386655688286\n",
            "Epoch 17549: train loss: 0.011058412492275238\n",
            "Epoch 17550: train loss: 0.011057436466217041\n",
            "Epoch 17551: train loss: 0.011056456714868546\n",
            "Epoch 17552: train loss: 0.011055479757487774\n",
            "Epoch 17553: train loss: 0.011054501868784428\n",
            "Epoch 17554: train loss: 0.011053524911403656\n",
            "Epoch 17555: train loss: 0.011052549816668034\n",
            "Epoch 17556: train loss: 0.011051575653254986\n",
            "Epoch 17557: train loss: 0.011050591245293617\n",
            "Epoch 17558: train loss: 0.011049618013203144\n",
            "Epoch 17559: train loss: 0.011048641987144947\n",
            "Epoch 17560: train loss: 0.011047668755054474\n",
            "Epoch 17561: train loss: 0.011046692728996277\n",
            "Epoch 17562: train loss: 0.011045720428228378\n",
            "Epoch 17563: train loss: 0.01104474626481533\n",
            "Epoch 17564: train loss: 0.011043773032724857\n",
            "Epoch 17565: train loss: 0.01104279700666666\n",
            "Epoch 17566: train loss: 0.011041822843253613\n",
            "Epoch 17567: train loss: 0.011040840297937393\n",
            "Epoch 17568: train loss: 0.011039869859814644\n",
            "Epoch 17569: train loss: 0.011038889177143574\n",
            "Epoch 17570: train loss: 0.011037920601665974\n",
            "Epoch 17571: train loss: 0.011036945506930351\n",
            "Epoch 17572: train loss: 0.011035970412194729\n",
            "Epoch 17573: train loss: 0.01103499997407198\n",
            "Epoch 17574: train loss: 0.011034022085368633\n",
            "Epoch 17575: train loss: 0.011033051647245884\n",
            "Epoch 17576: train loss: 0.011032079346477985\n",
            "Epoch 17577: train loss: 0.011031106114387512\n",
            "Epoch 17578: train loss: 0.011030134744942188\n",
            "Epoch 17579: train loss: 0.011029161512851715\n",
            "Epoch 17580: train loss: 0.011028180830180645\n",
            "Epoch 17581: train loss: 0.011027206666767597\n",
            "Epoch 17582: train loss: 0.011026240885257721\n",
            "Epoch 17583: train loss: 0.011025266721844673\n",
            "Epoch 17584: train loss: 0.011024296283721924\n",
            "Epoch 17585: train loss: 0.011023320257663727\n",
            "Epoch 17586: train loss: 0.011022352613508701\n",
            "Epoch 17587: train loss: 0.011021377518773079\n",
            "Epoch 17588: train loss: 0.011020404286682606\n",
            "Epoch 17589: train loss: 0.011019432917237282\n",
            "Epoch 17590: train loss: 0.011018460616469383\n",
            "Epoch 17591: train loss: 0.011017488315701485\n",
            "Epoch 17592: train loss: 0.011016519740223885\n",
            "Epoch 17593: train loss: 0.011015543714165688\n",
            "Epoch 17594: train loss: 0.011014576070010662\n",
            "Epoch 17595: train loss: 0.011013606563210487\n",
            "Epoch 17596: train loss: 0.011012628674507141\n",
            "Epoch 17597: train loss: 0.011011662892997265\n",
            "Epoch 17598: train loss: 0.011010687798261642\n",
            "Epoch 17599: train loss: 0.011009718291461468\n",
            "Epoch 17600: train loss: 0.011008747853338718\n",
            "Epoch 17601: train loss: 0.011007771827280521\n",
            "Epoch 17602: train loss: 0.011006808839738369\n",
            "Epoch 17603: train loss: 0.011005841195583344\n",
            "Epoch 17604: train loss: 0.011004878208041191\n",
            "Epoch 17605: train loss: 0.011003906838595867\n",
            "Epoch 17606: train loss: 0.01100294291973114\n",
            "Epoch 17607: train loss: 0.011001971550285816\n",
            "Epoch 17608: train loss: 0.011001009494066238\n",
            "Epoch 17609: train loss: 0.011000041849911213\n",
            "Epoch 17610: train loss: 0.010999075137078762\n",
            "Epoch 17611: train loss: 0.01099811028689146\n",
            "Epoch 17612: train loss: 0.01099714171141386\n",
            "Epoch 17613: train loss: 0.01099617313593626\n",
            "Epoch 17614: train loss: 0.010995212011039257\n",
            "Epoch 17615: train loss: 0.010994244366884232\n",
            "Epoch 17616: train loss: 0.01099327765405178\n",
            "Epoch 17617: train loss: 0.010992311872541904\n",
            "Epoch 17618: train loss: 0.010991339571774006\n",
            "Epoch 17619: train loss: 0.010990387760102749\n",
            "Epoch 17620: train loss: 0.010989416390657425\n",
            "Epoch 17621: train loss: 0.0109884487465024\n",
            "Epoch 17622: train loss: 0.01098749041557312\n",
            "Epoch 17623: train loss: 0.010986518114805222\n",
            "Epoch 17624: train loss: 0.010985556058585644\n",
            "Epoch 17625: train loss: 0.010984593071043491\n",
            "Epoch 17626: train loss: 0.01098362822085619\n",
            "Epoch 17627: train loss: 0.010982656851410866\n",
            "Epoch 17628: train loss: 0.010981699451804161\n",
            "Epoch 17629: train loss: 0.010980729013681412\n",
            "Epoch 17630: train loss: 0.010979768820106983\n",
            "Epoch 17631: train loss: 0.010978803038597107\n",
            "Epoch 17632: train loss: 0.01097783911973238\n",
            "Epoch 17633: train loss: 0.010976872406899929\n",
            "Epoch 17634: train loss: 0.010975910350680351\n",
            "Epoch 17635: train loss: 0.010974944569170475\n",
            "Epoch 17636: train loss: 0.01097398716956377\n",
            "Epoch 17637: train loss: 0.010973017662763596\n",
            "Epoch 17638: train loss: 0.010972056537866592\n",
            "Epoch 17639: train loss: 0.010971090756356716\n",
            "Epoch 17640: train loss: 0.01097012683749199\n",
            "Epoch 17641: train loss: 0.010969161987304688\n",
            "Epoch 17642: train loss: 0.01096819993108511\n",
            "Epoch 17643: train loss: 0.010967235080897808\n",
            "Epoch 17644: train loss: 0.010966272093355656\n",
            "Epoch 17645: train loss: 0.010965310037136078\n",
            "Epoch 17646: train loss: 0.010964347049593925\n",
            "Epoch 17647: train loss: 0.010963383130729198\n",
            "Epoch 17648: train loss: 0.01096242293715477\n",
            "Epoch 17649: train loss: 0.01096146460622549\n",
            "Epoch 17650: train loss: 0.010960496962070465\n",
            "Epoch 17651: train loss: 0.010959534905850887\n",
            "Epoch 17652: train loss: 0.010958576574921608\n",
            "Epoch 17653: train loss: 0.010957615450024605\n",
            "Epoch 17654: train loss: 0.010956658981740475\n",
            "Epoch 17655: train loss: 0.010955696925520897\n",
            "Epoch 17656: train loss: 0.010954732075333595\n",
            "Epoch 17657: train loss: 0.010953770019114017\n",
            "Epoch 17658: train loss: 0.010952815413475037\n",
            "Epoch 17659: train loss: 0.010951855219900608\n",
            "Epoch 17660: train loss: 0.01095089316368103\n",
            "Epoch 17661: train loss: 0.010949937626719475\n",
            "Epoch 17662: train loss: 0.010948971845209599\n",
            "Epoch 17663: train loss: 0.01094801351428032\n",
            "Epoch 17664: train loss: 0.01094705518335104\n",
            "Epoch 17665: train loss: 0.010946091264486313\n",
            "Epoch 17666: train loss: 0.010945138521492481\n",
            "Epoch 17667: train loss: 0.010944174602627754\n",
            "Epoch 17668: train loss: 0.010943224653601646\n",
            "Epoch 17669: train loss: 0.010942261666059494\n",
            "Epoch 17670: train loss: 0.010941303335130215\n",
            "Epoch 17671: train loss: 0.010940343141555786\n",
            "Epoch 17672: train loss: 0.01093938946723938\n",
            "Epoch 17673: train loss: 0.010938435792922974\n",
            "Epoch 17674: train loss: 0.01093747466802597\n",
            "Epoch 17675: train loss: 0.010936522856354713\n",
            "Epoch 17676: train loss: 0.010935571976006031\n",
            "Epoch 17677: train loss: 0.010934608988463879\n",
            "Epoch 17678: train loss: 0.01093365903943777\n",
            "Epoch 17679: train loss: 0.010932707227766514\n",
            "Epoch 17680: train loss: 0.010931743308901787\n",
            "Epoch 17681: train loss: 0.010930792428553104\n",
            "Epoch 17682: train loss: 0.010929839685559273\n",
            "Epoch 17683: train loss: 0.010928885079920292\n",
            "Epoch 17684: train loss: 0.010927933268249035\n",
            "Epoch 17685: train loss: 0.01092697586864233\n",
            "Epoch 17686: train loss: 0.01092602126300335\n",
            "Epoch 17687: train loss: 0.010925065726041794\n",
            "Epoch 17688: train loss: 0.010924113914370537\n",
            "Epoch 17689: train loss: 0.010923156514763832\n",
            "Epoch 17690: train loss: 0.010922202840447426\n",
            "Epoch 17691: train loss: 0.010921251960098743\n",
            "Epoch 17692: train loss: 0.010920297354459763\n",
            "Epoch 17693: train loss: 0.010919349268078804\n",
            "Epoch 17694: train loss: 0.010918394662439823\n",
            "Epoch 17695: train loss: 0.010917440988123417\n",
            "Epoch 17696: train loss: 0.010916486382484436\n",
            "Epoch 17697: train loss: 0.010915529914200306\n",
            "Epoch 17698: train loss: 0.010914581827819347\n",
            "Epoch 17699: train loss: 0.010913621634244919\n",
            "Epoch 17700: train loss: 0.010912668891251087\n",
            "Epoch 17701: train loss: 0.010911718010902405\n",
            "Epoch 17702: train loss: 0.010910766199231148\n",
            "Epoch 17703: train loss: 0.01090981625020504\n",
            "Epoch 17704: train loss: 0.01090885978192091\n",
            "Epoch 17705: train loss: 0.010907907038927078\n",
            "Epoch 17706: train loss: 0.010906959883868694\n",
            "Epoch 17707: train loss: 0.010906005278229713\n",
            "Epoch 17708: train loss: 0.010905053466558456\n",
            "Epoch 17709: train loss: 0.010904108174145222\n",
            "Epoch 17710: train loss: 0.01090315356850624\n",
            "Epoch 17711: train loss: 0.01090219896286726\n",
            "Epoch 17712: train loss: 0.010901245288550854\n",
            "Epoch 17713: train loss: 0.01090029627084732\n",
            "Epoch 17714: train loss: 0.010899343527853489\n",
            "Epoch 17715: train loss: 0.010898391716182232\n",
            "Epoch 17716: train loss: 0.010897442698478699\n",
            "Epoch 17717: train loss: 0.010896493680775166\n",
            "Epoch 17718: train loss: 0.01089554000645876\n",
            "Epoch 17719: train loss: 0.010894589126110077\n",
            "Epoch 17720: train loss: 0.010893643833696842\n",
            "Epoch 17721: train loss: 0.010892689228057861\n",
            "Epoch 17722: train loss: 0.010891739279031754\n",
            "Epoch 17723: train loss: 0.010890788398683071\n",
            "Epoch 17724: train loss: 0.010889838449656963\n",
            "Epoch 17725: train loss: 0.010888884775340557\n",
            "Epoch 17726: train loss: 0.010887938551604748\n",
            "Epoch 17727: train loss: 0.010886991396546364\n",
            "Epoch 17728: train loss: 0.010886043310165405\n",
            "Epoch 17729: train loss: 0.01088509801775217\n",
            "Epoch 17730: train loss: 0.010884149000048637\n",
            "Epoch 17731: train loss: 0.010883196257054806\n",
            "Epoch 17732: train loss: 0.01088225468993187\n",
            "Epoch 17733: train loss: 0.010881304740905762\n",
            "Epoch 17734: train loss: 0.01088035199791193\n",
            "Epoch 17735: train loss: 0.010879402980208397\n",
            "Epoch 17736: train loss: 0.010878464207053185\n",
            "Epoch 17737: train loss: 0.010877514258027077\n",
            "Epoch 17738: train loss: 0.010876571759581566\n",
            "Epoch 17739: train loss: 0.010875619947910309\n",
            "Epoch 17740: train loss: 0.010874675586819649\n",
            "Epoch 17741: train loss: 0.010873724706470966\n",
            "Epoch 17742: train loss: 0.010872777551412582\n",
            "Epoch 17743: train loss: 0.010871832258999348\n",
            "Epoch 17744: train loss: 0.010870888829231262\n",
            "Epoch 17745: train loss: 0.010869943536818027\n",
            "Epoch 17746: train loss: 0.010869006626307964\n",
            "Epoch 17747: train loss: 0.010868062265217304\n",
            "Epoch 17748: train loss: 0.010867113247513771\n",
            "Epoch 17749: train loss: 0.010866173543035984\n",
            "Epoch 17750: train loss: 0.010865231044590473\n",
            "Epoch 17751: train loss: 0.010864285752177238\n",
            "Epoch 17752: train loss: 0.010863341391086578\n",
            "Epoch 17753: train loss: 0.010862398892641068\n",
            "Epoch 17754: train loss: 0.010861450806260109\n",
            "Epoch 17755: train loss: 0.010860512033104897\n",
            "Epoch 17756: train loss: 0.010859563946723938\n",
            "Epoch 17757: train loss: 0.010858625173568726\n",
            "Epoch 17758: train loss: 0.010857685469090939\n",
            "Epoch 17759: train loss: 0.01085673738270998\n",
            "Epoch 17760: train loss: 0.010855797678232193\n",
            "Epoch 17761: train loss: 0.010854853317141533\n",
            "Epoch 17762: train loss: 0.010853911750018597\n",
            "Epoch 17763: train loss: 0.010852973908185959\n",
            "Epoch 17764: train loss: 0.010852031409740448\n",
            "Epoch 17765: train loss: 0.010851087979972363\n",
            "Epoch 17766: train loss: 0.010850145481526852\n",
            "Epoch 17767: train loss: 0.010849203914403915\n",
            "Epoch 17768: train loss: 0.010848265141248703\n",
            "Epoch 17769: train loss: 0.010847319848835468\n",
            "Epoch 17770: train loss: 0.01084638200700283\n",
            "Epoch 17771: train loss: 0.010845438577234745\n",
            "Epoch 17772: train loss: 0.010844497941434383\n",
            "Epoch 17773: train loss: 0.010843551717698574\n",
            "Epoch 17774: train loss: 0.010842608287930489\n",
            "Epoch 17775: train loss: 0.010841668583452702\n",
            "Epoch 17776: train loss: 0.010840733535587788\n",
            "Epoch 17777: train loss: 0.010839791037142277\n",
            "Epoch 17778: train loss: 0.010838848538696766\n",
            "Epoch 17779: train loss: 0.010837908834218979\n",
            "Epoch 17780: train loss: 0.010836970061063766\n",
            "Epoch 17781: train loss: 0.010836032219231129\n",
            "Epoch 17782: train loss: 0.010835093446075916\n",
            "Epoch 17783: train loss: 0.010834148153662682\n",
            "Epoch 17784: train loss: 0.010833214037120342\n",
            "Epoch 17785: train loss: 0.01083227340131998\n",
            "Epoch 17786: train loss: 0.010831327177584171\n",
            "Epoch 17787: train loss: 0.010830388404428959\n",
            "Epoch 17788: train loss: 0.010829450562596321\n",
            "Epoch 17789: train loss: 0.010828516446053982\n",
            "Epoch 17790: train loss: 0.01082757581025362\n",
            "Epoch 17791: train loss: 0.010826644487679005\n",
            "Epoch 17792: train loss: 0.010825702920556068\n",
            "Epoch 17793: train loss: 0.010824764147400856\n",
            "Epoch 17794: train loss: 0.010823828168213367\n",
            "Epoch 17795: train loss: 0.010822887532413006\n",
            "Epoch 17796: train loss: 0.010821953415870667\n",
            "Epoch 17797: train loss: 0.010821012780070305\n",
            "Epoch 17798: train loss: 0.01082008145749569\n",
            "Epoch 17799: train loss: 0.010819142684340477\n",
            "Epoch 17800: train loss: 0.010818205773830414\n",
            "Epoch 17801: train loss: 0.010817273519933224\n",
            "Epoch 17802: train loss: 0.01081633847206831\n",
            "Epoch 17803: train loss: 0.010815400630235672\n",
            "Epoch 17804: train loss: 0.01081446185708046\n",
            "Epoch 17805: train loss: 0.010813528671860695\n",
            "Epoch 17806: train loss: 0.010812591761350632\n",
            "Epoch 17807: train loss: 0.010811652056872845\n",
            "Epoch 17808: train loss: 0.010810715146362782\n",
            "Epoch 17809: train loss: 0.010809789411723614\n",
            "Epoch 17810: train loss: 0.010808845981955528\n",
            "Epoch 17811: train loss: 0.010807911865413189\n",
            "Epoch 17812: train loss: 0.010806972160935402\n",
            "Epoch 17813: train loss: 0.010806040838360786\n",
            "Epoch 17814: train loss: 0.010805102996528149\n",
            "Epoch 17815: train loss: 0.010804169811308384\n",
            "Epoch 17816: train loss: 0.010803235694766045\n",
            "Epoch 17817: train loss: 0.010802297852933407\n",
            "Epoch 17818: train loss: 0.010801366530358791\n",
            "Epoch 17819: train loss: 0.010800429619848728\n",
            "Epoch 17820: train loss: 0.010799492709338665\n",
            "Epoch 17821: train loss: 0.010798563249409199\n",
            "Epoch 17822: train loss: 0.010797633789479733\n",
            "Epoch 17823: train loss: 0.010796699672937393\n",
            "Epoch 17824: train loss: 0.010795765556395054\n",
            "Epoch 17825: train loss: 0.010794829577207565\n",
            "Epoch 17826: train loss: 0.01079389825463295\n",
            "Epoch 17827: train loss: 0.010792963206768036\n",
            "Epoch 17828: train loss: 0.01079203374683857\n",
            "Epoch 17829: train loss: 0.010791106149554253\n",
            "Epoch 17830: train loss: 0.010790168307721615\n",
            "Epoch 17831: train loss: 0.010789234191179276\n",
            "Epoch 17832: train loss: 0.010788303799927235\n",
            "Epoch 17833: train loss: 0.010787373408675194\n",
            "Epoch 17834: train loss: 0.010786439292132854\n",
            "Epoch 17835: train loss: 0.010785505175590515\n",
            "Epoch 17836: train loss: 0.010784576646983624\n",
            "Epoch 17837: train loss: 0.010783646255731583\n",
            "Epoch 17838: train loss: 0.010782715864479542\n",
            "Epoch 17839: train loss: 0.010781779885292053\n",
            "Epoch 17840: train loss: 0.010780856013298035\n",
            "Epoch 17841: train loss: 0.01077992282807827\n",
            "Epoch 17842: train loss: 0.010778994299471378\n",
            "Epoch 17843: train loss: 0.010778063908219337\n",
            "Epoch 17844: train loss: 0.010777134448289871\n",
            "Epoch 17845: train loss: 0.010776206851005554\n",
            "Epoch 17846: train loss: 0.010775280185043812\n",
            "Epoch 17847: train loss: 0.010774348862469196\n",
            "Epoch 17848: train loss: 0.01077341753989458\n",
            "Epoch 17849: train loss: 0.010772491805255413\n",
            "Epoch 17850: train loss: 0.010771558620035648\n",
            "Epoch 17851: train loss: 0.010770630091428757\n",
            "Epoch 17852: train loss: 0.010769701562821865\n",
            "Epoch 17853: train loss: 0.01076878048479557\n",
            "Epoch 17854: train loss: 0.010767852887511253\n",
            "Epoch 17855: train loss: 0.010766924358904362\n",
            "Epoch 17856: train loss: 0.010765999555587769\n",
            "Epoch 17857: train loss: 0.0107650738209486\n",
            "Epoch 17858: train loss: 0.010764148086309433\n",
            "Epoch 17859: train loss: 0.01076322328299284\n",
            "Epoch 17860: train loss: 0.01076230127364397\n",
            "Epoch 17861: train loss: 0.010761369951069355\n",
            "Epoch 17862: train loss: 0.010760445147752762\n",
            "Epoch 17863: train loss: 0.010759517550468445\n",
            "Epoch 17864: train loss: 0.010758589953184128\n",
            "Epoch 17865: train loss: 0.010757663287222385\n",
            "Epoch 17866: train loss: 0.010756735689938068\n",
            "Epoch 17867: train loss: 0.010755814611911774\n",
            "Epoch 17868: train loss: 0.010754886083304882\n",
            "Epoch 17869: train loss: 0.010753960348665714\n",
            "Epoch 17870: train loss: 0.010753038339316845\n",
            "Epoch 17871: train loss: 0.010752111673355103\n",
            "Epoch 17872: train loss: 0.01075118500739336\n",
            "Epoch 17873: train loss: 0.010750258341431618\n",
            "Epoch 17874: train loss: 0.010749335400760174\n",
            "Epoch 17875: train loss: 0.010748413391411304\n",
            "Epoch 17876: train loss: 0.010747493244707584\n",
            "Epoch 17877: train loss: 0.010746562853455544\n",
            "Epoch 17878: train loss: 0.010745638981461525\n",
            "Epoch 17879: train loss: 0.010744714178144932\n",
            "Epoch 17880: train loss: 0.010743792168796062\n",
            "Epoch 17881: train loss: 0.01074286550283432\n",
            "Epoch 17882: train loss: 0.01074194349348545\n",
            "Epoch 17883: train loss: 0.010741017758846283\n",
            "Epoch 17884: train loss: 0.010740092024207115\n",
            "Epoch 17885: train loss: 0.010739165358245373\n",
            "Epoch 17886: train loss: 0.010738243348896503\n",
            "Epoch 17887: train loss: 0.01073731854557991\n",
            "Epoch 17888: train loss: 0.010736391879618168\n",
            "Epoch 17889: train loss: 0.010735474526882172\n",
            "Epoch 17890: train loss: 0.010734550654888153\n",
            "Epoch 17891: train loss: 0.010733624920248985\n",
            "Epoch 17892: train loss: 0.010732701979577541\n",
            "Epoch 17893: train loss: 0.01073178369551897\n",
            "Epoch 17894: train loss: 0.010730858892202377\n",
            "Epoch 17895: train loss: 0.010729936882853508\n",
            "Epoch 17896: train loss: 0.010729019530117512\n",
            "Epoch 17897: train loss: 0.010728099383413792\n",
            "Epoch 17898: train loss: 0.010727178305387497\n",
            "Epoch 17899: train loss: 0.010726246982812881\n",
            "Epoch 17900: train loss: 0.010725329630076885\n",
            "Epoch 17901: train loss: 0.010724404826760292\n",
            "Epoch 17902: train loss: 0.010723489336669445\n",
            "Epoch 17903: train loss: 0.010722573846578598\n",
            "Epoch 17904: train loss: 0.010721655562520027\n",
            "Epoch 17905: train loss: 0.01072072982788086\n",
            "Epoch 17906: train loss: 0.01071980595588684\n",
            "Epoch 17907: train loss: 0.010718881152570248\n",
            "Epoch 17908: train loss: 0.010717971250414848\n",
            "Epoch 17909: train loss: 0.010717048309743404\n",
            "Epoch 17910: train loss: 0.010716129094362259\n",
            "Epoch 17911: train loss: 0.010715210810303688\n",
            "Epoch 17912: train loss: 0.010714292526245117\n",
            "Epoch 17913: train loss: 0.010713368654251099\n",
            "Epoch 17914: train loss: 0.010712448507547379\n",
            "Epoch 17915: train loss: 0.010711528360843658\n",
            "Epoch 17916: train loss: 0.010710611008107662\n",
            "Epoch 17917: train loss: 0.010709688998758793\n",
            "Epoch 17918: train loss: 0.010708779096603394\n",
            "Epoch 17919: train loss: 0.010707860812544823\n",
            "Epoch 17920: train loss: 0.010706945322453976\n",
            "Epoch 17921: train loss: 0.010706025175750256\n",
            "Epoch 17922: train loss: 0.010705112479627132\n",
            "Epoch 17923: train loss: 0.010704189538955688\n",
            "Epoch 17924: train loss: 0.01070327591150999\n",
            "Epoch 17925: train loss: 0.01070235576480627\n",
            "Epoch 17926: train loss: 0.010701441206037998\n",
            "Epoch 17927: train loss: 0.010700524784624577\n",
            "Epoch 17928: train loss: 0.010699611157178879\n",
            "Epoch 17929: train loss: 0.010698695667088032\n",
            "Epoch 17930: train loss: 0.010697777383029461\n",
            "Epoch 17931: train loss: 0.01069685909897089\n",
            "Epoch 17932: train loss: 0.010695948265492916\n",
            "Epoch 17933: train loss: 0.01069503091275692\n",
            "Epoch 17934: train loss: 0.010694113560020924\n",
            "Epoch 17935: train loss: 0.010693199932575226\n",
            "Epoch 17936: train loss: 0.010692286305129528\n",
            "Epoch 17937: train loss: 0.010691370815038681\n",
            "Epoch 17938: train loss: 0.01069045253098011\n",
            "Epoch 17939: train loss: 0.010689537040889263\n",
            "Epoch 17940: train loss: 0.010688629001379013\n",
            "Epoch 17941: train loss: 0.010687717236578465\n",
            "Epoch 17942: train loss: 0.01068679615855217\n",
            "Epoch 17943: train loss: 0.010685883462429047\n",
            "Epoch 17944: train loss: 0.0106849679723382\n",
            "Epoch 17945: train loss: 0.010684059001505375\n",
            "Epoch 17946: train loss: 0.010683145374059677\n",
            "Epoch 17947: train loss: 0.010682236403226852\n",
            "Epoch 17948: train loss: 0.010681312531232834\n",
            "Epoch 17949: train loss: 0.010680398903787136\n",
            "Epoch 17950: train loss: 0.010679483413696289\n",
            "Epoch 17951: train loss: 0.01067857351154089\n",
            "Epoch 17952: train loss: 0.010677658952772617\n",
            "Epoch 17953: train loss: 0.010676744394004345\n",
            "Epoch 17954: train loss: 0.010675831697881222\n",
            "Epoch 17955: train loss: 0.010674920864403248\n",
            "Epoch 17956: train loss: 0.01067400723695755\n",
            "Epoch 17957: train loss: 0.010673094540834427\n",
            "Epoch 17958: train loss: 0.010672185570001602\n",
            "Epoch 17959: train loss: 0.010671271942555904\n",
            "Epoch 17960: train loss: 0.010670353658497334\n",
            "Epoch 17961: train loss: 0.01066944282501936\n",
            "Epoch 17962: train loss: 0.010668528266251087\n",
            "Epoch 17963: train loss: 0.010667620226740837\n",
            "Epoch 17964: train loss: 0.01066670659929514\n",
            "Epoch 17965: train loss: 0.010665788315236568\n",
            "Epoch 17966: train loss: 0.010664881207048893\n",
            "Epoch 17967: train loss: 0.010663967579603195\n",
            "Epoch 17968: train loss: 0.01066306047141552\n",
            "Epoch 17969: train loss: 0.010662153363227844\n",
            "Epoch 17970: train loss: 0.010661237873136997\n",
            "Epoch 17971: train loss: 0.010660332627594471\n",
            "Epoch 17972: train loss: 0.01065942645072937\n",
            "Epoch 17973: train loss: 0.010658512823283672\n",
            "Epoch 17974: train loss: 0.010657601989805698\n",
            "Epoch 17975: train loss: 0.010656693950295448\n",
            "Epoch 17976: train loss: 0.010655784979462624\n",
            "Epoch 17977: train loss: 0.010654876008629799\n",
            "Epoch 17978: train loss: 0.010653968900442123\n",
            "Epoch 17979: train loss: 0.010653060860931873\n",
            "Epoch 17980: train loss: 0.010652152821421623\n",
            "Epoch 17981: train loss: 0.010651242919266224\n",
            "Epoch 17982: train loss: 0.010650333017110825\n",
            "Epoch 17983: train loss: 0.01064943429082632\n",
            "Epoch 17984: train loss: 0.010648522526025772\n",
            "Epoch 17985: train loss: 0.010647622868418694\n",
            "Epoch 17986: train loss: 0.010646716691553593\n",
            "Epoch 17987: train loss: 0.010645807720720768\n",
            "Epoch 17988: train loss: 0.010644899681210518\n",
            "Epoch 17989: train loss: 0.010643991641700268\n",
            "Epoch 17990: train loss: 0.010643084533512592\n",
            "Epoch 17991: train loss: 0.010642181150615215\n",
            "Epoch 17992: train loss: 0.010641271248459816\n",
            "Epoch 17993: train loss: 0.010640367865562439\n",
            "Epoch 17994: train loss: 0.01063946820795536\n",
            "Epoch 17995: train loss: 0.010638553649187088\n",
            "Epoch 17996: train loss: 0.010637651197612286\n",
            "Epoch 17997: train loss: 0.010636747814714909\n",
            "Epoch 17998: train loss: 0.010635843500494957\n",
            "Epoch 17999: train loss: 0.010634937323629856\n",
            "Epoch 18000: train loss: 0.01063403021544218\n",
            "Epoch 18001: train loss: 0.010633123107254505\n",
            "Epoch 18002: train loss: 0.010632219724357128\n",
            "Epoch 18003: train loss: 0.010631313547492027\n",
            "Epoch 18004: train loss: 0.01063041016459465\n",
            "Epoch 18005: train loss: 0.010629505850374699\n",
            "Epoch 18006: train loss: 0.01062860433012247\n",
            "Epoch 18007: train loss: 0.01062769815325737\n",
            "Epoch 18008: train loss: 0.010626797564327717\n",
            "Epoch 18009: train loss: 0.010625888593494892\n",
            "Epoch 18010: train loss: 0.010624981485307217\n",
            "Epoch 18011: train loss: 0.01062407810240984\n",
            "Epoch 18012: train loss: 0.010623171925544739\n",
            "Epoch 18013: train loss: 0.01062227226793766\n",
            "Epoch 18014: train loss: 0.01062136609107256\n",
            "Epoch 18015: train loss: 0.010620464570820332\n",
            "Epoch 18016: train loss: 0.010619557462632656\n",
            "Epoch 18017: train loss: 0.010618656873703003\n",
            "Epoch 18018: train loss: 0.0106177544221282\n",
            "Epoch 18019: train loss: 0.010616847313940525\n",
            "Epoch 18020: train loss: 0.010615944862365723\n",
            "Epoch 18021: train loss: 0.010615040548145771\n",
            "Epoch 18022: train loss: 0.010614139027893543\n",
            "Epoch 18023: train loss: 0.010613231919705868\n",
            "Epoch 18024: train loss: 0.010612335987389088\n",
            "Epoch 18025: train loss: 0.010611433535814285\n",
            "Epoch 18026: train loss: 0.010610523633658886\n",
            "Epoch 18027: train loss: 0.01060961838811636\n",
            "Epoch 18028: train loss: 0.010608717799186707\n",
            "Epoch 18029: train loss: 0.010607816278934479\n",
            "Epoch 18030: train loss: 0.01060691848397255\n",
            "Epoch 18031: train loss: 0.010606013238430023\n",
            "Epoch 18032: train loss: 0.010605111718177795\n",
            "Epoch 18033: train loss: 0.010604208335280418\n",
            "Epoch 18034: train loss: 0.010603305883705616\n",
            "Epoch 18035: train loss: 0.010602408088743687\n",
            "Epoch 18036: train loss: 0.010601506568491459\n",
            "Epoch 18037: train loss: 0.010600600391626358\n",
            "Epoch 18038: train loss: 0.010599703527987003\n",
            "Epoch 18039: train loss: 0.010598808526992798\n",
            "Epoch 18040: train loss: 0.010597904212772846\n",
            "Epoch 18041: train loss: 0.010597006417810917\n",
            "Epoch 18042: train loss: 0.01059610117226839\n",
            "Epoch 18043: train loss: 0.010595201514661312\n",
            "Epoch 18044: train loss: 0.010594303719699383\n",
            "Epoch 18045: train loss: 0.010593402199447155\n",
            "Epoch 18046: train loss: 0.010592509061098099\n",
            "Epoch 18047: train loss: 0.010591616854071617\n",
            "Epoch 18048: train loss: 0.01059071347117424\n",
            "Epoch 18049: train loss: 0.010589824058115482\n",
            "Epoch 18050: train loss: 0.010588928125798702\n",
            "Epoch 18051: train loss: 0.010588030330836773\n",
            "Epoch 18052: train loss: 0.010587135329842567\n",
            "Epoch 18053: train loss: 0.010586232878267765\n",
            "Epoch 18054: train loss: 0.010585339739918709\n",
            "Epoch 18055: train loss: 0.010584445670247078\n",
            "Epoch 18056: train loss: 0.010583546943962574\n",
            "Epoch 18057: train loss: 0.010582655668258667\n",
            "Epoch 18058: train loss: 0.010581758804619312\n",
            "Epoch 18059: train loss: 0.010580860078334808\n",
            "Epoch 18060: train loss: 0.010579967871308327\n",
            "Epoch 18061: train loss: 0.010579075664281845\n",
            "Epoch 18062: train loss: 0.010578176937997341\n",
            "Epoch 18063: train loss: 0.01057728286832571\n",
            "Epoch 18064: train loss: 0.010576385073363781\n",
            "Epoch 18065: train loss: 0.010575490072369576\n",
            "Epoch 18066: train loss: 0.010574600659310818\n",
            "Epoch 18067: train loss: 0.010573702864348888\n",
            "Epoch 18068: train loss: 0.010572806932032108\n",
            "Epoch 18069: train loss: 0.010571914725005627\n",
            "Epoch 18070: train loss: 0.010571015998721123\n",
            "Epoch 18071: train loss: 0.010570119135081768\n",
            "Epoch 18072: train loss: 0.010569230653345585\n",
            "Epoch 18073: train loss: 0.010568334721028805\n",
            "Epoch 18074: train loss: 0.010567441582679749\n",
            "Epoch 18075: train loss: 0.010566542856395245\n",
            "Epoch 18076: train loss: 0.01056565809994936\n",
            "Epoch 18077: train loss: 0.010564763098955154\n",
            "Epoch 18078: train loss: 0.010563862510025501\n",
            "Epoch 18079: train loss: 0.010562969371676445\n",
            "Epoch 18080: train loss: 0.010562079958617687\n",
            "Epoch 18081: train loss: 0.010561183094978333\n",
            "Epoch 18082: train loss: 0.010560284368693829\n",
            "Epoch 18083: train loss: 0.010559391230344772\n",
            "Epoch 18084: train loss: 0.010558496229350567\n",
            "Epoch 18085: train loss: 0.01055760309100151\n",
            "Epoch 18086: train loss: 0.010556708090007305\n",
            "Epoch 18087: train loss: 0.010555819608271122\n",
            "Epoch 18088: train loss: 0.010554923675954342\n",
            "Epoch 18089: train loss: 0.010554032400250435\n",
            "Epoch 18090: train loss: 0.010553144849836826\n",
            "Epoch 18091: train loss: 0.01055224984884262\n",
            "Epoch 18092: train loss: 0.010551352053880692\n",
            "Epoch 18093: train loss: 0.010550458915531635\n",
            "Epoch 18094: train loss: 0.010549568571150303\n",
            "Epoch 18095: train loss: 0.010548677295446396\n",
            "Epoch 18096: train loss: 0.010547791607677937\n",
            "Epoch 18097: train loss: 0.010546894744038582\n",
            "Epoch 18098: train loss: 0.010546001605689526\n",
            "Epoch 18099: train loss: 0.010545113123953342\n",
            "Epoch 18100: train loss: 0.010544217191636562\n",
            "Epoch 18101: train loss: 0.010543331503868103\n",
            "Epoch 18102: train loss: 0.010542438365519047\n",
            "Epoch 18103: train loss: 0.010541548021137714\n",
            "Epoch 18104: train loss: 0.010540654882788658\n",
            "Epoch 18105: train loss: 0.0105397654697299\n",
            "Epoch 18106: train loss: 0.010538868606090546\n",
            "Epoch 18107: train loss: 0.010537981055676937\n",
            "Epoch 18108: train loss: 0.010537086986005306\n",
            "Epoch 18109: train loss: 0.010536199435591698\n",
            "Epoch 18110: train loss: 0.010535312816500664\n",
            "Epoch 18111: train loss: 0.010534420609474182\n",
            "Epoch 18112: train loss: 0.010533532127737999\n",
            "Epoch 18113: train loss: 0.010532643646001816\n",
            "Epoch 18114: train loss: 0.010531752370297909\n",
            "Epoch 18115: train loss: 0.010530869476497173\n",
            "Epoch 18116: train loss: 0.010529979132115841\n",
            "Epoch 18117: train loss: 0.010529087856411934\n",
            "Epoch 18118: train loss: 0.0105282012373209\n",
            "Epoch 18119: train loss: 0.01052731741219759\n",
            "Epoch 18120: train loss: 0.010526428930461407\n",
            "Epoch 18121: train loss: 0.010525539517402649\n",
            "Epoch 18122: train loss: 0.010524651035666466\n",
            "Epoch 18123: train loss: 0.010523769073188305\n",
            "Epoch 18124: train loss: 0.010522880591452122\n",
            "Epoch 18125: train loss: 0.010521997697651386\n",
            "Epoch 18126: train loss: 0.010521105490624905\n",
            "Epoch 18127: train loss: 0.010520223528146744\n",
            "Epoch 18128: train loss: 0.010519335977733135\n",
            "Epoch 18129: train loss: 0.01051845122128725\n",
            "Epoch 18130: train loss: 0.010517566464841366\n",
            "Epoch 18131: train loss: 0.010516681708395481\n",
            "Epoch 18132: train loss: 0.010515796020627022\n",
            "Epoch 18133: train loss: 0.010514908470213413\n",
            "Epoch 18134: train loss: 0.010514028370380402\n",
            "Epoch 18135: train loss: 0.010513143613934517\n",
            "Epoch 18136: train loss: 0.010512261651456356\n",
            "Epoch 18137: train loss: 0.010511372238397598\n",
            "Epoch 18138: train loss: 0.010510489344596863\n",
            "Epoch 18139: train loss: 0.010509606450796127\n",
            "Epoch 18140: train loss: 0.010508726350963116\n",
            "Epoch 18141: train loss: 0.010507836937904358\n",
            "Epoch 18142: train loss: 0.010506958700716496\n",
            "Epoch 18143: train loss: 0.010506071150302887\n",
            "Epoch 18144: train loss: 0.010505189187824726\n",
            "Epoch 18145: train loss: 0.01050430629402399\n",
            "Epoch 18146: train loss: 0.01050342246890068\n",
            "Epoch 18147: train loss: 0.010502541437745094\n",
            "Epoch 18148: train loss: 0.010501652956008911\n",
            "Epoch 18149: train loss: 0.0105007728561759\n",
            "Epoch 18150: train loss: 0.010499890893697739\n",
            "Epoch 18151: train loss: 0.010499010793864727\n",
            "Epoch 18152: train loss: 0.010498123243451118\n",
            "Epoch 18153: train loss: 0.01049724780023098\n",
            "Epoch 18154: train loss: 0.010496359318494797\n",
            "Epoch 18155: train loss: 0.010495477356016636\n",
            "Epoch 18156: train loss: 0.010494598187506199\n",
            "Epoch 18157: train loss: 0.01049371249973774\n",
            "Epoch 18158: train loss: 0.010492824949324131\n",
            "Epoch 18159: train loss: 0.010491949506103992\n",
            "Epoch 18160: train loss: 0.010491062887012959\n",
            "Epoch 18161: train loss: 0.010490186512470245\n",
            "Epoch 18162: train loss: 0.010489300824701786\n",
            "Epoch 18163: train loss: 0.010488422587513924\n",
            "Epoch 18164: train loss: 0.010487540625035763\n",
            "Epoch 18165: train loss: 0.010486659593880177\n",
            "Epoch 18166: train loss: 0.010485777631402016\n",
            "Epoch 18167: train loss: 0.01048489660024643\n",
            "Epoch 18168: train loss: 0.010484007187187672\n",
            "Epoch 18169: train loss: 0.010483135469257832\n",
            "Epoch 18170: train loss: 0.01048225350677967\n",
            "Epoch 18171: train loss: 0.010481374338269234\n",
            "Epoch 18172: train loss: 0.010480488650500774\n",
            "Epoch 18173: train loss: 0.010479602962732315\n",
            "Epoch 18174: train loss: 0.010478728450834751\n",
            "Epoch 18175: train loss: 0.010477849282324314\n",
            "Epoch 18176: train loss: 0.010476969182491302\n",
            "Epoch 18177: train loss: 0.010476087220013142\n",
            "Epoch 18178: train loss: 0.010475210845470428\n",
            "Epoch 18179: train loss: 0.010474328882992268\n",
            "Epoch 18180: train loss: 0.010473458096385002\n",
            "Epoch 18181: train loss: 0.010472574271261692\n",
            "Epoch 18182: train loss: 0.010471695102751255\n",
            "Epoch 18183: train loss: 0.010470807552337646\n",
            "Epoch 18184: train loss: 0.010469936765730381\n",
            "Epoch 18185: train loss: 0.010469062253832817\n",
            "Epoch 18186: train loss: 0.010468182153999805\n",
            "Epoch 18187: train loss: 0.01046730112284422\n",
            "Epoch 18188: train loss: 0.010466426610946655\n",
            "Epoch 18189: train loss: 0.010465551167726517\n",
            "Epoch 18190: train loss: 0.010464674793183804\n",
            "Epoch 18191: train loss: 0.010463801212608814\n",
            "Epoch 18192: train loss: 0.01046291459351778\n",
            "Epoch 18193: train loss: 0.010462041012942791\n",
            "Epoch 18194: train loss: 0.010461161844432354\n",
            "Epoch 18195: train loss: 0.010460283607244492\n",
            "Epoch 18196: train loss: 0.010459409095346928\n",
            "Epoch 18197: train loss: 0.01045853178948164\n",
            "Epoch 18198: train loss: 0.01045765820890665\n",
            "Epoch 18199: train loss: 0.010456782765686512\n",
            "Epoch 18200: train loss: 0.010455906391143799\n",
            "Epoch 18201: train loss: 0.010455031879246235\n",
            "Epoch 18202: train loss: 0.010454153642058372\n",
            "Epoch 18203: train loss: 0.010453279130160809\n",
            "Epoch 18204: train loss: 0.01045240554958582\n",
            "Epoch 18205: train loss: 0.01045153196901083\n",
            "Epoch 18206: train loss: 0.010450653731822968\n",
            "Epoch 18207: train loss: 0.010449782013893127\n",
            "Epoch 18208: train loss: 0.010448909364640713\n",
            "Epoch 18209: train loss: 0.010448030196130276\n",
            "Epoch 18210: train loss: 0.010447155684232712\n",
            "Epoch 18211: train loss: 0.010446281172335148\n",
            "Epoch 18212: train loss: 0.01044540572911501\n",
            "Epoch 18213: train loss: 0.01044453214854002\n",
            "Epoch 18214: train loss: 0.010443657636642456\n",
            "Epoch 18215: train loss: 0.010442790575325489\n",
            "Epoch 18216: train loss: 0.01044191513210535\n",
            "Epoch 18217: train loss: 0.010441047139465809\n",
            "Epoch 18218: train loss: 0.01044017169624567\n",
            "Epoch 18219: train loss: 0.010439298115670681\n",
            "Epoch 18220: train loss: 0.010438422672450542\n",
            "Epoch 18221: train loss: 0.010437553748488426\n",
            "Epoch 18222: train loss: 0.010436676442623138\n",
            "Epoch 18223: train loss: 0.01043579913675785\n",
            "Epoch 18224: train loss: 0.01043492741882801\n",
            "Epoch 18225: train loss: 0.010434059426188469\n",
            "Epoch 18226: train loss: 0.010433184914290905\n",
            "Epoch 18227: train loss: 0.010432315990328789\n",
            "Epoch 18228: train loss: 0.010431439615786076\n",
            "Epoch 18229: train loss: 0.010430560447275639\n",
            "Epoch 18230: train loss: 0.01042968686670065\n",
            "Epoch 18231: train loss: 0.010428817942738533\n",
            "Epoch 18232: train loss: 0.010427947156131268\n",
            "Epoch 18233: train loss: 0.010427073575556278\n",
            "Epoch 18234: train loss: 0.01042620837688446\n",
            "Epoch 18235: train loss: 0.010425332002341747\n",
            "Epoch 18236: train loss: 0.010424459353089333\n",
            "Epoch 18237: train loss: 0.010423590429127216\n",
            "Epoch 18238: train loss: 0.010422720573842525\n",
            "Epoch 18239: train loss: 0.010421845130622387\n",
            "Epoch 18240: train loss: 0.010420973412692547\n",
            "Epoch 18241: train loss: 0.010420103557407856\n",
            "Epoch 18242: train loss: 0.01041923463344574\n",
            "Epoch 18243: train loss: 0.010418365709483624\n",
            "Epoch 18244: train loss: 0.010417496785521507\n",
            "Epoch 18245: train loss: 0.010416625067591667\n",
            "Epoch 18246: train loss: 0.010415750555694103\n",
            "Epoch 18247: train loss: 0.01041488442569971\n",
            "Epoch 18248: train loss: 0.010414020158350468\n",
            "Epoch 18249: train loss: 0.010413149371743202\n",
            "Epoch 18250: train loss: 0.010412286035716534\n",
            "Epoch 18251: train loss: 0.010411410592496395\n",
            "Epoch 18252: train loss: 0.010410548187792301\n",
            "Epoch 18253: train loss: 0.01040967833250761\n",
            "Epoch 18254: train loss: 0.01040880847722292\n",
            "Epoch 18255: train loss: 0.010407943278551102\n",
            "Epoch 18256: train loss: 0.010407076217234135\n",
            "Epoch 18257: train loss: 0.010406206361949444\n",
            "Epoch 18258: train loss: 0.010405340231955051\n",
            "Epoch 18259: train loss: 0.010404466651380062\n",
            "Epoch 18260: train loss: 0.010403606109321117\n",
            "Epoch 18261: train loss: 0.010402739979326725\n",
            "Epoch 18262: train loss: 0.010401878505945206\n",
            "Epoch 18263: train loss: 0.010401006788015366\n",
            "Epoch 18264: train loss: 0.010400145314633846\n",
            "Epoch 18265: train loss: 0.01039927639067173\n",
            "Epoch 18266: train loss: 0.010398403741419315\n",
            "Epoch 18267: train loss: 0.010397541336715221\n",
            "Epoch 18268: train loss: 0.010396679863333702\n",
            "Epoch 18269: train loss: 0.010395810008049011\n",
            "Epoch 18270: train loss: 0.010394941084086895\n",
            "Epoch 18271: train loss: 0.010394076816737652\n",
            "Epoch 18272: train loss: 0.01039321068674326\n",
            "Epoch 18273: train loss: 0.010392345488071442\n",
            "Epoch 18274: train loss: 0.01039147563278675\n",
            "Epoch 18275: train loss: 0.010390615090727806\n",
            "Epoch 18276: train loss: 0.010389742441475391\n",
            "Epoch 18277: train loss: 0.010388885624706745\n",
            "Epoch 18278: train loss: 0.010388019494712353\n",
            "Epoch 18279: train loss: 0.01038715336471796\n",
            "Epoch 18280: train loss: 0.010386291891336441\n",
            "Epoch 18281: train loss: 0.010385433211922646\n",
            "Epoch 18282: train loss: 0.010384568013250828\n",
            "Epoch 18283: train loss: 0.010383701883256435\n",
            "Epoch 18284: train loss: 0.010382840409874916\n",
            "Epoch 18285: train loss: 0.010381979867815971\n",
            "Epoch 18286: train loss: 0.010381112806499004\n",
            "Epoch 18287: train loss: 0.010380249470472336\n",
            "Epoch 18288: train loss: 0.010379391722381115\n",
            "Epoch 18289: train loss: 0.010378527455031872\n",
            "Epoch 18290: train loss: 0.010377665981650352\n",
            "Epoch 18291: train loss: 0.010376800782978535\n",
            "Epoch 18292: train loss: 0.010375933721661568\n",
            "Epoch 18293: train loss: 0.01037507876753807\n",
            "Epoch 18294: train loss: 0.010374213568866253\n",
            "Epoch 18295: train loss: 0.010373353958129883\n",
            "Epoch 18296: train loss: 0.010372490622103214\n",
            "Epoch 18297: train loss: 0.010371625423431396\n",
            "Epoch 18298: train loss: 0.010370767675340176\n",
            "Epoch 18299: train loss: 0.010369902476668358\n",
            "Epoch 18300: train loss: 0.010369040071964264\n",
            "Epoch 18301: train loss: 0.010368181392550468\n",
            "Epoch 18302: train loss: 0.010367317125201225\n",
            "Epoch 18303: train loss: 0.010366450995206833\n",
            "Epoch 18304: train loss: 0.010365596041083336\n",
            "Epoch 18305: train loss: 0.010364729911088943\n",
            "Epoch 18306: train loss: 0.010363868437707424\n",
            "Epoch 18307: train loss: 0.010363004170358181\n",
            "Epoch 18308: train loss: 0.01036214828491211\n",
            "Epoch 18309: train loss: 0.01036128681153059\n",
            "Epoch 18310: train loss: 0.010360423475503922\n",
            "Epoch 18311: train loss: 0.010359566658735275\n",
            "Epoch 18312: train loss: 0.01035870611667633\n",
            "Epoch 18313: train loss: 0.01035784650593996\n",
            "Epoch 18314: train loss: 0.010356985963881016\n",
            "Epoch 18315: train loss: 0.010356119833886623\n",
            "Epoch 18316: train loss: 0.010355262085795403\n",
            "Epoch 18317: train loss: 0.010354403406381607\n",
            "Epoch 18318: train loss: 0.010353547520935535\n",
            "Epoch 18319: train loss: 0.010352685116231441\n",
            "Epoch 18320: train loss: 0.010351826436817646\n",
            "Epoch 18321: train loss: 0.010350964963436127\n",
            "Epoch 18322: train loss: 0.01035011000931263\n",
            "Epoch 18323: train loss: 0.01034925039857626\n",
            "Epoch 18324: train loss: 0.010348392650485039\n",
            "Epoch 18325: train loss: 0.010347533971071243\n",
            "Epoch 18326: train loss: 0.010346678085625172\n",
            "Epoch 18327: train loss: 0.010345818474888802\n",
            "Epoch 18328: train loss: 0.010344958864152431\n",
            "Epoch 18329: train loss: 0.010344098322093487\n",
            "Epoch 18330: train loss: 0.010343249887228012\n",
            "Epoch 18331: train loss: 0.01034238375723362\n",
            "Epoch 18332: train loss: 0.010341527871787548\n",
            "Epoch 18333: train loss: 0.010340671055018902\n",
            "Epoch 18334: train loss: 0.010339811444282532\n",
            "Epoch 18335: train loss: 0.010338958352804184\n",
            "Epoch 18336: train loss: 0.010338098742067814\n",
            "Epoch 18337: train loss: 0.010337239131331444\n",
            "Epoch 18338: train loss: 0.010336382314562798\n",
            "Epoch 18339: train loss: 0.0103355273604393\n",
            "Epoch 18340: train loss: 0.01033466774970293\n",
            "Epoch 18341: train loss: 0.01033382024616003\n",
            "Epoch 18342: train loss: 0.010332964360713959\n",
            "Epoch 18343: train loss: 0.010332104749977589\n",
            "Epoch 18344: train loss: 0.010331248864531517\n",
            "Epoch 18345: train loss: 0.01033039204776287\n",
            "Epoch 18346: train loss: 0.010329538024961948\n",
            "Epoch 18347: train loss: 0.01032868679612875\n",
            "Epoch 18348: train loss: 0.010327829048037529\n",
            "Epoch 18349: train loss: 0.010326972231268883\n",
            "Epoch 18350: train loss: 0.010326117277145386\n",
            "Epoch 18351: train loss: 0.010325264185667038\n",
            "Epoch 18352: train loss: 0.010324409231543541\n",
            "Epoch 18353: train loss: 0.010323556140065193\n",
            "Epoch 18354: train loss: 0.010322698391973972\n",
            "Epoch 18355: train loss: 0.010321848094463348\n",
            "Epoch 18356: train loss: 0.010320994071662426\n",
            "Epoch 18357: train loss: 0.010320139117538929\n",
            "Epoch 18358: train loss: 0.010319286026060581\n",
            "Epoch 18359: train loss: 0.010318431071937084\n",
            "Epoch 18360: train loss: 0.010317575186491013\n",
            "Epoch 18361: train loss: 0.01031672302633524\n",
            "Epoch 18362: train loss: 0.010315870866179466\n",
            "Epoch 18363: train loss: 0.010315023362636566\n",
            "Epoch 18364: train loss: 0.010314173996448517\n",
            "Epoch 18365: train loss: 0.010313320904970169\n",
            "Epoch 18366: train loss: 0.010312467813491821\n",
            "Epoch 18367: train loss: 0.010311612859368324\n",
            "Epoch 18368: train loss: 0.010310758836567402\n",
            "Epoch 18369: train loss: 0.010309909470379353\n",
            "Epoch 18370: train loss: 0.010309062898159027\n",
            "Epoch 18371: train loss: 0.010308207012712955\n",
            "Epoch 18372: train loss: 0.010307354852557182\n",
            "Epoch 18373: train loss: 0.010306501761078835\n",
            "Epoch 18374: train loss: 0.010305652394890785\n",
            "Epoch 18375: train loss: 0.010304798372089863\n",
            "Epoch 18376: train loss: 0.010303950868546963\n",
            "Epoch 18377: train loss: 0.010303100571036339\n",
            "Epoch 18378: train loss: 0.010302246548235416\n",
            "Epoch 18379: train loss: 0.010301394388079643\n",
            "Epoch 18380: train loss: 0.010300541296601295\n",
            "Epoch 18381: train loss: 0.01029969286173582\n",
            "Epoch 18382: train loss: 0.010298838838934898\n",
            "Epoch 18383: train loss: 0.010297993198037148\n",
            "Epoch 18384: train loss: 0.010297141037881374\n",
            "Epoch 18385: train loss: 0.010296291671693325\n",
            "Epoch 18386: train loss: 0.010295434854924679\n",
            "Epoch 18387: train loss: 0.010294593870639801\n",
            "Epoch 18388: train loss: 0.010293740779161453\n",
            "Epoch 18389: train loss: 0.010292896069586277\n",
            "Epoch 18390: train loss: 0.010292044840753078\n",
            "Epoch 18391: train loss: 0.010291201993823051\n",
            "Epoch 18392: train loss: 0.010290351696312428\n",
            "Epoch 18393: train loss: 0.010289500467479229\n",
            "Epoch 18394: train loss: 0.01028865110129118\n",
            "Epoch 18395: train loss: 0.010287808254361153\n",
            "Epoch 18396: train loss: 0.010286958888173103\n",
            "Epoch 18397: train loss: 0.01028610672801733\n",
            "Epoch 18398: train loss: 0.010285260155797005\n",
            "Epoch 18399: train loss: 0.010284416377544403\n",
            "Epoch 18400: train loss: 0.010283572599291801\n",
            "Epoch 18401: train loss: 0.010282721370458603\n",
            "Epoch 18402: train loss: 0.010281872004270554\n",
            "Epoch 18403: train loss: 0.010281029157340527\n",
            "Epoch 18404: train loss: 0.010280183516442776\n",
            "Epoch 18405: train loss: 0.010279328562319279\n",
            "Epoch 18406: train loss: 0.010278487578034401\n",
            "Epoch 18407: train loss: 0.010277636349201202\n",
            "Epoch 18408: train loss: 0.010276793502271175\n",
            "Epoch 18409: train loss: 0.010275944136083126\n",
            "Epoch 18410: train loss: 0.010275095701217651\n",
            "Epoch 18411: train loss: 0.010274252854287624\n",
            "Epoch 18412: train loss: 0.010273408144712448\n",
            "Epoch 18413: train loss: 0.010272561572492123\n",
            "Epoch 18414: train loss: 0.010271713137626648\n",
            "Epoch 18415: train loss: 0.010270871222019196\n",
            "Epoch 18416: train loss: 0.010270019993185997\n",
            "Epoch 18417: train loss: 0.01026917714625597\n",
            "Epoch 18418: train loss: 0.01026833150535822\n",
            "Epoch 18419: train loss: 0.010267491452395916\n",
            "Epoch 18420: train loss: 0.010266638360917568\n",
            "Epoch 18421: train loss: 0.010265800170600414\n",
            "Epoch 18422: train loss: 0.010264948010444641\n",
            "Epoch 18423: train loss: 0.010264107957482338\n",
            "Epoch 18424: train loss: 0.010263260453939438\n",
            "Epoch 18425: train loss: 0.010262418538331985\n",
            "Epoch 18426: train loss: 0.010261566378176212\n",
            "Epoch 18427: train loss: 0.010260717011988163\n",
            "Epoch 18428: train loss: 0.01025987509638071\n",
            "Epoch 18429: train loss: 0.010259034112095833\n",
            "Epoch 18430: train loss: 0.010258187539875507\n",
            "Epoch 18431: train loss: 0.010257342830300331\n",
            "Epoch 18432: train loss: 0.010256499983370304\n",
            "Epoch 18433: train loss: 0.010255658067762852\n",
            "Epoch 18434: train loss: 0.010254810564219952\n",
            "Epoch 18435: train loss: 0.010253967717289925\n",
            "Epoch 18436: train loss: 0.010253120213747025\n",
            "Epoch 18437: train loss: 0.010252274572849274\n",
            "Epoch 18438: train loss: 0.01025143451988697\n",
            "Epoch 18439: train loss: 0.010250591672956944\n",
            "Epoch 18440: train loss: 0.010249747894704342\n",
            "Epoch 18441: train loss: 0.010248907841742039\n",
            "Epoch 18442: train loss: 0.010248066857457161\n",
            "Epoch 18443: train loss: 0.01024722307920456\n",
            "Epoch 18444: train loss: 0.010246378369629383\n",
            "Epoch 18445: train loss: 0.010245542973279953\n",
            "Epoch 18446: train loss: 0.010244695469737053\n",
            "Epoch 18447: train loss: 0.01024385541677475\n",
            "Epoch 18448: train loss: 0.010243012569844723\n",
            "Epoch 18449: train loss: 0.01024217065423727\n",
            "Epoch 18450: train loss: 0.010241331532597542\n",
            "Epoch 18451: train loss: 0.010240490548312664\n",
            "Epoch 18452: train loss: 0.010239650495350361\n",
            "Epoch 18453: train loss: 0.010238810442388058\n",
            "Epoch 18454: train loss: 0.01023797132074833\n",
            "Epoch 18455: train loss: 0.010237128473818302\n",
            "Epoch 18456: train loss: 0.010236288420855999\n",
            "Epoch 18457: train loss: 0.010235448367893696\n",
            "Epoch 18458: train loss: 0.010234612040221691\n",
            "Epoch 18459: train loss: 0.010233779437839985\n",
            "Epoch 18460: train loss: 0.010232941247522831\n",
            "Epoch 18461: train loss: 0.010232101194560528\n",
            "Epoch 18462: train loss: 0.010231264866888523\n",
            "Epoch 18463: train loss: 0.010230422019958496\n",
            "Epoch 18464: train loss: 0.010229586623609066\n",
            "Epoch 18465: train loss: 0.01022874005138874\n",
            "Epoch 18466: train loss: 0.010227905586361885\n",
            "Epoch 18467: train loss: 0.010227072052657604\n",
            "Epoch 18468: train loss: 0.0102262357249856\n",
            "Epoch 18469: train loss: 0.0102253882214427\n",
            "Epoch 18470: train loss: 0.010224556550383568\n",
            "Epoch 18471: train loss: 0.010223719291388988\n",
            "Epoch 18472: train loss: 0.010222883895039558\n",
            "Epoch 18473: train loss: 0.010222043842077255\n",
            "Epoch 18474: train loss: 0.010221211239695549\n",
            "Epoch 18475: train loss: 0.010220374912023544\n",
            "Epoch 18476: train loss: 0.010219542309641838\n",
            "Epoch 18477: train loss: 0.010218700394034386\n",
            "Epoch 18478: train loss: 0.010217870585620403\n",
            "Epoch 18479: train loss: 0.010217024944722652\n",
            "Epoch 18480: train loss: 0.01021619327366352\n",
            "Epoch 18481: train loss: 0.010215356945991516\n",
            "Epoch 18482: train loss: 0.010214518755674362\n",
            "Epoch 18483: train loss: 0.010213681496679783\n",
            "Epoch 18484: train loss: 0.010212848894298077\n",
            "Epoch 18485: train loss: 0.010212014429271221\n",
            "Epoch 18486: train loss: 0.010211175307631493\n",
            "Epoch 18487: train loss: 0.010210338048636913\n",
            "Epoch 18488: train loss: 0.010209504514932632\n",
            "Epoch 18489: train loss: 0.010208670049905777\n",
            "Epoch 18490: train loss: 0.010207832790911198\n",
            "Epoch 18491: train loss: 0.01020700205117464\n",
            "Epoch 18492: train loss: 0.010206162929534912\n",
            "Epoch 18493: train loss: 0.010205323807895184\n",
            "Epoch 18494: train loss: 0.010204489342868328\n",
            "Epoch 18495: train loss: 0.010203657671809196\n",
            "Epoch 18496: train loss: 0.010202821344137192\n",
            "Epoch 18497: train loss: 0.010201987810432911\n",
            "Epoch 18498: train loss: 0.010201153345406055\n",
            "Epoch 18499: train loss: 0.010200317949056625\n",
            "Epoch 18500: train loss: 0.010199476964771748\n",
            "Epoch 18501: train loss: 0.010198650881648064\n",
            "Epoch 18502: train loss: 0.01019781082868576\n",
            "Epoch 18503: train loss: 0.01019697729498148\n",
            "Epoch 18504: train loss: 0.010196144692599773\n",
            "Epoch 18505: train loss: 0.010195307433605194\n",
            "Epoch 18506: train loss: 0.010194473899900913\n",
            "Epoch 18507: train loss: 0.010193638503551483\n",
            "Epoch 18508: train loss: 0.010192801244556904\n",
            "Epoch 18509: train loss: 0.010191968642175198\n",
            "Epoch 18510: train loss: 0.010191135108470917\n",
            "Epoch 18511: train loss: 0.010190300643444061\n",
            "Epoch 18512: train loss: 0.010189465247094631\n",
            "Epoch 18513: train loss: 0.0101886335760355\n",
            "Epoch 18514: train loss: 0.010187802836298943\n",
            "Epoch 18515: train loss: 0.010186964645981789\n",
            "Epoch 18516: train loss: 0.010186132974922657\n",
            "Epoch 18517: train loss: 0.01018529199063778\n",
            "Epoch 18518: train loss: 0.010184465907514095\n",
            "Epoch 18519: train loss: 0.010183625854551792\n",
            "Epoch 18520: train loss: 0.010182801634073257\n",
            "Epoch 18521: train loss: 0.010181968100368977\n",
            "Epoch 18522: train loss: 0.010181130841374397\n",
            "Epoch 18523: train loss: 0.010180302895605564\n",
            "Epoch 18524: train loss: 0.010179470293223858\n",
            "Epoch 18525: train loss: 0.010178635828197002\n",
            "Epoch 18526: train loss: 0.010177806951105595\n",
            "Epoch 18527: train loss: 0.010176978074014187\n",
            "Epoch 18528: train loss: 0.01017614733427763\n",
            "Epoch 18529: train loss: 0.010175319388508797\n",
            "Epoch 18530: train loss: 0.010174487717449665\n",
            "Epoch 18531: train loss: 0.010173658840358257\n",
            "Epoch 18532: train loss: 0.010172825306653976\n",
            "Epoch 18533: train loss: 0.010171991772949696\n",
            "Epoch 18534: train loss: 0.010171168483793736\n",
            "Epoch 18535: train loss: 0.010170342400670052\n",
            "Epoch 18536: train loss: 0.010169516317546368\n",
            "Epoch 18537: train loss: 0.010168680921196938\n",
            "Epoch 18538: train loss: 0.010167854838073254\n",
            "Epoch 18539: train loss: 0.010167023167014122\n",
            "Epoch 18540: train loss: 0.010166197083890438\n",
            "Epoch 18541: train loss: 0.010165371000766754\n",
            "Epoch 18542: train loss: 0.010164542123675346\n",
            "Epoch 18543: train loss: 0.01016371138393879\n",
            "Epoch 18544: train loss: 0.010162885300815105\n",
            "Epoch 18545: train loss: 0.010162058286368847\n",
            "Epoch 18546: train loss: 0.010161230340600014\n",
            "Epoch 18547: train loss: 0.010160407051444054\n",
            "Epoch 18548: train loss: 0.010159578174352646\n",
            "Epoch 18549: train loss: 0.010158747434616089\n",
            "Epoch 18550: train loss: 0.010157921351492405\n",
            "Epoch 18551: train loss: 0.010157094337046146\n",
            "Epoch 18552: train loss: 0.010156270116567612\n",
            "Epoch 18553: train loss: 0.010155441239476204\n",
            "Epoch 18554: train loss: 0.010154616087675095\n",
            "Epoch 18555: train loss: 0.010153787210583687\n",
            "Epoch 18556: train loss: 0.010152969509363174\n",
            "Epoch 18557: train loss: 0.010152142494916916\n",
            "Epoch 18558: train loss: 0.010151315480470657\n",
            "Epoch 18559: train loss: 0.010150487534701824\n",
            "Epoch 18560: train loss: 0.010149667039513588\n",
            "Epoch 18561: train loss: 0.010148844681680202\n",
            "Epoch 18562: train loss: 0.01014801301062107\n",
            "Epoch 18563: train loss: 0.010147184133529663\n",
            "Epoch 18564: train loss: 0.010146359913051128\n",
            "Epoch 18565: train loss: 0.01014553289860487\n",
            "Epoch 18566: train loss: 0.010144705884158611\n",
            "Epoch 18567: train loss: 0.0101438844576478\n",
            "Epoch 18568: train loss: 0.010143059305846691\n",
            "Epoch 18569: train loss: 0.010142235085368156\n",
            "Epoch 18570: train loss: 0.010141409933567047\n",
            "Epoch 18571: train loss: 0.010140589438378811\n",
            "Epoch 18572: train loss: 0.01013976614922285\n",
            "Epoch 18573: train loss: 0.010138939134776592\n",
            "Epoch 18574: train loss: 0.010138111189007759\n",
            "Epoch 18575: train loss: 0.010137290693819523\n",
            "Epoch 18576: train loss: 0.010136461816728115\n",
            "Epoch 18577: train loss: 0.01013563759624958\n",
            "Epoch 18578: train loss: 0.010134813375771046\n",
            "Epoch 18579: train loss: 0.010133987292647362\n",
            "Epoch 18580: train loss: 0.010133166797459126\n",
            "Epoch 18581: train loss: 0.01013234630227089\n",
            "Epoch 18582: train loss: 0.010131516493856907\n",
            "Epoch 18583: train loss: 0.010130693204700947\n",
            "Epoch 18584: train loss: 0.01012987457215786\n",
            "Epoch 18585: train loss: 0.01012904942035675\n",
            "Epoch 18586: train loss: 0.010128219611942768\n",
            "Epoch 18587: train loss: 0.010127397254109383\n",
            "Epoch 18588: train loss: 0.010126580484211445\n",
            "Epoch 18589: train loss: 0.010125753469765186\n",
            "Epoch 18590: train loss: 0.010124927386641502\n",
            "Epoch 18591: train loss: 0.010124108754098415\n",
            "Epoch 18592: train loss: 0.010123283602297306\n",
            "Epoch 18593: train loss: 0.010122464038431644\n",
            "Epoch 18594: train loss: 0.010121635161340237\n",
            "Epoch 18595: train loss: 0.010120815597474575\n",
            "Epoch 18596: train loss: 0.010119989514350891\n",
            "Epoch 18597: train loss: 0.010119160637259483\n",
            "Epoch 18598: train loss: 0.010118348523974419\n",
            "Epoch 18599: train loss: 0.01011752337217331\n",
            "Epoch 18600: train loss: 0.010116701945662498\n",
            "Epoch 18601: train loss: 0.010115883313119411\n",
            "Epoch 18602: train loss: 0.010115062817931175\n",
            "Epoch 18603: train loss: 0.010114246048033237\n",
            "Epoch 18604: train loss: 0.010113419964909554\n",
            "Epoch 18605: train loss: 0.01011260598897934\n",
            "Epoch 18606: train loss: 0.010111786425113678\n",
            "Epoch 18607: train loss: 0.010110967792570591\n",
            "Epoch 18608: train loss: 0.010110147297382355\n",
            "Epoch 18609: train loss: 0.010109329596161842\n",
            "Epoch 18610: train loss: 0.010108508169651031\n",
            "Epoch 18611: train loss: 0.010107694193720818\n",
            "Epoch 18612: train loss: 0.010106872767210007\n",
            "Epoch 18613: train loss: 0.010106056928634644\n",
            "Epoch 18614: train loss: 0.010105238296091557\n",
            "Epoch 18615: train loss: 0.010104425251483917\n",
            "Epoch 18616: train loss: 0.010103602893650532\n",
            "Epoch 18617: train loss: 0.010102786123752594\n",
            "Epoch 18618: train loss: 0.010101965628564358\n",
            "Epoch 18619: train loss: 0.010101147927343845\n",
            "Epoch 18620: train loss: 0.010100334882736206\n",
            "Epoch 18621: train loss: 0.01009951252490282\n",
            "Epoch 18622: train loss: 0.010098692029714584\n",
            "Epoch 18623: train loss: 0.010097880847752094\n",
            "Epoch 18624: train loss: 0.010097058489918709\n",
            "Epoch 18625: train loss: 0.010096241720020771\n",
            "Epoch 18626: train loss: 0.010095427744090557\n",
            "Epoch 18627: train loss: 0.01009460911154747\n",
            "Epoch 18628: train loss: 0.010093791410326958\n",
            "Epoch 18629: train loss: 0.010092970915138721\n",
            "Epoch 18630: train loss: 0.01009216159582138\n",
            "Epoch 18631: train loss: 0.010091341100633144\n",
            "Epoch 18632: train loss: 0.010090523399412632\n",
            "Epoch 18633: train loss: 0.010089709423482418\n",
            "Epoch 18634: train loss: 0.010088893584907055\n",
            "Epoch 18635: train loss: 0.010088076815009117\n",
            "Epoch 18636: train loss: 0.01008726004511118\n",
            "Epoch 18637: train loss: 0.010086446069180965\n",
            "Epoch 18638: train loss: 0.010085630230605602\n",
            "Epoch 18639: train loss: 0.01008481066673994\n",
            "Epoch 18640: train loss: 0.010083992034196854\n",
            "Epoch 18641: train loss: 0.01008317619562149\n",
            "Epoch 18642: train loss: 0.010082368738949299\n",
            "Epoch 18643: train loss: 0.010081552900373936\n",
            "Epoch 18644: train loss: 0.010080728679895401\n",
            "Epoch 18645: train loss: 0.010079914703965187\n",
            "Epoch 18646: train loss: 0.010079104453325272\n",
            "Epoch 18647: train loss: 0.010078283958137035\n",
            "Epoch 18648: train loss: 0.010077464394271374\n",
            "Epoch 18649: train loss: 0.010076651349663734\n",
            "Epoch 18650: train loss: 0.01007583737373352\n",
            "Epoch 18651: train loss: 0.010075022466480732\n",
            "Epoch 18652: train loss: 0.010074208490550518\n",
            "Epoch 18653: train loss: 0.01007339172065258\n",
            "Epoch 18654: train loss: 0.010072579607367516\n",
            "Epoch 18655: train loss: 0.010071761906147003\n",
            "Epoch 18656: train loss: 0.010070948861539364\n",
            "Epoch 18657: train loss: 0.010070135816931725\n",
            "Epoch 18658: train loss: 0.01006932370364666\n",
            "Epoch 18659: train loss: 0.010068506002426147\n",
            "Epoch 18660: train loss: 0.010067691095173359\n",
            "Epoch 18661: train loss: 0.010066877119243145\n",
            "Epoch 18662: train loss: 0.010066063143312931\n",
            "Epoch 18663: train loss: 0.010065249167382717\n",
            "Epoch 18664: train loss: 0.010064436122775078\n",
            "Epoch 18665: train loss: 0.010063620284199715\n",
            "Epoch 18666: train loss: 0.010062807239592075\n",
            "Epoch 18667: train loss: 0.010061991401016712\n",
            "Epoch 18668: train loss: 0.01006118580698967\n",
            "Epoch 18669: train loss: 0.010060369037091732\n",
            "Epoch 18670: train loss: 0.01005955133587122\n",
            "Epoch 18671: train loss: 0.010058742016553879\n",
            "Epoch 18672: train loss: 0.010057932697236538\n",
            "Epoch 18673: train loss: 0.010057123377919197\n",
            "Epoch 18674: train loss: 0.010056311264634132\n",
            "Epoch 18675: train loss: 0.01005550567060709\n",
            "Epoch 18676: train loss: 0.010054691694676876\n",
            "Epoch 18677: train loss: 0.010053876787424088\n",
            "Epoch 18678: train loss: 0.010053068399429321\n",
            "Epoch 18679: train loss: 0.01005225908011198\n",
            "Epoch 18680: train loss: 0.010051448829472065\n",
            "Epoch 18681: train loss: 0.010050636716187\n",
            "Epoch 18682: train loss: 0.010049829259514809\n",
            "Epoch 18683: train loss: 0.010049013420939445\n",
            "Epoch 18684: train loss: 0.010048207826912403\n",
            "Epoch 18685: train loss: 0.010047398507595062\n",
            "Epoch 18686: train loss: 0.010046583600342274\n",
            "Epoch 18687: train loss: 0.010045776143670082\n",
            "Epoch 18688: train loss: 0.010044971480965614\n",
            "Epoch 18689: train loss: 0.010044164955615997\n",
            "Epoch 18690: train loss: 0.010043354704976082\n",
            "Epoch 18691: train loss: 0.010042548179626465\n",
            "Epoch 18692: train loss: 0.010041744448244572\n",
            "Epoch 18693: train loss: 0.010040929540991783\n",
            "Epoch 18694: train loss: 0.010040123015642166\n",
            "Epoch 18695: train loss: 0.010039319284260273\n",
            "Epoch 18696: train loss: 0.010038509033620358\n",
            "Epoch 18697: train loss: 0.01003770250827074\n",
            "Epoch 18698: train loss: 0.010036895982921124\n",
            "Epoch 18699: train loss: 0.010036088526248932\n",
            "Epoch 18700: train loss: 0.01003528106957674\n",
            "Epoch 18701: train loss: 0.010034477338194847\n",
            "Epoch 18702: train loss: 0.010033668950200081\n",
            "Epoch 18703: train loss: 0.010032856836915016\n",
            "Epoch 18704: train loss: 0.010032053105533123\n",
            "Epoch 18705: train loss: 0.010031244717538357\n",
            "Epoch 18706: train loss: 0.010030441917479038\n",
            "Epoch 18707: train loss: 0.010029636323451996\n",
            "Epoch 18708: train loss: 0.01002882793545723\n",
            "Epoch 18709: train loss: 0.010028018616139889\n",
            "Epoch 18710: train loss: 0.010027213022112846\n",
            "Epoch 18711: train loss: 0.010026402771472931\n",
            "Epoch 18712: train loss: 0.010025600902736187\n",
            "Epoch 18713: train loss: 0.010024793446063995\n",
            "Epoch 18714: train loss: 0.010023988783359528\n",
            "Epoch 18715: train loss: 0.01002318225800991\n",
            "Epoch 18716: train loss: 0.010022372007369995\n",
            "Epoch 18717: train loss: 0.010021570138633251\n",
            "Epoch 18718: train loss: 0.010020763613283634\n",
            "Epoch 18719: train loss: 0.010019958950579166\n",
            "Epoch 18720: train loss: 0.010019149631261826\n",
            "Epoch 18721: train loss: 0.010018345899879932\n",
            "Epoch 18722: train loss: 0.010017535649240017\n",
            "Epoch 18723: train loss: 0.010016735643148422\n",
            "Epoch 18724: train loss: 0.010015927255153656\n",
            "Epoch 18725: train loss: 0.010015121661126614\n",
            "Epoch 18726: train loss: 0.010014320723712444\n",
            "Epoch 18727: train loss: 0.010013515129685402\n",
            "Epoch 18728: train loss: 0.010012708604335785\n",
            "Epoch 18729: train loss: 0.010011900216341019\n",
            "Epoch 18730: train loss: 0.010011094622313976\n",
            "Epoch 18731: train loss: 0.010010292753577232\n",
            "Epoch 18732: train loss: 0.01000948715955019\n",
            "Epoch 18733: train loss: 0.01000868622213602\n",
            "Epoch 18734: train loss: 0.010007881559431553\n",
            "Epoch 18735: train loss: 0.010007072240114212\n",
            "Epoch 18736: train loss: 0.010006269440054893\n",
            "Epoch 18737: train loss: 0.010005462914705276\n",
            "Epoch 18738: train loss: 0.010004662908613682\n",
            "Epoch 18739: train loss: 0.01000385545194149\n",
            "Epoch 18740: train loss: 0.010003052651882172\n",
            "Epoch 18741: train loss: 0.010002249851822853\n",
            "Epoch 18742: train loss: 0.010001449845731258\n",
            "Epoch 18743: train loss: 0.010000641457736492\n",
            "Epoch 18744: train loss: 0.009999841451644897\n",
            "Epoch 18745: train loss: 0.009999043308198452\n",
            "Epoch 18746: train loss: 0.009998233057558537\n",
            "Epoch 18747: train loss: 0.00999743677675724\n",
            "Epoch 18748: train loss: 0.009996633976697922\n",
            "Epoch 18749: train loss: 0.009995834901928902\n",
            "Epoch 18750: train loss: 0.00999502744525671\n",
            "Epoch 18751: train loss: 0.009994231164455414\n",
            "Epoch 18752: train loss: 0.009993433952331543\n",
            "Epoch 18753: train loss: 0.00999263022094965\n",
            "Epoch 18754: train loss: 0.009991827420890331\n",
            "Epoch 18755: train loss: 0.00999103020876646\n",
            "Epoch 18756: train loss: 0.009990222752094269\n",
            "Epoch 18757: train loss: 0.009989422746002674\n",
            "Epoch 18758: train loss: 0.009988624602556229\n",
            "Epoch 18759: train loss: 0.00998781993985176\n",
            "Epoch 18760: train loss: 0.009987019933760166\n",
            "Epoch 18761: train loss: 0.009986218065023422\n",
            "Epoch 18762: train loss: 0.009985416196286678\n",
            "Epoch 18763: train loss: 0.009984621778130531\n",
            "Epoch 18764: train loss: 0.009983813390135765\n",
            "Epoch 18765: train loss: 0.00998301524668932\n",
            "Epoch 18766: train loss: 0.0099822161719203\n",
            "Epoch 18767: train loss: 0.00998141523450613\n",
            "Epoch 18768: train loss: 0.009980618953704834\n",
            "Epoch 18769: train loss: 0.009979812428355217\n",
            "Epoch 18770: train loss: 0.009979010559618473\n",
            "Epoch 18771: train loss: 0.009978215210139751\n",
            "Epoch 18772: train loss: 0.009977420791983604\n",
            "Epoch 18773: train loss: 0.009976627305150032\n",
            "Epoch 18774: train loss: 0.009975825436413288\n",
            "Epoch 18775: train loss: 0.009975029155611992\n",
            "Epoch 18776: train loss: 0.009974231943488121\n",
            "Epoch 18777: train loss: 0.0099734365940094\n",
            "Epoch 18778: train loss: 0.009972631931304932\n",
            "Epoch 18779: train loss: 0.009971837513148785\n",
            "Epoch 18780: train loss: 0.009971044026315212\n",
            "Epoch 18781: train loss: 0.009970246814191341\n",
            "Epoch 18782: train loss: 0.009969450533390045\n",
            "Epoch 18783: train loss: 0.0099686523899436\n",
            "Epoch 18784: train loss: 0.009967857971787453\n",
            "Epoch 18785: train loss: 0.009967057965695858\n",
            "Epoch 18786: train loss: 0.009966266341507435\n",
            "Epoch 18787: train loss: 0.009965465404093266\n",
            "Epoch 18788: train loss: 0.009964674711227417\n",
            "Epoch 18789: train loss: 0.009963874705135822\n",
            "Epoch 18790: train loss: 0.009963075630366802\n",
            "Epoch 18791: train loss: 0.009962279349565506\n",
            "Epoch 18792: train loss: 0.009961486794054508\n",
            "Epoch 18793: train loss: 0.009960686787962914\n",
            "Epoch 18794: train loss: 0.009959893301129341\n",
            "Epoch 18795: train loss: 0.009959100745618343\n",
            "Epoch 18796: train loss: 0.009958302602171898\n",
            "Epoch 18797: train loss: 0.009957501664757729\n",
            "Epoch 18798: train loss: 0.009956707246601582\n",
            "Epoch 18799: train loss: 0.009955916553735733\n",
            "Epoch 18800: train loss: 0.009955118410289288\n",
            "Epoch 18801: train loss: 0.009954321198165417\n",
            "Epoch 18802: train loss: 0.009953528642654419\n",
            "Epoch 18803: train loss: 0.009952731430530548\n",
            "Epoch 18804: train loss: 0.009951937943696976\n",
            "Epoch 18805: train loss: 0.00995113980025053\n",
            "Epoch 18806: train loss: 0.009950350038707256\n",
            "Epoch 18807: train loss: 0.009949550963938236\n",
            "Epoch 18808: train loss: 0.009948758408427238\n",
            "Epoch 18809: train loss: 0.009947968646883965\n",
            "Epoch 18810: train loss: 0.009947171434760094\n",
            "Epoch 18811: train loss: 0.009946376085281372\n",
            "Epoch 18812: train loss: 0.00994558073580265\n",
            "Epoch 18813: train loss: 0.00994478352367878\n",
            "Epoch 18814: train loss: 0.009943993762135506\n",
            "Epoch 18815: train loss: 0.00994319748133421\n",
            "Epoch 18816: train loss: 0.009942403063178062\n",
            "Epoch 18817: train loss: 0.009941608645021915\n",
            "Epoch 18818: train loss: 0.009940819814801216\n",
            "Epoch 18819: train loss: 0.009940024465322495\n",
            "Epoch 18820: train loss: 0.009939230047166348\n",
            "Epoch 18821: train loss: 0.0099384356290102\n",
            "Epoch 18822: train loss: 0.009937643073499203\n",
            "Epoch 18823: train loss: 0.00993685144931078\n",
            "Epoch 18824: train loss: 0.00993606448173523\n",
            "Epoch 18825: train loss: 0.009935272857546806\n",
            "Epoch 18826: train loss: 0.00993447843939066\n",
            "Epoch 18827: train loss: 0.009933692403137684\n",
            "Epoch 18828: train loss: 0.009932897984981537\n",
            "Epoch 18829: train loss: 0.00993210356682539\n",
            "Epoch 18830: train loss: 0.009931318461894989\n",
            "Epoch 18831: train loss: 0.009930521249771118\n",
            "Epoch 18832: train loss: 0.009929733350872993\n",
            "Epoch 18833: train loss: 0.00992894172668457\n",
            "Epoch 18834: train loss: 0.009928149171173573\n",
            "Epoch 18835: train loss: 0.009927362203598022\n",
            "Epoch 18836: train loss: 0.009926565922796726\n",
            "Epoch 18837: train loss: 0.009925777092576027\n",
            "Epoch 18838: train loss: 0.009924986399710178\n",
            "Epoch 18839: train loss: 0.00992419384419918\n",
            "Epoch 18840: train loss: 0.00992340873926878\n",
            "Epoch 18841: train loss: 0.009922613389790058\n",
            "Epoch 18842: train loss: 0.009921827353537083\n",
            "Epoch 18843: train loss: 0.009921036660671234\n",
            "Epoch 18844: train loss: 0.009920242242515087\n",
            "Epoch 18845: train loss: 0.009919452480971813\n",
            "Epoch 18846: train loss: 0.009918663650751114\n",
            "Epoch 18847: train loss: 0.009917872957885265\n",
            "Epoch 18848: train loss: 0.009917091578245163\n",
            "Epoch 18849: train loss: 0.00991629995405674\n",
            "Epoch 18850: train loss: 0.009915507398545742\n",
            "Epoch 18851: train loss: 0.009914714843034744\n",
            "Epoch 18852: train loss: 0.009913927875459194\n",
            "Epoch 18853: train loss: 0.009913131594657898\n",
            "Epoch 18854: train loss: 0.009912348352372646\n",
            "Epoch 18855: train loss: 0.009911558590829372\n",
            "Epoch 18856: train loss: 0.009910767897963524\n",
            "Epoch 18857: train loss: 0.009909980930387974\n",
            "Epoch 18858: train loss: 0.009909196756780148\n",
            "Epoch 18859: train loss: 0.009908410720527172\n",
            "Epoch 18860: train loss: 0.009907620958983898\n",
            "Epoch 18861: train loss: 0.009906832128763199\n",
            "Epoch 18862: train loss: 0.009906042367219925\n",
            "Epoch 18863: train loss: 0.009905250743031502\n",
            "Epoch 18864: train loss: 0.009904471226036549\n",
            "Epoch 18865: train loss: 0.009903681464493275\n",
            "Epoch 18866: train loss: 0.0099028954282403\n",
            "Epoch 18867: train loss: 0.0099021065980196\n",
            "Epoch 18868: train loss: 0.0099013177677989\n",
            "Epoch 18869: train loss: 0.00990053080022335\n",
            "Epoch 18870: train loss: 0.009899742901325226\n",
            "Epoch 18871: train loss: 0.009898960590362549\n",
            "Epoch 18872: train loss: 0.009898178279399872\n",
            "Epoch 18873: train loss: 0.009897380135953426\n",
            "Epoch 18874: train loss: 0.009896601550281048\n",
            "Epoch 18875: train loss: 0.009895812720060349\n",
            "Epoch 18876: train loss: 0.00989502388983965\n",
            "Epoch 18877: train loss: 0.009894237853586674\n",
            "Epoch 18878: train loss: 0.009893451817333698\n",
            "Epoch 18879: train loss: 0.0098926592618227\n",
            "Epoch 18880: train loss: 0.009891881607472897\n",
            "Epoch 18881: train loss: 0.009891093708574772\n",
            "Epoch 18882: train loss: 0.00989031046628952\n",
            "Epoch 18883: train loss: 0.009889522567391396\n",
            "Epoch 18884: train loss: 0.009888739325106144\n",
            "Epoch 18885: train loss: 0.009887956082820892\n",
            "Epoch 18886: train loss: 0.009887166321277618\n",
            "Epoch 18887: train loss: 0.009886381216347218\n",
            "Epoch 18888: train loss: 0.009885597974061966\n",
            "Epoch 18889: train loss: 0.009884814731776714\n",
            "Epoch 18890: train loss: 0.009884027764201164\n",
            "Epoch 18891: train loss: 0.009883243590593338\n",
            "Epoch 18892: train loss: 0.009882460348308086\n",
            "Epoch 18893: train loss: 0.009881682693958282\n",
            "Epoch 18894: train loss: 0.009880894795060158\n",
            "Epoch 18895: train loss: 0.009880115278065205\n",
            "Epoch 18896: train loss: 0.009879335761070251\n",
            "Epoch 18897: train loss: 0.009878549724817276\n",
            "Epoch 18898: train loss: 0.009877769276499748\n",
            "Epoch 18899: train loss: 0.009876982308924198\n",
            "Epoch 18900: train loss: 0.009876204654574394\n",
            "Epoch 18901: train loss: 0.009875425137579441\n",
            "Epoch 18902: train loss: 0.009874639101326466\n",
            "Epoch 18903: train loss: 0.009873859584331512\n",
            "Epoch 18904: train loss: 0.009873077273368835\n",
            "Epoch 18905: train loss: 0.009872295893728733\n",
            "Epoch 18906: train loss: 0.009871512651443481\n",
            "Epoch 18907: train loss: 0.009870721027255058\n",
            "Epoch 18908: train loss: 0.009869948029518127\n",
            "Epoch 18909: train loss: 0.009869169443845749\n",
            "Epoch 18910: train loss: 0.009868386201560497\n",
            "Epoch 18911: train loss: 0.009867601096630096\n",
            "Epoch 18912: train loss: 0.009866823442280293\n",
            "Epoch 18913: train loss: 0.009866037406027317\n",
            "Epoch 18914: train loss: 0.009865261614322662\n",
            "Epoch 18915: train loss: 0.009864477440714836\n",
            "Epoch 18916: train loss: 0.009863696992397308\n",
            "Epoch 18917: train loss: 0.009862915612757206\n",
            "Epoch 18918: train loss: 0.009862139821052551\n",
            "Epoch 18919: train loss: 0.009861357510089874\n",
            "Epoch 18920: train loss: 0.009860575199127197\n",
            "Epoch 18921: train loss: 0.009859791956841946\n",
            "Epoch 18922: train loss: 0.009859013371169567\n",
            "Epoch 18923: train loss: 0.009858240373432636\n",
            "Epoch 18924: train loss: 0.009857451543211937\n",
            "Epoch 18925: train loss: 0.009856673888862133\n",
            "Epoch 18926: train loss: 0.009855897165834904\n",
            "Epoch 18927: train loss: 0.00985511764883995\n",
            "Epoch 18928: train loss: 0.009854335337877274\n",
            "Epoch 18929: train loss: 0.00985355768352747\n",
            "Epoch 18930: train loss: 0.009852776303887367\n",
            "Epoch 18931: train loss: 0.009851998649537563\n",
            "Epoch 18932: train loss: 0.009851223789155483\n",
            "Epoch 18933: train loss: 0.009850438684225082\n",
            "Epoch 18934: train loss: 0.00984965730458498\n",
            "Epoch 18935: train loss: 0.009848874062299728\n",
            "Epoch 18936: train loss: 0.009848104789853096\n",
            "Epoch 18937: train loss: 0.009847325272858143\n",
            "Epoch 18938: train loss: 0.009846539236605167\n",
            "Epoch 18939: train loss: 0.009845763444900513\n",
            "Epoch 18940: train loss: 0.009844986721873283\n",
            "Epoch 18941: train loss: 0.009844200685620308\n",
            "Epoch 18942: train loss: 0.009843426756560802\n",
            "Epoch 18943: train loss: 0.009842648170888424\n",
            "Epoch 18944: train loss: 0.009841864928603172\n",
            "Epoch 18945: train loss: 0.009841094724833965\n",
            "Epoch 18946: train loss: 0.009840310551226139\n",
            "Epoch 18947: train loss: 0.009839542210102081\n",
            "Epoch 18948: train loss: 0.009838767349720001\n",
            "Epoch 18949: train loss: 0.009837991558015347\n",
            "Epoch 18950: train loss: 0.009837212972342968\n",
            "Epoch 18951: train loss: 0.009836435317993164\n",
            "Epoch 18952: train loss: 0.009835658594965935\n",
            "Epoch 18953: train loss: 0.009834878146648407\n",
            "Epoch 18954: train loss: 0.009834100492298603\n",
            "Epoch 18955: train loss: 0.009833325631916523\n",
            "Epoch 18956: train loss: 0.009832551702857018\n",
            "Epoch 18957: train loss: 0.009831768460571766\n",
            "Epoch 18958: train loss: 0.009830998256802559\n",
            "Epoch 18959: train loss: 0.009830225259065628\n",
            "Epoch 18960: train loss: 0.009829450398683548\n",
            "Epoch 18961: train loss: 0.009828670881688595\n",
            "Epoch 18962: train loss: 0.00982789695262909\n",
            "Epoch 18963: train loss: 0.009827119298279285\n",
            "Epoch 18964: train loss: 0.00982634536921978\n",
            "Epoch 18965: train loss: 0.0098255705088377\n",
            "Epoch 18966: train loss: 0.009824794717133045\n",
            "Epoch 18967: train loss: 0.009824015200138092\n",
            "Epoch 18968: train loss: 0.009823244996368885\n",
            "Epoch 18969: train loss: 0.009822472929954529\n",
            "Epoch 18970: train loss: 0.009821692481637001\n",
            "Epoch 18971: train loss: 0.009820922277867794\n",
            "Epoch 18972: train loss: 0.009820147417485714\n",
            "Epoch 18973: train loss: 0.009819374419748783\n",
            "Epoch 18974: train loss: 0.009818600490689278\n",
            "Epoch 18975: train loss: 0.009817821905016899\n",
            "Epoch 18976: train loss: 0.009817048907279968\n",
            "Epoch 18977: train loss: 0.009816278703510761\n",
            "Epoch 18978: train loss: 0.009815503843128681\n",
            "Epoch 18979: train loss: 0.009814736433327198\n",
            "Epoch 18980: train loss: 0.009813960641622543\n",
            "Epoch 18981: train loss: 0.009813189506530762\n",
            "Epoch 18982: train loss: 0.009812421165406704\n",
            "Epoch 18983: train loss: 0.009811637923121452\n",
            "Epoch 18984: train loss: 0.00981087051331997\n",
            "Epoch 18985: train loss: 0.009810092858970165\n",
            "Epoch 18986: train loss: 0.009809321723878384\n",
            "Epoch 18987: train loss: 0.009808550588786602\n",
            "Epoch 18988: train loss: 0.009807775728404522\n",
            "Epoch 18989: train loss: 0.009807003661990166\n",
            "Epoch 18990: train loss: 0.009806228801608086\n",
            "Epoch 18991: train loss: 0.009805458597838879\n",
            "Epoch 18992: train loss: 0.0098046800121665\n",
            "Epoch 18993: train loss: 0.00980391912162304\n",
            "Epoch 18994: train loss: 0.009803139604628086\n",
            "Epoch 18995: train loss: 0.009802368469536304\n",
            "Epoch 18996: train loss: 0.009801595471799374\n",
            "Epoch 18997: train loss: 0.009800828993320465\n",
            "Epoch 18998: train loss: 0.009800056926906109\n",
            "Epoch 18999: train loss: 0.009799284860491753\n",
            "Epoch 19000: train loss: 0.009798510000109673\n",
            "Epoch 19001: train loss: 0.009797739796340466\n",
            "Epoch 19002: train loss: 0.009796971455216408\n",
            "Epoch 19003: train loss: 0.0097962012514472\n",
            "Epoch 19004: train loss: 0.00979542825371027\n",
            "Epoch 19005: train loss: 0.009794655255973339\n",
            "Epoch 19006: train loss: 0.009793887846171856\n",
            "Epoch 19007: train loss: 0.009793119505047798\n",
            "Epoch 19008: train loss: 0.00979235302656889\n",
            "Epoch 19009: train loss: 0.009791580028831959\n",
            "Epoch 19010: train loss: 0.009790809825062752\n",
            "Epoch 19011: train loss: 0.009790036827325821\n",
            "Epoch 19012: train loss: 0.00978926382958889\n",
            "Epoch 19013: train loss: 0.009788496419787407\n",
            "Epoch 19014: train loss: 0.009787729941308498\n",
            "Epoch 19015: train loss: 0.00978696160018444\n",
            "Epoch 19016: train loss: 0.009786189533770084\n",
            "Epoch 19017: train loss: 0.009785421192646027\n",
            "Epoch 19018: train loss: 0.009784652851521969\n",
            "Epoch 19019: train loss: 0.009783882647752762\n",
            "Epoch 19020: train loss: 0.009783121757209301\n",
            "Epoch 19021: train loss: 0.009782351553440094\n",
            "Epoch 19022: train loss: 0.009781581349670887\n",
            "Epoch 19023: train loss: 0.009780810214579105\n",
            "Epoch 19024: train loss: 0.009780043736100197\n",
            "Epoch 19025: train loss: 0.00977927353233099\n",
            "Epoch 19026: train loss: 0.009778503328561783\n",
            "Epoch 19027: train loss: 0.0097777359187603\n",
            "Epoch 19028: train loss: 0.009776968508958817\n",
            "Epoch 19029: train loss: 0.009776202030479908\n",
            "Epoch 19030: train loss: 0.009775434620678425\n",
            "Epoch 19031: train loss: 0.009774665348231792\n",
            "Epoch 19032: train loss: 0.009773893281817436\n",
            "Epoch 19033: train loss: 0.009773130528628826\n",
            "Epoch 19034: train loss: 0.009772367775440216\n",
            "Epoch 19035: train loss: 0.009771604090929031\n",
            "Epoch 19036: train loss: 0.009770836681127548\n",
            "Epoch 19037: train loss: 0.009770063683390617\n",
            "Epoch 19038: train loss: 0.009769300930202007\n",
            "Epoch 19039: train loss: 0.009768535383045673\n",
            "Epoch 19040: train loss: 0.009767768904566765\n",
            "Epoch 19041: train loss: 0.009766998700797558\n",
            "Epoch 19042: train loss: 0.009766238741576672\n",
            "Epoch 19043: train loss: 0.00976547785103321\n",
            "Epoch 19044: train loss: 0.009764703921973705\n",
            "Epoch 19045: train loss: 0.009763944894075394\n",
            "Epoch 19046: train loss: 0.009763174690306187\n",
            "Epoch 19047: train loss: 0.00976241659373045\n",
            "Epoch 19048: train loss: 0.009761648252606392\n",
            "Epoch 19049: train loss: 0.009760883636772633\n",
            "Epoch 19050: train loss: 0.009760119952261448\n",
            "Epoch 19051: train loss: 0.009759355336427689\n",
            "Epoch 19052: train loss: 0.009758587926626205\n",
            "Epoch 19053: train loss: 0.00975782610476017\n",
            "Epoch 19054: train loss: 0.00975706148892641\n",
            "Epoch 19055: train loss: 0.009756296873092651\n",
            "Epoch 19056: train loss: 0.00975553784519434\n",
            "Epoch 19057: train loss: 0.00975477509200573\n",
            "Epoch 19058: train loss: 0.009754013270139694\n",
            "Epoch 19059: train loss: 0.009753248654305935\n",
            "Epoch 19060: train loss: 0.00975248496979475\n",
            "Epoch 19061: train loss: 0.009751718491315842\n",
            "Epoch 19062: train loss: 0.009750956669449806\n",
            "Epoch 19063: train loss: 0.009750193916261196\n",
            "Epoch 19064: train loss: 0.009749430231750011\n",
            "Epoch 19065: train loss: 0.009748668409883976\n",
            "Epoch 19066: train loss: 0.00974790845066309\n",
            "Epoch 19067: train loss: 0.009747141972184181\n",
            "Epoch 19068: train loss: 0.009746379218995571\n",
            "Epoch 19069: train loss: 0.009745616465806961\n",
            "Epoch 19070: train loss: 0.009744852781295776\n",
            "Epoch 19071: train loss: 0.009744096547365189\n",
            "Epoch 19072: train loss: 0.009743331000208855\n",
            "Epoch 19073: train loss: 0.009742568247020245\n",
            "Epoch 19074: train loss: 0.00974180269986391\n",
            "Epoch 19075: train loss: 0.009741047397255898\n",
            "Epoch 19076: train loss: 0.009740279987454414\n",
            "Epoch 19077: train loss: 0.009739520959556103\n",
            "Epoch 19078: train loss: 0.009738760069012642\n",
            "Epoch 19079: train loss: 0.009737996384501457\n",
            "Epoch 19080: train loss: 0.009737231768667698\n",
            "Epoch 19081: train loss: 0.009736469015479088\n",
            "Epoch 19082: train loss: 0.0097357127815485\n",
            "Epoch 19083: train loss: 0.009734953753650188\n",
            "Epoch 19084: train loss: 0.009734186343848705\n",
            "Epoch 19085: train loss: 0.009733431041240692\n",
            "Epoch 19086: train loss: 0.009732670150697231\n",
            "Epoch 19087: train loss: 0.009731901809573174\n",
            "Epoch 19088: train loss: 0.009731137193739414\n",
            "Epoch 19089: train loss: 0.009730378165841103\n",
            "Epoch 19090: train loss: 0.009729621931910515\n",
            "Epoch 19091: train loss: 0.00972886011004448\n",
            "Epoch 19092: train loss: 0.009728101082146168\n",
            "Epoch 19093: train loss: 0.00972734484821558\n",
            "Epoch 19094: train loss: 0.009726579301059246\n",
            "Epoch 19095: train loss: 0.00972581934183836\n",
            "Epoch 19096: train loss: 0.009725058451294899\n",
            "Epoch 19097: train loss: 0.009724300354719162\n",
            "Epoch 19098: train loss: 0.009723539464175701\n",
            "Epoch 19099: train loss: 0.009722781367599964\n",
            "Epoch 19100: train loss: 0.009722016751766205\n",
            "Epoch 19101: train loss: 0.009721262380480766\n",
            "Epoch 19102: train loss: 0.009720495902001858\n",
            "Epoch 19103: train loss: 0.009719735011458397\n",
            "Epoch 19104: train loss: 0.00971897877752781\n",
            "Epoch 19105: train loss: 0.009718219749629498\n",
            "Epoch 19106: train loss: 0.00971746165305376\n",
            "Epoch 19107: train loss: 0.009716697968542576\n",
            "Epoch 19108: train loss: 0.00971593800932169\n",
            "Epoch 19109: train loss: 0.009715181775391102\n",
            "Epoch 19110: train loss: 0.009714419953525066\n",
            "Epoch 19111: train loss: 0.009713659062981606\n",
            "Epoch 19112: train loss: 0.009712904691696167\n",
            "Epoch 19113: train loss: 0.009712145663797855\n",
            "Epoch 19114: train loss: 0.009711386635899544\n",
            "Epoch 19115: train loss: 0.009710624814033508\n",
            "Epoch 19116: train loss: 0.009709863923490047\n",
            "Epoch 19117: train loss: 0.009709101170301437\n",
            "Epoch 19118: train loss: 0.009708351455628872\n",
            "Epoch 19119: train loss: 0.009707593359053135\n",
            "Epoch 19120: train loss: 0.009706835262477398\n",
            "Epoch 19121: train loss: 0.009706074371933937\n",
            "Epoch 19122: train loss: 0.009705322794616222\n",
            "Epoch 19123: train loss: 0.00970456749200821\n",
            "Epoch 19124: train loss: 0.009703807532787323\n",
            "Epoch 19125: train loss: 0.009703055024147034\n",
            "Epoch 19126: train loss: 0.009702303446829319\n",
            "Epoch 19127: train loss: 0.009701544418931007\n",
            "Epoch 19128: train loss: 0.009700792841613293\n",
            "Epoch 19129: train loss: 0.009700041264295578\n",
            "Epoch 19130: train loss: 0.00969928503036499\n",
            "Epoch 19131: train loss: 0.009698533453047276\n",
            "Epoch 19132: train loss: 0.00969776976853609\n",
            "Epoch 19133: train loss: 0.009697024710476398\n",
            "Epoch 19134: train loss: 0.00969626847654581\n",
            "Epoch 19135: train loss: 0.009695513173937798\n",
            "Epoch 19136: train loss: 0.009694760665297508\n",
            "Epoch 19137: train loss: 0.009694005362689495\n",
            "Epoch 19138: train loss: 0.009693254716694355\n",
            "Epoch 19139: train loss: 0.009692499414086342\n",
            "Epoch 19140: train loss: 0.009691741317510605\n",
            "Epoch 19141: train loss: 0.009690994396805763\n",
            "Epoch 19142: train loss: 0.0096902409568429\n",
            "Epoch 19143: train loss: 0.009689484722912312\n",
            "Epoch 19144: train loss: 0.00968873780220747\n",
            "Epoch 19145: train loss: 0.009687979705631733\n",
            "Epoch 19146: train loss: 0.009687228128314018\n",
            "Epoch 19147: train loss: 0.009686472825706005\n",
            "Epoch 19148: train loss: 0.009685720317065716\n",
            "Epoch 19149: train loss: 0.009684965945780277\n",
            "Epoch 19150: train loss: 0.009684215299785137\n",
            "Epoch 19151: train loss: 0.009683460928499699\n",
            "Epoch 19152: train loss: 0.009682709351181984\n",
            "Epoch 19153: train loss: 0.00968195777386427\n",
            "Epoch 19154: train loss: 0.00968120526522398\n",
            "Epoch 19155: train loss: 0.00968045461922884\n",
            "Epoch 19156: train loss: 0.00967970211058855\n",
            "Epoch 19157: train loss: 0.009678945876657963\n",
            "Epoch 19158: train loss: 0.009678196161985397\n",
            "Epoch 19159: train loss: 0.009677443653345108\n",
            "Epoch 19160: train loss: 0.009676691144704819\n",
            "Epoch 19161: train loss: 0.00967593677341938\n",
            "Epoch 19162: train loss: 0.00967518612742424\n",
            "Epoch 19163: train loss: 0.009674442932009697\n",
            "Epoch 19164: train loss: 0.009673681110143661\n",
            "Epoch 19165: train loss: 0.009672935120761395\n",
            "Epoch 19166: train loss: 0.00967218354344368\n",
            "Epoch 19167: train loss: 0.00967143289744854\n",
            "Epoch 19168: train loss: 0.009670677594840527\n",
            "Epoch 19169: train loss: 0.009669936262071133\n",
            "Epoch 19170: train loss: 0.009669180028140545\n",
            "Epoch 19171: train loss: 0.009668429382145405\n",
            "Epoch 19172: train loss: 0.009667684324085712\n",
            "Epoch 19173: train loss: 0.009666924364864826\n",
            "Epoch 19174: train loss: 0.00966617465019226\n",
            "Epoch 19175: train loss: 0.009665424935519695\n",
            "Epoch 19176: train loss: 0.009664671495556831\n",
            "Epoch 19177: train loss: 0.009663925506174564\n",
            "Epoch 19178: train loss: 0.00966317392885685\n",
            "Epoch 19179: train loss: 0.009662427939474583\n",
            "Epoch 19180: train loss: 0.009661673568189144\n",
            "Epoch 19181: train loss: 0.009660923853516579\n",
            "Epoch 19182: train loss: 0.009660175070166588\n",
            "Epoch 19183: train loss: 0.009659426286816597\n",
            "Epoch 19184: train loss: 0.009658664464950562\n",
            "Epoch 19185: train loss: 0.009657923132181168\n",
            "Epoch 19186: train loss: 0.009657175280153751\n",
            "Epoch 19187: train loss: 0.009656429290771484\n",
            "Epoch 19188: train loss: 0.009655676782131195\n",
            "Epoch 19189: train loss: 0.009654922410845757\n",
            "Epoch 19190: train loss: 0.009654172696173191\n",
            "Epoch 19191: train loss: 0.009653421118855476\n",
            "Epoch 19192: train loss: 0.00965267512947321\n",
            "Epoch 19193: train loss: 0.00965192448347807\n",
            "Epoch 19194: train loss: 0.009651177562773228\n",
            "Epoch 19195: train loss: 0.009650422260165215\n",
            "Epoch 19196: train loss: 0.009649676270782948\n",
            "Epoch 19197: train loss: 0.00964893028140068\n",
            "Epoch 19198: train loss: 0.009648176841437817\n",
            "Epoch 19199: train loss: 0.009647433646023273\n",
            "Epoch 19200: train loss: 0.00964668020606041\n",
            "Epoch 19201: train loss: 0.009645928628742695\n",
            "Epoch 19202: train loss: 0.009645184502005577\n",
            "Epoch 19203: train loss: 0.009644437581300735\n",
            "Epoch 19204: train loss: 0.009643691591918468\n",
            "Epoch 19205: train loss: 0.00964293722063303\n",
            "Epoch 19206: train loss: 0.009642194956541061\n",
            "Epoch 19207: train loss: 0.009641447104513645\n",
            "Epoch 19208: train loss: 0.009640699252486229\n",
            "Epoch 19209: train loss: 0.009639952331781387\n",
            "Epoch 19210: train loss: 0.009639204479753971\n",
            "Epoch 19211: train loss: 0.009638458490371704\n",
            "Epoch 19212: train loss: 0.009637713432312012\n",
            "Epoch 19213: train loss: 0.009636964648962021\n",
            "Epoch 19214: train loss: 0.009636219590902328\n",
            "Epoch 19215: train loss: 0.009635478258132935\n",
            "Epoch 19216: train loss: 0.009634732268750668\n",
            "Epoch 19217: train loss: 0.00963398814201355\n",
            "Epoch 19218: train loss: 0.009633242152631283\n",
            "Epoch 19219: train loss: 0.009632492437958717\n",
            "Epoch 19220: train loss: 0.009631752036511898\n",
            "Epoch 19221: train loss: 0.009631002321839333\n",
            "Epoch 19222: train loss: 0.009630260989069939\n",
            "Epoch 19223: train loss: 0.009629517793655396\n",
            "Epoch 19224: train loss: 0.009628769010305405\n",
            "Epoch 19225: train loss: 0.009628023952245712\n",
            "Epoch 19226: train loss: 0.009627282619476318\n",
            "Epoch 19227: train loss: 0.009626534767448902\n",
            "Epoch 19228: train loss: 0.009625796228647232\n",
            "Epoch 19229: train loss: 0.00962505117058754\n",
            "Epoch 19230: train loss: 0.009624306112527847\n",
            "Epoch 19231: train loss: 0.009623557329177856\n",
            "Epoch 19232: train loss: 0.00962281133979559\n",
            "Epoch 19233: train loss: 0.009622074663639069\n",
            "Epoch 19234: train loss: 0.009621327742934227\n",
            "Epoch 19235: train loss: 0.009620584547519684\n",
            "Epoch 19236: train loss: 0.009619839489459991\n",
            "Epoch 19237: train loss: 0.009619095362722874\n",
            "Epoch 19238: train loss: 0.009618359617888927\n",
            "Epoch 19239: train loss: 0.009617616422474384\n",
            "Epoch 19240: train loss: 0.009616867639124393\n",
            "Epoch 19241: train loss: 0.009616123512387276\n",
            "Epoch 19242: train loss: 0.009615388698875904\n",
            "Epoch 19243: train loss: 0.009614638984203339\n",
            "Epoch 19244: train loss: 0.009613900445401669\n",
            "Epoch 19245: train loss: 0.009613155387341976\n",
            "Epoch 19246: train loss: 0.009612422436475754\n",
            "Epoch 19247: train loss: 0.009611676447093487\n",
            "Epoch 19248: train loss: 0.009610936045646667\n",
            "Epoch 19249: train loss: 0.009610193781554699\n",
            "Epoch 19250: train loss: 0.009609455242753029\n",
            "Epoch 19251: train loss: 0.009608710184693336\n",
            "Epoch 19252: train loss: 0.009607968851923943\n",
            "Epoch 19253: train loss: 0.0096072256565094\n",
            "Epoch 19254: train loss: 0.009606484323740005\n",
            "Epoch 19255: train loss: 0.009605749510228634\n",
            "Epoch 19256: train loss: 0.009605009108781815\n",
            "Epoch 19257: train loss: 0.009604271501302719\n",
            "Epoch 19258: train loss: 0.009603526443243027\n",
            "Epoch 19259: train loss: 0.009602786973118782\n",
            "Epoch 19260: train loss: 0.009602040983736515\n",
            "Epoch 19261: train loss: 0.00960130151361227\n",
            "Epoch 19262: train loss: 0.009600563906133175\n",
            "Epoch 19263: train loss: 0.009599821642041206\n",
            "Epoch 19264: train loss: 0.009599079377949238\n",
            "Epoch 19265: train loss: 0.00959833711385727\n",
            "Epoch 19266: train loss: 0.009597599506378174\n",
            "Epoch 19267: train loss: 0.00959686003625393\n",
            "Epoch 19268: train loss: 0.00959611963480711\n",
            "Epoch 19269: train loss: 0.009595378302037716\n",
            "Epoch 19270: train loss: 0.009594641625881195\n",
            "Epoch 19271: train loss: 0.009593897499144077\n",
            "Epoch 19272: train loss: 0.009593155235052109\n",
            "Epoch 19273: train loss: 0.009592417627573013\n",
            "Epoch 19274: train loss: 0.009591680020093918\n",
            "Epoch 19275: train loss: 0.009590936824679375\n",
            "Epoch 19276: train loss: 0.009590198285877705\n",
            "Epoch 19277: train loss: 0.009589459747076035\n",
            "Epoch 19278: train loss: 0.009588724002242088\n",
            "Epoch 19279: train loss: 0.009587983600795269\n",
            "Epoch 19280: train loss: 0.009587242268025875\n",
            "Epoch 19281: train loss: 0.009586505591869354\n",
            "Epoch 19282: train loss: 0.009585763327777386\n",
            "Epoch 19283: train loss: 0.00958502572029829\n",
            "Epoch 19284: train loss: 0.009584293700754642\n",
            "Epoch 19285: train loss: 0.0095835505053401\n",
            "Epoch 19286: train loss: 0.009582814760506153\n",
            "Epoch 19287: train loss: 0.009582076221704483\n",
            "Epoch 19288: train loss: 0.009581338614225388\n",
            "Epoch 19289: train loss: 0.009580598212778568\n",
            "Epoch 19290: train loss: 0.00957986619323492\n",
            "Epoch 19291: train loss: 0.009579122066497803\n",
            "Epoch 19292: train loss: 0.009578386321663857\n",
            "Epoch 19293: train loss: 0.009577649645507336\n",
            "Epoch 19294: train loss: 0.009576909244060516\n",
            "Epoch 19295: train loss: 0.009576172567903996\n",
            "Epoch 19296: train loss: 0.009575435891747475\n",
            "Epoch 19297: train loss: 0.009574703872203827\n",
            "Epoch 19298: train loss: 0.009573965333402157\n",
            "Epoch 19299: train loss: 0.009573226794600487\n",
            "Epoch 19300: train loss: 0.009572490118443966\n",
            "Epoch 19301: train loss: 0.009571759030222893\n",
            "Epoch 19302: train loss: 0.009571014903485775\n",
            "Epoch 19303: train loss: 0.009570283815264702\n",
            "Epoch 19304: train loss: 0.009569549933075905\n",
            "Epoch 19305: train loss: 0.009568809531629086\n",
            "Epoch 19306: train loss: 0.009568077512085438\n",
            "Epoch 19307: train loss: 0.009567341767251492\n",
            "Epoch 19308: train loss: 0.009566607885062695\n",
            "Epoch 19309: train loss: 0.009565867483615875\n",
            "Epoch 19310: train loss: 0.009565135464072227\n",
            "Epoch 19311: train loss: 0.009564399719238281\n",
            "Epoch 19312: train loss: 0.009563665837049484\n",
            "Epoch 19313: train loss: 0.009562935680150986\n",
            "Epoch 19314: train loss: 0.009562197141349316\n",
            "Epoch 19315: train loss: 0.009561464190483093\n",
            "Epoch 19316: train loss: 0.00956073496490717\n",
            "Epoch 19317: train loss: 0.009559998288750648\n",
            "Epoch 19318: train loss: 0.009559261612594128\n",
            "Epoch 19319: train loss: 0.009558528661727905\n",
            "Epoch 19320: train loss: 0.009557794779539108\n",
            "Epoch 19321: train loss: 0.009557061828672886\n",
            "Epoch 19322: train loss: 0.009556325152516365\n",
            "Epoch 19323: train loss: 0.009555591270327568\n",
            "Epoch 19324: train loss: 0.009554862976074219\n",
            "Epoch 19325: train loss: 0.009554125368595123\n",
            "Epoch 19326: train loss: 0.009553398005664349\n",
            "Epoch 19327: train loss: 0.009552662260830402\n",
            "Epoch 19328: train loss: 0.00955192744731903\n",
            "Epoch 19329: train loss: 0.009551196359097958\n",
            "Epoch 19330: train loss: 0.00955046247690916\n",
            "Epoch 19331: train loss: 0.009549733251333237\n",
            "Epoch 19332: train loss: 0.009548994712531567\n",
            "Epoch 19333: train loss: 0.009548268280923367\n",
            "Epoch 19334: train loss: 0.009547536261379719\n",
            "Epoch 19335: train loss: 0.009546798653900623\n",
            "Epoch 19336: train loss: 0.009546065703034401\n",
            "Epoch 19337: train loss: 0.009545333683490753\n",
            "Epoch 19338: train loss: 0.00954460073262453\n",
            "Epoch 19339: train loss: 0.009543873369693756\n",
            "Epoch 19340: train loss: 0.009543140418827534\n",
            "Epoch 19341: train loss: 0.00954240933060646\n",
            "Epoch 19342: train loss: 0.009541679173707962\n",
            "Epoch 19343: train loss: 0.009540935978293419\n",
            "Epoch 19344: train loss: 0.009540212340652943\n",
            "Epoch 19345: train loss: 0.009539477527141571\n",
            "Epoch 19346: train loss: 0.009538743644952774\n",
            "Epoch 19347: train loss: 0.009538013488054276\n",
            "Epoch 19348: train loss: 0.009537284262478352\n",
            "Epoch 19349: train loss: 0.009536553174257278\n",
            "Epoch 19350: train loss: 0.009535819292068481\n",
            "Epoch 19351: train loss: 0.009535086341202259\n",
            "Epoch 19352: train loss: 0.009534360840916634\n",
            "Epoch 19353: train loss: 0.009533626958727837\n",
            "Epoch 19354: train loss: 0.009532894939184189\n",
            "Epoch 19355: train loss: 0.009532165713608265\n",
            "Epoch 19356: train loss: 0.009531430900096893\n",
            "Epoch 19357: train loss: 0.009530700743198395\n",
            "Epoch 19358: train loss: 0.009529968723654747\n",
            "Epoch 19359: train loss: 0.0095292367041111\n",
            "Epoch 19360: train loss: 0.009528511203825474\n",
            "Epoch 19361: train loss: 0.009527777321636677\n",
            "Epoch 19362: train loss: 0.009527049958705902\n",
            "Epoch 19363: train loss: 0.009526321664452553\n",
            "Epoch 19364: train loss: 0.009525586850941181\n",
            "Epoch 19365: train loss: 0.009524861350655556\n",
            "Epoch 19366: train loss: 0.009524134919047356\n",
            "Epoch 19367: train loss: 0.009523403830826283\n",
            "Epoch 19368: train loss: 0.009522683918476105\n",
            "Epoch 19369: train loss: 0.009521950036287308\n",
            "Epoch 19370: train loss: 0.009521220810711384\n",
            "Epoch 19371: train loss: 0.009520498104393482\n",
            "Epoch 19372: train loss: 0.009519767016172409\n",
            "Epoch 19373: train loss: 0.009519040584564209\n",
            "Epoch 19374: train loss: 0.009518317878246307\n",
            "Epoch 19375: train loss: 0.009517583064734936\n",
            "Epoch 19376: train loss: 0.009516852907836437\n",
            "Epoch 19377: train loss: 0.009516130201518536\n",
            "Epoch 19378: train loss: 0.009515399113297462\n",
            "Epoch 19379: train loss: 0.009514675475656986\n",
            "Epoch 19380: train loss: 0.009513948112726212\n",
            "Epoch 19381: train loss: 0.009513216093182564\n",
            "Epoch 19382: train loss: 0.00951248686760664\n",
            "Epoch 19383: train loss: 0.009511762298643589\n",
            "Epoch 19384: train loss: 0.009511036798357964\n",
            "Epoch 19385: train loss: 0.009510308504104614\n",
            "Epoch 19386: train loss: 0.00950958114117384\n",
            "Epoch 19387: train loss: 0.009508855640888214\n",
            "Epoch 19388: train loss: 0.00950812827795744\n",
            "Epoch 19389: train loss: 0.009507402777671814\n",
            "Epoch 19390: train loss: 0.009506676346063614\n",
            "Epoch 19391: train loss: 0.009505953639745712\n",
            "Epoch 19392: train loss: 0.009505222551524639\n",
            "Epoch 19393: train loss: 0.00950449239462614\n",
            "Epoch 19394: train loss: 0.009503771550953388\n",
            "Epoch 19395: train loss: 0.009503043256700039\n",
            "Epoch 19396: train loss: 0.009502318687736988\n",
            "Epoch 19397: train loss: 0.009501596912741661\n",
            "Epoch 19398: train loss: 0.009500864893198013\n",
            "Epoch 19399: train loss: 0.009500144049525261\n",
            "Epoch 19400: train loss: 0.00949941948056221\n",
            "Epoch 19401: train loss: 0.009498689323663712\n",
            "Epoch 19402: train loss: 0.009497970342636108\n",
            "Epoch 19403: train loss: 0.00949724018573761\n",
            "Epoch 19404: train loss: 0.009496524930000305\n",
            "Epoch 19405: train loss: 0.009495798498392105\n",
            "Epoch 19406: train loss: 0.009495076723396778\n",
            "Epoch 19407: train loss: 0.009494348429143429\n",
            "Epoch 19408: train loss: 0.009493624791502953\n",
            "Epoch 19409: train loss: 0.0094929039478302\n",
            "Epoch 19410: train loss: 0.009492177516222\n",
            "Epoch 19411: train loss: 0.009491455741226673\n",
            "Epoch 19412: train loss: 0.00949072651565075\n",
            "Epoch 19413: train loss: 0.009490001015365124\n",
            "Epoch 19414: train loss: 0.009489281103014946\n",
            "Epoch 19415: train loss: 0.009488562121987343\n",
            "Epoch 19416: train loss: 0.009487832896411419\n",
            "Epoch 19417: train loss: 0.009487112984061241\n",
            "Epoch 19418: train loss: 0.00948638841509819\n",
            "Epoch 19419: train loss: 0.009485666640102863\n",
            "Epoch 19420: train loss: 0.009484941139817238\n",
            "Epoch 19421: train loss: 0.00948422122746706\n",
            "Epoch 19422: train loss: 0.009483497589826584\n",
            "Epoch 19423: train loss: 0.009482777677476406\n",
            "Epoch 19424: train loss: 0.009482053108513355\n",
            "Epoch 19425: train loss: 0.009481328539550304\n",
            "Epoch 19426: train loss: 0.00948061142116785\n",
            "Epoch 19427: train loss: 0.009479884058237076\n",
            "Epoch 19428: train loss: 0.009479159489274025\n",
            "Epoch 19429: train loss: 0.009478438645601273\n",
            "Epoch 19430: train loss: 0.009477715007960796\n",
            "Epoch 19431: train loss: 0.009476995095610619\n",
            "Epoch 19432: train loss: 0.009476272389292717\n",
            "Epoch 19433: train loss: 0.009475551545619965\n",
            "Epoch 19434: train loss: 0.009474828839302063\n",
            "Epoch 19435: train loss: 0.009474105201661587\n",
            "Epoch 19436: train loss: 0.009473384357988834\n",
            "Epoch 19437: train loss: 0.009472662582993507\n",
            "Epoch 19438: train loss: 0.009471938945353031\n",
            "Epoch 19439: train loss: 0.009471217170357704\n",
            "Epoch 19440: train loss: 0.0094705019146204\n",
            "Epoch 19441: train loss: 0.009469778276979923\n",
            "Epoch 19442: train loss: 0.009469056501984596\n",
            "Epoch 19443: train loss: 0.009468331933021545\n",
            "Epoch 19444: train loss: 0.009467612020671368\n",
            "Epoch 19445: train loss: 0.009466893970966339\n",
            "Epoch 19446: train loss: 0.009466174058616161\n",
            "Epoch 19447: train loss: 0.009465450420975685\n",
            "Epoch 19448: train loss: 0.009464733302593231\n",
            "Epoch 19449: train loss: 0.009464011527597904\n",
            "Epoch 19450: train loss: 0.009463298134505749\n",
            "Epoch 19451: train loss: 0.009462573565542698\n",
            "Epoch 19452: train loss: 0.009461852721869946\n",
            "Epoch 19453: train loss: 0.009461133740842342\n",
            "Epoch 19454: train loss: 0.009460410103201866\n",
            "Epoch 19455: train loss: 0.009459693916141987\n",
            "Epoch 19456: train loss: 0.009458973072469234\n",
            "Epoch 19457: train loss: 0.009458260610699654\n",
            "Epoch 19458: train loss: 0.009457536041736603\n",
            "Epoch 19459: train loss: 0.009456819854676723\n",
            "Epoch 19460: train loss: 0.00945610273629427\n",
            "Epoch 19461: train loss: 0.009455384686589241\n",
            "Epoch 19462: train loss: 0.009454665705561638\n",
            "Epoch 19463: train loss: 0.009453943930566311\n",
            "Epoch 19464: train loss: 0.009453226812183857\n",
            "Epoch 19465: train loss: 0.009452509693801403\n",
            "Epoch 19466: train loss: 0.009451786056160927\n",
            "Epoch 19467: train loss: 0.009451071731746197\n",
            "Epoch 19468: train loss: 0.009450352750718594\n",
            "Epoch 19469: train loss: 0.009449632838368416\n",
            "Epoch 19470: train loss: 0.009448918513953686\n",
            "Epoch 19471: train loss: 0.009448192082345486\n",
            "Epoch 19472: train loss: 0.00944747868925333\n",
            "Epoch 19473: train loss: 0.009446757845580578\n",
            "Epoch 19474: train loss: 0.009446043521165848\n",
            "Epoch 19475: train loss: 0.009445324540138245\n",
            "Epoch 19476: train loss: 0.00944460928440094\n",
            "Epoch 19477: train loss: 0.009443888440728188\n",
            "Epoch 19478: train loss: 0.009443174116313457\n",
            "Epoch 19479: train loss: 0.009442456997931004\n",
            "Epoch 19480: train loss: 0.009441738948225975\n",
            "Epoch 19481: train loss: 0.009441021829843521\n",
            "Epoch 19482: train loss: 0.009440300986170769\n",
            "Epoch 19483: train loss: 0.009439585730433464\n",
            "Epoch 19484: train loss: 0.009438869543373585\n",
            "Epoch 19485: train loss: 0.009438148699700832\n",
            "Epoch 19486: train loss: 0.009437431581318378\n",
            "Epoch 19487: train loss: 0.009436719119548798\n",
            "Epoch 19488: train loss: 0.009436002932488918\n",
            "Epoch 19489: train loss: 0.009435288608074188\n",
            "Epoch 19490: train loss: 0.009434573352336884\n",
            "Epoch 19491: train loss: 0.009433861821889877\n",
            "Epoch 19492: train loss: 0.009433140978217125\n",
            "Epoch 19493: train loss: 0.00943242758512497\n",
            "Epoch 19494: train loss: 0.009431709535419941\n",
            "Epoch 19495: train loss: 0.00943099893629551\n",
            "Epoch 19496: train loss: 0.00943028088659048\n",
            "Epoch 19497: train loss: 0.0094295684248209\n",
            "Epoch 19498: train loss: 0.009428855031728745\n",
            "Epoch 19499: train loss: 0.009428140707314014\n",
            "Epoch 19500: train loss: 0.00942743569612503\n",
            "Epoch 19501: train loss: 0.009426717646420002\n",
            "Epoch 19502: train loss: 0.009426004253327847\n",
            "Epoch 19503: train loss: 0.00942529272288084\n",
            "Epoch 19504: train loss: 0.009424574673175812\n",
            "Epoch 19505: train loss: 0.009423862211406231\n",
            "Epoch 19506: train loss: 0.0094231478869915\n",
            "Epoch 19507: train loss: 0.009422431699931622\n",
            "Epoch 19508: train loss: 0.00942172296345234\n",
            "Epoch 19509: train loss: 0.009421002119779587\n",
            "Epoch 19510: train loss: 0.009420297108590603\n",
            "Epoch 19511: train loss: 0.009419579990208149\n",
            "Epoch 19512: train loss: 0.009418871253728867\n",
            "Epoch 19513: train loss: 0.009418153204023838\n",
            "Epoch 19514: train loss: 0.009417443536221981\n",
            "Epoch 19515: train loss: 0.009416732005774975\n",
            "Epoch 19516: train loss: 0.009416017681360245\n",
            "Epoch 19517: train loss: 0.00941530242562294\n",
            "Epoch 19518: train loss: 0.00941458623856306\n",
            "Epoch 19519: train loss: 0.009413878433406353\n",
            "Epoch 19520: train loss: 0.009413164108991623\n",
            "Epoch 19521: train loss: 0.009412450715899467\n",
            "Epoch 19522: train loss: 0.009411740116775036\n",
            "Epoch 19523: train loss: 0.009411024861037731\n",
            "Epoch 19524: train loss: 0.009410311467945576\n",
            "Epoch 19525: train loss: 0.009409603662788868\n",
            "Epoch 19526: train loss: 0.009408890269696712\n",
            "Epoch 19527: train loss: 0.009408170357346535\n",
            "Epoch 19528: train loss: 0.0094074672088027\n",
            "Epoch 19529: train loss: 0.009406758472323418\n",
            "Epoch 19530: train loss: 0.009406049735844135\n",
            "Epoch 19531: train loss: 0.00940533634275198\n",
            "Epoch 19532: train loss: 0.009404626674950123\n",
            "Epoch 19533: train loss: 0.009403916075825691\n",
            "Epoch 19534: train loss: 0.009403204545378685\n",
            "Epoch 19535: train loss: 0.009402490220963955\n",
            "Epoch 19536: train loss: 0.009401786141097546\n",
            "Epoch 19537: train loss: 0.00940107274800539\n",
            "Epoch 19538: train loss: 0.009400363080203533\n",
            "Epoch 19539: train loss: 0.0093996562063694\n",
            "Epoch 19540: train loss: 0.009398947469890118\n",
            "Epoch 19541: train loss: 0.009398233145475388\n",
            "Epoch 19542: train loss: 0.009397528134286404\n",
            "Epoch 19543: train loss: 0.009396824985742569\n",
            "Epoch 19544: train loss: 0.009396105073392391\n",
            "Epoch 19545: train loss: 0.009395396336913109\n",
            "Epoch 19546: train loss: 0.009394686669111252\n",
            "Epoch 19547: train loss: 0.009393979795277119\n",
            "Epoch 19548: train loss: 0.009393264539539814\n",
            "Epoch 19549: train loss: 0.009392560459673405\n",
            "Epoch 19550: train loss: 0.00939185544848442\n",
            "Epoch 19551: train loss: 0.00939114298671484\n",
            "Epoch 19552: train loss: 0.009390439838171005\n",
            "Epoch 19553: train loss: 0.009389728307723999\n",
            "Epoch 19554: train loss: 0.009389019571244717\n",
            "Epoch 19555: train loss: 0.00938830990344286\n",
            "Epoch 19556: train loss: 0.009387603960931301\n",
            "Epoch 19557: train loss: 0.00938689149916172\n",
            "Epoch 19558: train loss: 0.009386185556650162\n",
            "Epoch 19559: train loss: 0.009385474026203156\n",
            "Epoch 19560: train loss: 0.009384765289723873\n",
            "Epoch 19561: train loss: 0.009384061209857464\n",
            "Epoch 19562: train loss: 0.009383353404700756\n",
            "Epoch 19563: train loss: 0.009382637217640877\n",
            "Epoch 19564: train loss: 0.009381932206451893\n",
            "Epoch 19565: train loss: 0.00938122533261776\n",
            "Epoch 19566: train loss: 0.009380517527461052\n",
            "Epoch 19567: train loss: 0.009379810653626919\n",
            "Epoch 19568: train loss: 0.009379101917147636\n",
            "Epoch 19569: train loss: 0.009378397837281227\n",
            "Epoch 19570: train loss: 0.009377686306834221\n",
            "Epoch 19571: train loss: 0.009376974776387215\n",
            "Epoch 19572: train loss: 0.009376272559165955\n",
            "Epoch 19573: train loss: 0.009375564754009247\n",
            "Epoch 19574: train loss: 0.00937485508620739\n",
            "Epoch 19575: train loss: 0.009374148212373257\n",
            "Epoch 19576: train loss: 0.009373435750603676\n",
            "Epoch 19577: train loss: 0.009372729808092117\n",
            "Epoch 19578: train loss: 0.009372026659548283\n",
            "Epoch 19579: train loss: 0.009371315129101276\n",
            "Epoch 19580: train loss: 0.009370606392621994\n",
            "Epoch 19581: train loss: 0.009369902312755585\n",
            "Epoch 19582: train loss: 0.009369198232889175\n",
            "Epoch 19583: train loss: 0.009368485771119595\n",
            "Epoch 19584: train loss: 0.009367778897285461\n",
            "Epoch 19585: train loss: 0.009367072954773903\n",
            "Epoch 19586: train loss: 0.009366370737552643\n",
            "Epoch 19587: train loss: 0.00936566200107336\n",
            "Epoch 19588: train loss: 0.009364953264594078\n",
            "Epoch 19589: train loss: 0.009364251047372818\n",
            "Epoch 19590: train loss: 0.00936354510486126\n",
            "Epoch 19591: train loss: 0.009362844750285149\n",
            "Epoch 19592: train loss: 0.009362137876451015\n",
            "Epoch 19593: train loss: 0.009361431002616882\n",
            "Epoch 19594: train loss: 0.009360725991427898\n",
            "Epoch 19595: train loss: 0.009360026568174362\n",
            "Epoch 19596: train loss: 0.009359315037727356\n",
            "Epoch 19597: train loss: 0.009358614683151245\n",
            "Epoch 19598: train loss: 0.009357909671962261\n",
            "Epoch 19599: train loss: 0.009357204660773277\n",
            "Epoch 19600: train loss: 0.009356499649584293\n",
            "Epoch 19601: train loss: 0.00935579277575016\n",
            "Epoch 19602: train loss: 0.009355091489851475\n",
            "Epoch 19603: train loss: 0.00935438647866249\n",
            "Epoch 19604: train loss: 0.009353681467473507\n",
            "Epoch 19605: train loss: 0.009352978318929672\n",
            "Epoch 19606: train loss: 0.009352271445095539\n",
            "Epoch 19607: train loss: 0.009351571090519428\n",
            "Epoch 19608: train loss: 0.009350864216685295\n",
            "Epoch 19609: train loss: 0.009350161999464035\n",
            "Epoch 19610: train loss: 0.009349456988275051\n",
            "Epoch 19611: train loss: 0.009348753839731216\n",
            "Epoch 19612: train loss: 0.009348050691187382\n",
            "Epoch 19613: train loss: 0.009347354993224144\n",
            "Epoch 19614: train loss: 0.009346652776002884\n",
            "Epoch 19615: train loss: 0.009345948696136475\n",
            "Epoch 19616: train loss: 0.009345253929495811\n",
            "Epoch 19617: train loss: 0.009344547055661678\n",
            "Epoch 19618: train loss: 0.009343845769762993\n",
            "Epoch 19619: train loss: 0.009343146346509457\n",
            "Epoch 19620: train loss: 0.009342449717223644\n",
            "Epoch 19621: train loss: 0.009341745637357235\n",
            "Epoch 19622: train loss: 0.009341041557490826\n",
            "Epoch 19623: train loss: 0.009340343065559864\n",
            "Epoch 19624: train loss: 0.009339647367596626\n",
            "Epoch 19625: train loss: 0.009338942356407642\n",
            "Epoch 19626: train loss: 0.00933824386447668\n",
            "Epoch 19627: train loss: 0.009337536990642548\n",
            "Epoch 19628: train loss: 0.00933683943003416\n",
            "Epoch 19629: train loss: 0.009336144663393497\n",
            "Epoch 19630: train loss: 0.009335445240139961\n",
            "Epoch 19631: train loss: 0.009334743954241276\n",
            "Epoch 19632: train loss: 0.009334040805697441\n",
            "Epoch 19633: train loss: 0.009333338588476181\n",
            "Epoch 19634: train loss: 0.009332643821835518\n",
            "Epoch 19635: train loss: 0.009331944398581982\n",
            "Epoch 19636: train loss: 0.009331238456070423\n",
            "Epoch 19637: train loss: 0.009330544620752335\n",
            "Epoch 19638: train loss: 0.009329845197498798\n",
            "Epoch 19639: train loss: 0.009329143911600113\n",
            "Epoch 19640: train loss: 0.009328446350991726\n",
            "Epoch 19641: train loss: 0.009327742271125317\n",
            "Epoch 19642: train loss: 0.009327040985226631\n",
            "Epoch 19643: train loss: 0.009326345287263393\n",
            "Epoch 19644: train loss: 0.009325648657977581\n",
            "Epoch 19645: train loss: 0.009324947372078896\n",
            "Epoch 19646: train loss: 0.00932424794882536\n",
            "Epoch 19647: train loss: 0.009323548525571823\n",
            "Epoch 19648: train loss: 0.009322848170995712\n",
            "Epoch 19649: train loss: 0.009322150610387325\n",
            "Epoch 19650: train loss: 0.009321452118456364\n",
            "Epoch 19651: train loss: 0.009320748038589954\n",
            "Epoch 19652: train loss: 0.00932005513459444\n",
            "Epoch 19653: train loss: 0.009319355711340904\n",
            "Epoch 19654: train loss: 0.009318660013377666\n",
            "Epoch 19655: train loss: 0.009317959658801556\n",
            "Epoch 19656: train loss: 0.009317259304225445\n",
            "Epoch 19657: train loss: 0.009316563606262207\n",
            "Epoch 19658: train loss: 0.009315859526395798\n",
            "Epoch 19659: train loss: 0.00931516569107771\n",
            "Epoch 19660: train loss: 0.009314468130469322\n",
            "Epoch 19661: train loss: 0.00931376963853836\n",
            "Epoch 19662: train loss: 0.009313070215284824\n",
            "Epoch 19663: train loss: 0.009312375448644161\n",
            "Epoch 19664: train loss: 0.009311679750680923\n",
            "Epoch 19665: train loss: 0.00931098498404026\n",
            "Epoch 19666: train loss: 0.009310287423431873\n",
            "Epoch 19667: train loss: 0.00930958054959774\n",
            "Epoch 19668: train loss: 0.009308882057666779\n",
            "Epoch 19669: train loss: 0.009308185428380966\n",
            "Epoch 19670: train loss: 0.009307488799095154\n",
            "Epoch 19671: train loss: 0.009306786581873894\n",
            "Epoch 19672: train loss: 0.009306089952588081\n",
            "Epoch 19673: train loss: 0.009305398911237717\n",
            "Epoch 19674: train loss: 0.009304697625339031\n",
            "Epoch 19675: train loss: 0.009304000064730644\n",
            "Epoch 19676: train loss: 0.00930330716073513\n",
            "Epoch 19677: train loss: 0.009302608668804169\n",
            "Epoch 19678: train loss: 0.009301908314228058\n",
            "Epoch 19679: train loss: 0.009301215410232544\n",
            "Epoch 19680: train loss: 0.009300519712269306\n",
            "Epoch 19681: train loss: 0.009299821220338345\n",
            "Epoch 19682: train loss: 0.009299131110310555\n",
            "Epoch 19683: train loss: 0.00929843820631504\n",
            "Epoch 19684: train loss: 0.00929773785173893\n",
            "Epoch 19685: train loss: 0.009297040291130543\n",
            "Epoch 19686: train loss: 0.009296347387135029\n",
            "Epoch 19687: train loss: 0.009295649826526642\n",
            "Epoch 19688: train loss: 0.00929495133459568\n",
            "Epoch 19689: train loss: 0.009294259361922741\n",
            "Epoch 19690: train loss: 0.009293567389249802\n",
            "Epoch 19691: train loss: 0.00929287075996399\n",
            "Epoch 19692: train loss: 0.009292174130678177\n",
            "Epoch 19693: train loss: 0.009291483089327812\n",
            "Epoch 19694: train loss: 0.009290786460042\n",
            "Epoch 19695: train loss: 0.009290093556046486\n",
            "Epoch 19696: train loss: 0.009289405308663845\n",
            "Epoch 19697: train loss: 0.009288706816732883\n",
            "Epoch 19698: train loss: 0.00928801205009222\n",
            "Epoch 19699: train loss: 0.009287326596677303\n",
            "Epoch 19700: train loss: 0.009286629036068916\n",
            "Epoch 19701: train loss: 0.009285935200750828\n",
            "Epoch 19702: train loss: 0.009285246953368187\n",
            "Epoch 19703: train loss: 0.009284554980695248\n",
            "Epoch 19704: train loss: 0.009283863939344883\n",
            "Epoch 19705: train loss: 0.009283168241381645\n",
            "Epoch 19706: train loss: 0.009282476268708706\n",
            "Epoch 19707: train loss: 0.009281781502068043\n",
            "Epoch 19708: train loss: 0.009281091392040253\n",
            "Epoch 19709: train loss: 0.009280400350689888\n",
            "Epoch 19710: train loss: 0.009279708378016949\n",
            "Epoch 19711: train loss: 0.009279010817408562\n",
            "Epoch 19712: train loss: 0.009278323501348495\n",
            "Epoch 19713: train loss: 0.009277631528675556\n",
            "Epoch 19714: train loss: 0.009276940487325191\n",
            "Epoch 19715: train loss: 0.009276245720684528\n",
            "Epoch 19716: train loss: 0.009275557473301888\n",
            "Epoch 19717: train loss: 0.009274863637983799\n",
            "Epoch 19718: train loss: 0.009274174459278584\n",
            "Epoch 19719: train loss: 0.009273482486605644\n",
            "Epoch 19720: train loss: 0.009272784925997257\n",
            "Epoch 19721: train loss: 0.009272095747292042\n",
            "Epoch 19722: train loss: 0.009271410293877125\n",
            "Epoch 19723: train loss: 0.009270713664591312\n",
            "Epoch 19724: train loss: 0.009270025417208672\n",
            "Epoch 19725: train loss: 0.009269332513213158\n",
            "Epoch 19726: train loss: 0.009268641471862793\n",
            "Epoch 19727: train loss: 0.009267953224480152\n",
            "Epoch 19728: train loss: 0.009267252869904041\n",
            "Epoch 19729: train loss: 0.009266563691198826\n",
            "Epoch 19730: train loss: 0.009265878237783909\n",
            "Epoch 19731: train loss: 0.009265182539820671\n",
            "Epoch 19732: train loss: 0.009264498017728329\n",
            "Epoch 19733: train loss: 0.009263802319765091\n",
            "Epoch 19734: train loss: 0.0092631159350276\n",
            "Epoch 19735: train loss: 0.00926242582499981\n",
            "Epoch 19736: train loss: 0.009261732921004295\n",
            "Epoch 19737: train loss: 0.009261039085686207\n",
            "Epoch 19738: train loss: 0.009260349906980991\n",
            "Epoch 19739: train loss: 0.009259660728275776\n",
            "Epoch 19740: train loss: 0.009258967824280262\n",
            "Epoch 19741: train loss: 0.009258276782929897\n",
            "Epoch 19742: train loss: 0.009257583878934383\n",
            "Epoch 19743: train loss: 0.009256893768906593\n",
            "Epoch 19744: train loss: 0.009256213903427124\n",
            "Epoch 19745: train loss: 0.009255524724721909\n",
            "Epoch 19746: train loss: 0.009254829958081245\n",
            "Epoch 19747: train loss: 0.009254144504666328\n",
            "Epoch 19748: train loss: 0.009253454394638538\n",
            "Epoch 19749: train loss: 0.009252763353288174\n",
            "Epoch 19750: train loss: 0.009252077899873257\n",
            "Epoch 19751: train loss: 0.009251384995877743\n",
            "Epoch 19752: train loss: 0.009250693023204803\n",
            "Epoch 19753: train loss: 0.009250007569789886\n",
            "Epoch 19754: train loss: 0.009249317459762096\n",
            "Epoch 19755: train loss: 0.009248628281056881\n",
            "Epoch 19756: train loss: 0.009247939102351665\n",
            "Epoch 19757: train loss: 0.009247250854969025\n",
            "Epoch 19758: train loss: 0.00924656167626381\n",
            "Epoch 19759: train loss: 0.009245872497558594\n",
            "Epoch 19760: train loss: 0.009245190769433975\n",
            "Epoch 19761: train loss: 0.00924450159072876\n",
            "Epoch 19762: train loss: 0.00924380961805582\n",
            "Epoch 19763: train loss: 0.009243124164640903\n",
            "Epoch 19764: train loss: 0.009242436848580837\n",
            "Epoch 19765: train loss: 0.009241748601198196\n",
            "Epoch 19766: train loss: 0.00924106128513813\n",
            "Epoch 19767: train loss: 0.009240373969078064\n",
            "Epoch 19768: train loss: 0.00923968106508255\n",
            "Epoch 19769: train loss: 0.00923899281769991\n",
            "Epoch 19770: train loss: 0.009238310158252716\n",
            "Epoch 19771: train loss: 0.0092376209795475\n",
            "Epoch 19772: train loss: 0.009236934594810009\n",
            "Epoch 19773: train loss: 0.009236248210072517\n",
            "Epoch 19774: train loss: 0.009235563687980175\n",
            "Epoch 19775: train loss: 0.009234869852662086\n",
            "Epoch 19776: train loss: 0.009234185330569744\n",
            "Epoch 19777: train loss: 0.009233498945832253\n",
            "Epoch 19778: train loss: 0.009232810698449612\n",
            "Epoch 19779: train loss: 0.009232128039002419\n",
            "Epoch 19780: train loss: 0.009231439791619778\n",
            "Epoch 19781: train loss: 0.009230753406882286\n",
            "Epoch 19782: train loss: 0.009230068884789944\n",
            "Epoch 19783: train loss: 0.009229391813278198\n",
            "Epoch 19784: train loss: 0.00922870822250843\n",
            "Epoch 19785: train loss: 0.009228024631738663\n",
            "Epoch 19786: train loss: 0.009227337315678596\n",
            "Epoch 19787: train loss: 0.00922665186226368\n",
            "Epoch 19788: train loss: 0.009225966408848763\n",
            "Epoch 19789: train loss: 0.009225290268659592\n",
            "Epoch 19790: train loss: 0.009224609471857548\n",
            "Epoch 19791: train loss: 0.009223917499184608\n",
            "Epoch 19792: train loss: 0.00922323763370514\n",
            "Epoch 19793: train loss: 0.00922255590558052\n",
            "Epoch 19794: train loss: 0.009221872314810753\n",
            "Epoch 19795: train loss: 0.009221194311976433\n",
            "Epoch 19796: train loss: 0.009220508858561516\n",
            "Epoch 19797: train loss: 0.009219827130436897\n",
            "Epoch 19798: train loss: 0.009219137951731682\n",
            "Epoch 19799: train loss: 0.009218459017574787\n",
            "Epoch 19800: train loss: 0.009217780083417892\n",
            "Epoch 19801: train loss: 0.0092170936986804\n",
            "Epoch 19802: train loss: 0.009216408245265484\n",
            "Epoch 19803: train loss: 0.009215730242431164\n",
            "Epoch 19804: train loss: 0.009215046651661396\n",
            "Epoch 19805: train loss: 0.009214364923536777\n",
            "Epoch 19806: train loss: 0.009213683195412159\n",
            "Epoch 19807: train loss: 0.009213007055222988\n",
            "Epoch 19808: train loss: 0.009212317876517773\n",
            "Epoch 19809: train loss: 0.009211636148393154\n",
            "Epoch 19810: train loss: 0.009210950694978237\n",
            "Epoch 19811: train loss: 0.009210269898176193\n",
            "Epoch 19812: train loss: 0.009209588170051575\n",
            "Epoch 19813: train loss: 0.00920891109853983\n",
            "Epoch 19814: train loss: 0.00920821912586689\n",
            "Epoch 19815: train loss: 0.009207542054355145\n",
            "Epoch 19816: train loss: 0.0092068612575531\n",
            "Epoch 19817: train loss: 0.009206177666783333\n",
            "Epoch 19818: train loss: 0.009205495938658714\n",
            "Epoch 19819: train loss: 0.00920481700450182\n",
            "Epoch 19820: train loss: 0.009204132482409477\n",
            "Epoch 19821: train loss: 0.009203454479575157\n",
            "Epoch 19822: train loss: 0.009202775545418262\n",
            "Epoch 19823: train loss: 0.00920209288597107\n",
            "Epoch 19824: train loss: 0.00920141115784645\n",
            "Epoch 19825: train loss: 0.00920072291046381\n",
            "Epoch 19826: train loss: 0.009200045838952065\n",
            "Epoch 19827: train loss: 0.009199368767440319\n",
            "Epoch 19828: train loss: 0.009198685176670551\n",
            "Epoch 19829: train loss: 0.009197999723255634\n",
            "Epoch 19830: train loss: 0.009197316132485867\n",
            "Epoch 19831: train loss: 0.009196636267006397\n",
            "Epoch 19832: train loss: 0.009195958264172077\n",
            "Epoch 19833: train loss: 0.009195279330015182\n",
            "Epoch 19834: train loss: 0.00919459667056799\n",
            "Epoch 19835: train loss: 0.009193913079798222\n",
            "Epoch 19836: train loss: 0.009193231351673603\n",
            "Epoch 19837: train loss: 0.009192555211484432\n",
            "Epoch 19838: train loss: 0.009191876277327538\n",
            "Epoch 19839: train loss: 0.00919119268655777\n",
            "Epoch 19840: train loss: 0.009190513752400875\n",
            "Epoch 19841: train loss: 0.009189833886921406\n",
            "Epoch 19842: train loss: 0.009189150296151638\n",
            "Epoch 19843: train loss: 0.009188472293317318\n",
            "Epoch 19844: train loss: 0.009187791496515274\n",
            "Epoch 19845: train loss: 0.009187113493680954\n",
            "Epoch 19846: train loss: 0.009186435490846634\n",
            "Epoch 19847: train loss: 0.009185750968754292\n",
            "Epoch 19848: train loss: 0.009185071103274822\n",
            "Epoch 19849: train loss: 0.009184394031763077\n",
            "Epoch 19850: train loss: 0.009183707647025585\n",
            "Epoch 19851: train loss: 0.009183026850223541\n",
            "Epoch 19852: train loss: 0.009182353504002094\n",
            "Epoch 19853: train loss: 0.009181671775877476\n",
            "Epoch 19854: train loss: 0.009180990047752857\n",
            "Epoch 19855: train loss: 0.009180311113595963\n",
            "Epoch 19856: train loss: 0.009179634973406792\n",
            "Epoch 19857: train loss: 0.009178955107927322\n",
            "Epoch 19858: train loss: 0.00917827058583498\n",
            "Epoch 19859: train loss: 0.009177597239613533\n",
            "Epoch 19860: train loss: 0.009176918305456638\n",
            "Epoch 19861: train loss: 0.009176235646009445\n",
            "Epoch 19862: train loss: 0.009175561368465424\n",
            "Epoch 19863: train loss: 0.009174876846373081\n",
            "Epoch 19864: train loss: 0.009174205362796783\n",
            "Epoch 19865: train loss: 0.009173526428639889\n",
            "Epoch 19866: train loss: 0.009172843769192696\n",
            "Epoch 19867: train loss: 0.009172172285616398\n",
            "Epoch 19868: train loss: 0.009171494282782078\n",
            "Epoch 19869: train loss: 0.009170813485980034\n",
            "Epoch 19870: train loss: 0.009170139208436012\n",
            "Epoch 19871: train loss: 0.009169463068246841\n",
            "Epoch 19872: train loss: 0.009168783202767372\n",
            "Epoch 19873: train loss: 0.0091681107878685\n",
            "Epoch 19874: train loss: 0.009167431853711605\n",
            "Epoch 19875: train loss: 0.009166751988232136\n",
            "Epoch 19876: train loss: 0.009166082367300987\n",
            "Epoch 19877: train loss: 0.009165401570498943\n",
            "Epoch 19878: train loss: 0.009164731949567795\n",
            "Epoch 19879: train loss: 0.009164056740701199\n",
            "Epoch 19880: train loss: 0.009163379669189453\n",
            "Epoch 19881: train loss: 0.009162708185613155\n",
            "Epoch 19882: train loss: 0.009162036702036858\n",
            "Epoch 19883: train loss: 0.009161357767879963\n",
            "Epoch 19884: train loss: 0.009160683490335941\n",
            "Epoch 19885: train loss: 0.009160014800727367\n",
            "Epoch 19886: train loss: 0.009159340523183346\n",
            "Epoch 19887: train loss: 0.009158671833574772\n",
            "Epoch 19888: train loss: 0.009157989174127579\n",
            "Epoch 19889: train loss: 0.009157325141131878\n",
            "Epoch 19890: train loss: 0.00915664341300726\n",
            "Epoch 19891: train loss: 0.009155970998108387\n",
            "Epoch 19892: train loss: 0.009155303239822388\n",
            "Epoch 19893: train loss: 0.009154624305665493\n",
            "Epoch 19894: train loss: 0.00915395375341177\n",
            "Epoch 19895: train loss: 0.00915327575057745\n",
            "Epoch 19896: train loss: 0.009152611717581749\n",
            "Epoch 19897: train loss: 0.00915192998945713\n",
            "Epoch 19898: train loss: 0.009151261299848557\n",
            "Epoch 19899: train loss: 0.009150588884949684\n",
            "Epoch 19900: train loss: 0.009149911813437939\n",
            "Epoch 19901: train loss: 0.009149243123829365\n",
            "Epoch 19902: train loss: 0.00914856418967247\n",
            "Epoch 19903: train loss: 0.00914789829403162\n",
            "Epoch 19904: train loss: 0.009147225879132748\n",
            "Epoch 19905: train loss: 0.009146549738943577\n",
            "Epoch 19906: train loss: 0.009145879186689854\n",
            "Epoch 19907: train loss: 0.009145203977823257\n",
            "Epoch 19908: train loss: 0.009144526906311512\n",
            "Epoch 19909: train loss: 0.009143861010670662\n",
            "Epoch 19910: train loss: 0.009143191389739513\n",
            "Epoch 19911: train loss: 0.00914251059293747\n",
            "Epoch 19912: train loss: 0.009141839109361172\n",
            "Epoch 19913: train loss: 0.009141172282397747\n",
            "Epoch 19914: train loss: 0.009140501730144024\n",
            "Epoch 19915: train loss: 0.00913982093334198\n",
            "Epoch 19916: train loss: 0.00913915503770113\n",
            "Epoch 19917: train loss: 0.009138482622802258\n",
            "Epoch 19918: train loss: 0.00913781113922596\n",
            "Epoch 19919: train loss: 0.009137138724327087\n",
            "Epoch 19920: train loss: 0.009136462584137917\n",
            "Epoch 19921: train loss: 0.009135794825851917\n",
            "Epoch 19922: train loss: 0.009135116823017597\n",
            "Epoch 19923: train loss: 0.009134447202086449\n",
            "Epoch 19924: train loss: 0.00913377944380045\n",
            "Epoch 19925: train loss: 0.009133107960224152\n",
            "Epoch 19926: train loss: 0.009132437407970428\n",
            "Epoch 19927: train loss: 0.009131764061748981\n",
            "Epoch 19928: train loss: 0.00913108978420496\n",
            "Epoch 19929: train loss: 0.009130422957241535\n",
            "Epoch 19930: train loss: 0.009129748679697514\n",
            "Epoch 19931: train loss: 0.009129076264798641\n",
            "Epoch 19932: train loss: 0.009128406643867493\n",
            "Epoch 19933: train loss: 0.00912773422896862\n",
            "Epoch 19934: train loss: 0.009127061814069748\n",
            "Epoch 19935: train loss: 0.009126391261816025\n",
            "Epoch 19936: train loss: 0.009125723503530025\n",
            "Epoch 19937: train loss: 0.009125051088631153\n",
            "Epoch 19938: train loss: 0.009124383330345154\n",
            "Epoch 19939: train loss: 0.009123709052801132\n",
            "Epoch 19940: train loss: 0.009123042225837708\n",
            "Epoch 19941: train loss: 0.009122372604906559\n",
            "Epoch 19942: train loss: 0.009121698327362537\n",
            "Epoch 19943: train loss: 0.009121030569076538\n",
            "Epoch 19944: train loss: 0.009120355360209942\n",
            "Epoch 19945: train loss: 0.009119685739278793\n",
            "Epoch 19946: train loss: 0.009119018912315369\n",
            "Epoch 19947: train loss: 0.009118346497416496\n",
            "Epoch 19948: train loss: 0.009117679670453072\n",
            "Epoch 19949: train loss: 0.0091170072555542\n",
            "Epoch 19950: train loss: 0.009116336703300476\n",
            "Epoch 19951: train loss: 0.009115670807659626\n",
            "Epoch 19952: train loss: 0.009115004912018776\n",
            "Epoch 19953: train loss: 0.009114323183894157\n",
            "Epoch 19954: train loss: 0.009113659150898457\n",
            "Epoch 19955: train loss: 0.009112988598644733\n",
            "Epoch 19956: train loss: 0.009112328290939331\n",
            "Epoch 19957: train loss: 0.009111663326621056\n",
            "Epoch 19958: train loss: 0.009110989980399609\n",
            "Epoch 19959: train loss: 0.00911032222211361\n",
            "Epoch 19960: train loss: 0.009109653532505035\n",
            "Epoch 19961: train loss: 0.00910898670554161\n",
            "Epoch 19962: train loss: 0.009108314290642738\n",
            "Epoch 19963: train loss: 0.009107651188969612\n",
            "Epoch 19964: train loss: 0.009106981568038464\n",
            "Epoch 19965: train loss: 0.009106315672397614\n",
            "Epoch 19966: train loss: 0.009105647914111614\n",
            "Epoch 19967: train loss: 0.009104977361857891\n",
            "Epoch 19968: train loss: 0.00910431332886219\n",
            "Epoch 19969: train loss: 0.009103648364543915\n",
            "Epoch 19970: train loss: 0.009102978743612766\n",
            "Epoch 19971: train loss: 0.009102310985326767\n",
            "Epoch 19972: train loss: 0.009101640433073044\n",
            "Epoch 19973: train loss: 0.009100972674787045\n",
            "Epoch 19974: train loss: 0.009100310504436493\n",
            "Epoch 19975: train loss: 0.00909963995218277\n",
            "Epoch 19976: train loss: 0.009098976850509644\n",
            "Epoch 19977: train loss: 0.009098305366933346\n",
            "Epoch 19978: train loss: 0.009097638539969921\n",
            "Epoch 19979: train loss: 0.009096971713006496\n",
            "Epoch 19980: train loss: 0.009096315130591393\n",
            "Epoch 19981: train loss: 0.009095643647015095\n",
            "Epoch 19982: train loss: 0.009094974026083946\n",
            "Epoch 19983: train loss: 0.009094314649701118\n",
            "Epoch 19984: train loss: 0.009093652479350567\n",
            "Epoch 19985: train loss: 0.009092988446354866\n",
            "Epoch 19986: train loss: 0.009092320688068867\n",
            "Epoch 19987: train loss: 0.009091654792428017\n",
            "Epoch 19988: train loss: 0.009090988896787167\n",
            "Epoch 19989: train loss: 0.009090319275856018\n",
            "Epoch 19990: train loss: 0.00908966176211834\n",
            "Epoch 19991: train loss: 0.009088993072509766\n",
            "Epoch 19992: train loss: 0.009088322520256042\n",
            "Epoch 19993: train loss: 0.009087665006518364\n",
            "Epoch 19994: train loss: 0.009087002836167812\n",
            "Epoch 19995: train loss: 0.00908632855862379\n",
            "Epoch 19996: train loss: 0.009085670113563538\n",
            "Epoch 19997: train loss: 0.009085004217922688\n",
            "Epoch 19998: train loss: 0.009084347635507584\n",
            "Epoch 19999: train loss: 0.009083676151931286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "#train\n",
        "y_pred = model(x_train) #predict\n",
        "y_pred=(y_pred>0.5).int().flatten() #argmax class lable\n",
        "train_acc =torch.sum(y_pred == y_train.int())/ y_train.shape[0]\n",
        "print(\"train ACC: \",train_acc.float())"
      ],
      "metadata": {
        "id": "yXh19WH5JezP",
        "outputId": "77e5d5aa-13ac-4688-8fcf-cfdcde562496",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 481,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train ACC:  tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "y_pred = model(x_test) #predict\n",
        "y_pred = (y_pred>0.5).int().flatten() #argmax class lable\n",
        "test_acc = torch.sum(y_pred == y_test.int()) / y_test.shape[0]\n",
        "print(\"test ACC: \",test_acc.float())"
      ],
      "metadata": {
        "id": "1lNr1QnpKopY",
        "outputId": "95f20a67-27e0-415f-960b-347f9c4c0f36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 482,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test ACC:  tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(x_test[:,0], x_test[:,1], c=y_pred)\n",
        "plt.colorbar()"
      ],
      "metadata": {
        "id": "XzxvYA1oKp64",
        "outputId": "ad88fc15-3877-4b8c-8737-56678e0f6fec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        }
      },
      "execution_count": 483,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.colorbar.Colorbar at 0x7e78ccaf7050>"
            ]
          },
          "metadata": {},
          "execution_count": 483
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Assignment_Basic_MLP_in_Pytorch.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}