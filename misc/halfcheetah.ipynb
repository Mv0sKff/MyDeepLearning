{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mv0sKff/MyDeepLearning/blob/main/misc/halfcheetah.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0d7acd9-e134-4599-b0ad-95c2f652f2a5",
      "metadata": {
        "id": "c0d7acd9-e134-4599-b0ad-95c2f652f2a5"
      },
      "source": [
        "# Deep Reinforcement Learning with Stable Baselines 3\n",
        "\n",
        "In this exercise, you'll train a walking behavior using stable baselines, a library made for deep reinforcement learning."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"MUJOCO_GL\"] = \"egl\"\n",
        "os.environ[\"EGL_DEVICE_ID\"] = \"0\""
      ],
      "metadata": {
        "id": "DgbPuBjHjCd_"
      },
      "id": "DgbPuBjHjCd_",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium==0.28.1 gymnasium[mujoco] stable_baselines3[extra]==2.0.0 -q"
      ],
      "metadata": {
        "id": "XxdMfPAXZzd0"
      },
      "id": "XxdMfPAXZzd0",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "d6f30340-4197-42a6-b488-4ca44e316fcd",
      "metadata": {
        "id": "d6f30340-4197-42a6-b488-4ca44e316fcd"
      },
      "source": [
        "First, let's import the necessary parts of the libraries for this exercise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "722a690a-8d49-4fa4-a9af-0caf43f00151",
      "metadata": {
        "tags": [],
        "id": "722a690a-8d49-4fa4-a9af-0caf43f00151"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.monitor import Monitor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a0845db-74f8-4a3a-b6fd-da5b275c91c9",
      "metadata": {
        "id": "1a0845db-74f8-4a3a-b6fd-da5b275c91c9"
      },
      "source": [
        "**Important:** should this step yield any errors, check if this notebook runs in the correct conda environment. Without the imports working, none of the following steps will work!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6df99342-99ea-4afa-889e-b896f9b41c39",
      "metadata": {
        "id": "6df99342-99ea-4afa-889e-b896f9b41c39"
      },
      "source": [
        "Next, we need to create the environments for which we want to learn a walking behavior. The goal of this environment is to make the robot move to the right side (positive x direction). For more information about the HalfCheetah environment, visit https://gymnasium.farama.org/environments/mujoco/half_cheetah/."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "f7d300aa-5f29-4f04-b014-f5ba528ca8f8",
      "metadata": {
        "tags": [],
        "id": "f7d300aa-5f29-4f04-b014-f5ba528ca8f8"
      },
      "outputs": [],
      "source": [
        "env_id = \"HalfCheetah-v4\"\n",
        "env_raw = gym.make(env_id, render_mode=\"rgb_array\")\n",
        "# A seperate environment is required for evaluation\n",
        "eval_env_raw = gym.make(env_id, render_mode=\"rgb_array\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d21dde0d-5197-47ec-9c93-59b38ee45cf2",
      "metadata": {
        "id": "d21dde0d-5197-47ec-9c93-59b38ee45cf2"
      },
      "source": [
        "With Deep Reinforcement Learning, an agent tries to maximize the reward it gets for the actions he performs. Your task is now to write a function to calculate the reward for the agent.\n",
        "\n",
        "The following are the most important values that you can use for calculating the reward.\n",
        "- `action` is an array containing the actions for the agent. The documentation linked above explains what these values represent.\n",
        "- Use `self.env.data.xpos` to get the positions of the robot's body parts. As calling the \"inner\" environment's `step` method advances the simulation using the generated action, it makes a difference if you obtain those positions before calling `step` or after. This is a two-dimensional environment. Therefore, `self.env.data.xpos[?][1]` (the y-coordinate) is of no interest for all body parts. The following information is taken from the definition of the environment: https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/mujoco/assets/half_cheetah.xml. The body parts should usually appear in the same order in the array as defined in the XML file using `<body>` tags.\n",
        "  * `self.env.data.xpos[1]`: torso\n",
        "  * `self.env.data.xpos[2]`: back thigh\n",
        "  * `self.env.data.xpos[3]`: back shin\n",
        "  * `self.env.data.xpos[4]`: back foot\n",
        "  * `self.env.data.xpos[5]`: front thigh\n",
        "  * `self.env.data.xpos[6]`: front shin\n",
        "  * `self.env.data.xpos[7]`: front foot\n",
        "- `self.dt` contains the time elapsed during the simulation step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "50b1cf89-4f57-4ff4-8b25-19cd899e70fc",
      "metadata": {
        "id": "50b1cf89-4f57-4ff4-8b25-19cd899e70fc"
      },
      "outputs": [],
      "source": [
        "class HalfCheetahRewardWrapper(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "\n",
        "    def step(self, action):\n",
        "        x_position_torso_before = self.env.data.xpos[1][0]\n",
        "\n",
        "        # Run the simulation and apply the action\n",
        "        observation, _, terminated, truncated, info = self.env.step(action)\n",
        "\n",
        "        x_position_torso_after = self.env.data.xpos[1][0]\n",
        "\n",
        "        # TODO: calculate the reward here\n",
        "        reward = 0\n",
        "\n",
        "        return observation, reward, terminated, truncated, info\n",
        "\n",
        "# Wrap the existing environments\n",
        "env = HalfCheetahRewardWrapper(env_raw)\n",
        "eval_env = Monitor(HalfCheetahRewardWrapper(eval_env_raw))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acc0e7b0-8a04-44dc-ad3c-049ee012bfc7",
      "metadata": {
        "id": "acc0e7b0-8a04-44dc-ad3c-049ee012bfc7"
      },
      "source": [
        "Now, we need to create the model. It is initialized with a random policy.\n",
        "For this exercise, we'll use the PPO (Proximal Policy Optimization) algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "9f4bf73a-52c9-4d85-8219-632811776a88",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f4bf73a-52c9-4d85-8219-632811776a88",
        "outputId": "a8a6cc31-42a4-49bf-cae8-d0b51d3c1368"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n"
          ]
        }
      ],
      "source": [
        "model = PPO(\"MlpPolicy\", env, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c861c79-7203-40b6-b0cb-38e73f2e5e4c",
      "metadata": {
        "id": "5c861c79-7203-40b6-b0cb-38e73f2e5e4c"
      },
      "source": [
        "If you'd like to see how random policy performs, you can comment out the following block:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "d382dac1-c1bc-4021-9ed8-803915297a86",
      "metadata": {
        "tags": [],
        "id": "d382dac1-c1bc-4021-9ed8-803915297a86"
      },
      "outputs": [],
      "source": [
        "# mean_reward, std_deviation = evaluate_policy(model, eval_env, n_eval_episodes=100)\n",
        "# print(\"mean reward:\", mean_reward)\n",
        "# print(\"standard deviation:\", std_deviation)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e6dce8f-d694-41b2-a275-ab6cb001dd54",
      "metadata": {
        "id": "8e6dce8f-d694-41b2-a275-ab6cb001dd54"
      },
      "source": [
        "Now, it's time for learning how to walk. You can adjust the number of timesteps to learn on based on the strength of your computer. More timesteps take longer but may improve the resulting behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "beae75e2-5743-44d7-9d9b-9ea48978c362",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beae75e2-5743-44d7-9d9b-9ea48978c362",
        "outputId": "e64b47bb-91b4-4f6e-90b0-c25ec7713c44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 0        |\n",
            "| time/              |          |\n",
            "|    fps             | 992      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 2        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines3.ppo.ppo.PPO at 0x7891e02a4ed0>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "#model.learn(total_timesteps=200000)\n",
        "model.learn(total_timesteps=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0575cf68-565d-4af5-9fe8-84e815e6ff36",
      "metadata": {
        "id": "0575cf68-565d-4af5-9fe8-84e815e6ff36"
      },
      "source": [
        "Again, you can comment out the following block to measure if an improvement was made to the model compared to its initial random state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "347df854-1b69-425c-8c73-4b42d008817e",
      "metadata": {
        "tags": [],
        "id": "347df854-1b69-425c-8c73-4b42d008817e"
      },
      "outputs": [],
      "source": [
        "# mean_reward, std_deviation = evaluate_policy(model, eval_env, n_eval_episodes=100)\n",
        "# print(\"mean reward:\", mean_reward)\n",
        "# print(\"standard deviation:\", std_deviation)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc8ecbe2-46a5-40d1-b2b4-39da72550348",
      "metadata": {
        "id": "cc8ecbe2-46a5-40d1-b2b4-39da72550348"
      },
      "source": [
        "You can export a GIF of your trained behavior using the following code."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import imageio\n",
        "import numpy as np\n",
        "\n",
        "filename = \"halfcheetah.gif\"\n",
        "\n",
        "images = []\n",
        "obs = model.env.reset()\n",
        "img = model.env.render()\n",
        "start_pos = model.env.envs[0].data.xpos[1][0]\n",
        "for _ in range(200):\n",
        "    images.append(img)\n",
        "    action, _ = model.predict(obs)\n",
        "    obs, _, _, _ = model.env.step(action)\n",
        "    img = model.env.render()\n",
        "\n",
        "end_pos = model.env.envs[0].data.xpos[1][0]\n",
        "print(f\"Distance travelled: {end_pos - start_pos}m\")\n",
        "\n",
        "imageio.mimsave(filename, [np.array(img) for img in images], duration=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "Soy3vFrMh2bR",
        "outputId": "edbe71c7-3869-4c23-9767-236f855ca80a"
      },
      "id": "Soy3vFrMh2bR",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distance travelled: 1.6435030432231437m\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-4a10d6732ba8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Distance travelled: {end_pos - start_pos}m\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mimageio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmimsave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mduration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/imageio/v2.py\u001b[0m in \u001b[0;36mmimwrite\u001b[0;34m(uri, ims, format, **kwargs)\u001b[0m\n\u001b[1;32m    492\u001b[0m     \u001b[0mimopen_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecypher_format_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0mimopen_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"legacy_mode\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mimopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wI\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mimopen_args\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/imageio/core/v3_plugin_api.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__del__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/imageio/plugins/pillow.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flush_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_image\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/imageio/plugins/pillow.py\u001b[0m in \u001b[0;36m_flush_writer\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    483\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"append_images\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages_to_write\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 485\u001b[0;31m         \u001b[0mprimary_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages_to_write\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2580\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2581\u001b[0;31m             \u001b[0msave_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2582\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2583\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mopen_fp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/GifImagePlugin.py\u001b[0m in \u001b[0;36m_save_all\u001b[0;34m(im, fp, filename)\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_save_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m     \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/GifImagePlugin.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(im, fp, filename, save_all)\u001b[0m\n\u001b[1;32m    795\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoderinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"optimize\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msave_all\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_write_multiple_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpalette\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m         \u001b[0m_write_single_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpalette\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/GifImagePlugin.py\u001b[0m in \u001b[0;36m_write_multiple_frames\u001b[0;34m(im, fp, palette)\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mim_frame\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mImageSequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m             \u001b[0;31m# a copy is required here since seek can still mutate the image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 666\u001b[0;31m             \u001b[0mim_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_normalize_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    667\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mframe_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mim_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/GifImagePlugin.py\u001b[0m in \u001b[0;36m_normalize_mode\u001b[0;34m(im)\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetmodebase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m         \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"P\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpalette\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPalette\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mADAPTIVE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpalette\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpalette\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"RGBA\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"P\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mpalette\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPalette\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mADAPTIVE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1093\u001b[0;31m             \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1094\u001b[0m             \u001b[0mnew_im\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImagePalette\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ecb1b3d-fb55-4f30-be2c-41347e017b13",
      "metadata": {
        "id": "4ecb1b3d-fb55-4f30-be2c-41347e017b13"
      },
      "source": [
        "Within the visualization, one side of a (black or white) square is $\\frac{2}{3}$ meters long."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9d3b41f-d0ab-4bad-8a36-ee3175bd3c3c",
      "metadata": {
        "id": "d9d3b41f-d0ab-4bad-8a36-ee3175bd3c3c"
      },
      "source": [
        "If you are interested in training robots in other environments, check out the already existing environments in Gymnasium: https://gymnasium.farama.org/environments/mujoco/.\n",
        "You should be able to use another environment id and everything else in the notebook should mostly continue to work. Of course, you then need to find out yourself what the positions in `self.env.data.xpos` represent for your chosen environment.\n",
        "Note: as of writing this (August 2023), there seem to be some problems with the Hopper and the Walker2D environment that prevent these environments from being used."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d885a1e",
      "metadata": {
        "id": "6d885a1e"
      },
      "source": [
        "Now let's see how high the robot is able to jump. Once again, we need to define a reward function.\n",
        "It might make sense to manually set `terminated` and `truncated` in this case.\n",
        "Have a look at https://gymnasium.farama.org/api/env/#gymnasium.Env.step for an explanation of their meaning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "75a8891d",
      "metadata": {
        "id": "75a8891d"
      },
      "outputs": [],
      "source": [
        "class HalfCheetahJumpRewardWrapper(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "\n",
        "    def step(self, action):\n",
        "        x_position_torso_before = self.env.unwrapped.data.xpos[1][0]\n",
        "\n",
        "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
        "\n",
        "        x_position_torso_after = self.env.unwrapped.data.xpos[1][0]\n",
        "        custom_reward = x_position_torso_after - x_position_torso_before\n",
        "\n",
        "        return obs, custom_reward, terminated, truncated, info\n",
        "\n",
        "# Wrap the existing environments\n",
        "jump_env = HalfCheetahJumpRewardWrapper(env_raw)\n",
        "jump_eval_env = Monitor(HalfCheetahJumpRewardWrapper(eval_env_raw))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42e3dc40",
      "metadata": {
        "id": "42e3dc40"
      },
      "source": [
        "Again, we need to create a new model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "2937a633",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2937a633",
        "outputId": "c29fca3c-d2a9-48aa-f3b1-ceb9720f4dba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n"
          ]
        }
      ],
      "source": [
        "jump_model = PPO(\"MlpPolicy\", jump_env, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "210b78c7",
      "metadata": {
        "id": "210b78c7"
      },
      "source": [
        "To measure the performance of the random policy, comment out the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "a089a18c",
      "metadata": {
        "tags": [],
        "id": "a089a18c"
      },
      "outputs": [],
      "source": [
        "# mean_reward, std_deviation = evaluate_policy(jump_model, jump_eval_env, n_eval_episodes=100)\n",
        "# print(\"mean reward:\", mean_reward)\n",
        "# print(\"standard deviation:\", std_deviation)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6045c7df",
      "metadata": {
        "id": "6045c7df"
      },
      "source": [
        "Train the network:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "99c63314",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99c63314",
        "outputId": "bd404730-871f-475d-c787-b3577f067f25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | -8.63    |\n",
            "| time/              |          |\n",
            "|    fps             | 1031     |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 1        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines3.ppo.ppo.PPO at 0x7891e0189550>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "jump_model.learn(total_timesteps=200) #200000"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5defac0e",
      "metadata": {
        "id": "5defac0e"
      },
      "source": [
        "As usual, you can run an evaluation to measure the new result:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "d239ff47",
      "metadata": {
        "tags": [],
        "id": "d239ff47"
      },
      "outputs": [],
      "source": [
        "# mean_reward, std_deviation = evaluate_policy(jump_model, jump_eval_env, n_eval_episodes=100)\n",
        "# print(\"mean reward:\", mean_reward)\n",
        "# print(\"standard deviation:\", std_deviation)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da6899bf",
      "metadata": {
        "id": "da6899bf"
      },
      "source": [
        "Of course, you can also export another GIF of your trained jumping behavior using the following code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "e16cc34b",
      "metadata": {
        "tags": [],
        "id": "e16cc34b"
      },
      "outputs": [],
      "source": [
        "import imageio\n",
        "import numpy as np\n",
        "\n",
        "filename = \"halfcheetah_jump.gif\"\n",
        "\n",
        "images = []\n",
        "obs = jump_model.env.reset()\n",
        "img = jump_model.env.render()\n",
        "for _ in range(100):\n",
        "    images.append(img)\n",
        "    action, _ = jump_model.predict(obs)\n",
        "    obs, _, _, _ = jump_model.env.step(action)\n",
        "    img = jump_model.env.render()\n",
        "\n",
        "imageio.mimsave(filename, [np.array(img) for img in images], duration=50)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:drl3]",
      "language": "python",
      "name": "conda-env-drl3-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}