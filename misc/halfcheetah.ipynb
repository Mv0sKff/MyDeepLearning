{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mv0sKff/MyDeepLearning/blob/main/misc/halfcheetah.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0d7acd9-e134-4599-b0ad-95c2f652f2a5",
      "metadata": {
        "id": "c0d7acd9-e134-4599-b0ad-95c2f652f2a5"
      },
      "source": [
        "# Deep Reinforcement Learning with Stable Baselines 3\n",
        "\n",
        "In this exercise, you'll train a walking behavior using stable baselines, a library made for deep reinforcement learning."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"MUJOCO_GL\"] = \"egl\"\n",
        "os.environ[\"EGL_DEVICE_ID\"] = \"0\""
      ],
      "metadata": {
        "id": "DgbPuBjHjCd_"
      },
      "id": "DgbPuBjHjCd_",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium==0.28.1 gymnasium[mujoco] stable_baselines3[extra]==2.0.0 -q"
      ],
      "metadata": {
        "id": "XxdMfPAXZzd0",
        "outputId": "c287e4ed-fc3e-4415-f475-9998a92d29d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "XxdMfPAXZzd0",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/434.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m925.5/925.5 kB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.4/178.4 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m114.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m131.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m98.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m109.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.5/243.5 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.1.2 requires ale-py>=0.10.1, but you have ale-py 0.8.1 which is incompatible.\n",
            "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.28.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6f30340-4197-42a6-b488-4ca44e316fcd",
      "metadata": {
        "id": "d6f30340-4197-42a6-b488-4ca44e316fcd"
      },
      "source": [
        "First, let's import the necessary parts of the libraries for this exercise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "722a690a-8d49-4fa4-a9af-0caf43f00151",
      "metadata": {
        "tags": [],
        "id": "722a690a-8d49-4fa4-a9af-0caf43f00151"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.monitor import Monitor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a0845db-74f8-4a3a-b6fd-da5b275c91c9",
      "metadata": {
        "id": "1a0845db-74f8-4a3a-b6fd-da5b275c91c9"
      },
      "source": [
        "**Important:** should this step yield any errors, check if this notebook runs in the correct conda environment. Without the imports working, none of the following steps will work!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6df99342-99ea-4afa-889e-b896f9b41c39",
      "metadata": {
        "id": "6df99342-99ea-4afa-889e-b896f9b41c39"
      },
      "source": [
        "Next, we need to create the environments for which we want to learn a walking behavior. The goal of this environment is to make the robot move to the right side (positive x direction). For more information about the HalfCheetah environment, visit https://gymnasium.farama.org/environments/mujoco/half_cheetah/."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "f7d300aa-5f29-4f04-b014-f5ba528ca8f8",
      "metadata": {
        "tags": [],
        "id": "f7d300aa-5f29-4f04-b014-f5ba528ca8f8"
      },
      "outputs": [],
      "source": [
        "env_id = \"HalfCheetah-v4\"\n",
        "env_raw = gym.make(env_id, render_mode=\"rgb_array\")\n",
        "# A seperate environment is required for evaluation\n",
        "eval_env_raw = gym.make(env_id, render_mode=\"rgb_array\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d21dde0d-5197-47ec-9c93-59b38ee45cf2",
      "metadata": {
        "id": "d21dde0d-5197-47ec-9c93-59b38ee45cf2"
      },
      "source": [
        "With Deep Reinforcement Learning, an agent tries to maximize the reward it gets for the actions he performs. Your task is now to write a function to calculate the reward for the agent.\n",
        "\n",
        "The following are the most important values that you can use for calculating the reward.\n",
        "- `action` is an array containing the actions for the agent. The documentation linked above explains what these values represent.\n",
        "- Use `self.env.data.xpos` to get the positions of the robot's body parts. As calling the \"inner\" environment's `step` method advances the simulation using the generated action, it makes a difference if you obtain those positions before calling `step` or after. This is a two-dimensional environment. Therefore, `self.env.data.xpos[?][1]` (the y-coordinate) is of no interest for all body parts. The following information is taken from the definition of the environment: https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/mujoco/assets/half_cheetah.xml. The body parts should usually appear in the same order in the array as defined in the XML file using `<body>` tags.\n",
        "  * `self.env.data.xpos[1]`: torso\n",
        "  * `self.env.data.xpos[2]`: back thigh\n",
        "  * `self.env.data.xpos[3]`: back shin\n",
        "  * `self.env.data.xpos[4]`: back foot\n",
        "  * `self.env.data.xpos[5]`: front thigh\n",
        "  * `self.env.data.xpos[6]`: front shin\n",
        "  * `self.env.data.xpos[7]`: front foot\n",
        "- `self.dt` contains the time elapsed during the simulation step."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def writeRewardToFile(reward):\n",
        "  with open(\"reward.txt\", \"a\") as f:\n",
        "    f.write(str(reward)+\"\\n\")"
      ],
      "metadata": {
        "id": "RwL80DW7iweQ"
      },
      "id": "RwL80DW7iweQ",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "50b1cf89-4f57-4ff4-8b25-19cd899e70fc",
      "metadata": {
        "id": "50b1cf89-4f57-4ff4-8b25-19cd899e70fc"
      },
      "outputs": [],
      "source": [
        "class HalfCheetahRewardWrapper(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "\n",
        "    def step(self, action):\n",
        "        x_position_torso_before = self.env.data.xpos[1][0]\n",
        "\n",
        "        # Run the simulation and apply the action\n",
        "        observation, _, terminated, truncated, info = self.env.step(action)\n",
        "\n",
        "        x_position_torso_after = self.env.data.xpos[1][0]\n",
        "\n",
        "        # TODO: calculate the reward here\n",
        "        reward = x_position_torso_after\n",
        "        writeRewardToFile(reward)\n",
        "\n",
        "        return observation, reward, terminated, truncated, info\n",
        "\n",
        "# Wrap the existing environments\n",
        "env = HalfCheetahRewardWrapper(env_raw)\n",
        "eval_env = Monitor(HalfCheetahRewardWrapper(eval_env_raw))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acc0e7b0-8a04-44dc-ad3c-049ee012bfc7",
      "metadata": {
        "id": "acc0e7b0-8a04-44dc-ad3c-049ee012bfc7"
      },
      "source": [
        "Now, we need to create the model. It is initialized with a random policy.\n",
        "For this exercise, we'll use the PPO (Proximal Policy Optimization) algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "9f4bf73a-52c9-4d85-8219-632811776a88",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f4bf73a-52c9-4d85-8219-632811776a88",
        "outputId": "29c9c2c2-356c-47eb-ef0b-c72ea9238b41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n"
          ]
        }
      ],
      "source": [
        "model = PPO(\"MlpPolicy\", env, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c861c79-7203-40b6-b0cb-38e73f2e5e4c",
      "metadata": {
        "id": "5c861c79-7203-40b6-b0cb-38e73f2e5e4c"
      },
      "source": [
        "If you'd like to see how random policy performs, you can comment out the following block:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "d382dac1-c1bc-4021-9ed8-803915297a86",
      "metadata": {
        "tags": [],
        "id": "d382dac1-c1bc-4021-9ed8-803915297a86"
      },
      "outputs": [],
      "source": [
        "# mean_reward, std_deviation = evaluate_policy(model, eval_env, n_eval_episodes=100)\n",
        "# print(\"mean reward:\", mean_reward)\n",
        "# print(\"standard deviation:\", std_deviation)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e6dce8f-d694-41b2-a275-ab6cb001dd54",
      "metadata": {
        "id": "8e6dce8f-d694-41b2-a275-ab6cb001dd54"
      },
      "source": [
        "Now, it's time for learning how to walk. You can adjust the number of timesteps to learn on based on the strength of your computer. More timesteps take longer but may improve the resulting behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "beae75e2-5743-44d7-9d9b-9ea48978c362",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beae75e2-5743-44d7-9d9b-9ea48978c362",
        "outputId": "1c9d7464-ecfc-472f-b669-2c689bd23c5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | -1.21e+03 |\n",
            "| time/              |           |\n",
            "|    fps             | 687       |\n",
            "|    iterations      | 1         |\n",
            "|    time_elapsed    | 2         |\n",
            "|    total_timesteps | 2048      |\n",
            "----------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines3.ppo.ppo.PPO at 0x7ddcb047fd10>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "#model.learn(total_timesteps=200000)\n",
        "model.learn(total_timesteps=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0575cf68-565d-4af5-9fe8-84e815e6ff36",
      "metadata": {
        "id": "0575cf68-565d-4af5-9fe8-84e815e6ff36"
      },
      "source": [
        "Again, you can comment out the following block to measure if an improvement was made to the model compared to its initial random state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "347df854-1b69-425c-8c73-4b42d008817e",
      "metadata": {
        "tags": [],
        "id": "347df854-1b69-425c-8c73-4b42d008817e"
      },
      "outputs": [],
      "source": [
        "# mean_reward, std_deviation = evaluate_policy(model, eval_env, n_eval_episodes=100)\n",
        "# print(\"mean reward:\", mean_reward)\n",
        "# print(\"standard deviation:\", std_deviation)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc8ecbe2-46a5-40d1-b2b4-39da72550348",
      "metadata": {
        "id": "cc8ecbe2-46a5-40d1-b2b4-39da72550348"
      },
      "source": [
        "You can export a GIF of your trained behavior using the following code."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import imageio\n",
        "import numpy as np\n",
        "\n",
        "filename = \"halfcheetah.gif\"\n",
        "\n",
        "images = []\n",
        "obs = model.env.reset()\n",
        "img = model.env.render()\n",
        "start_pos = model.env.envs[0].data.xpos[1][0]\n",
        "for _ in range(200):\n",
        "    images.append(img)\n",
        "    action, _ = model.predict(obs)\n",
        "    obs, _, _, _ = model.env.step(action)\n",
        "    img = model.env.render()\n",
        "\n",
        "end_pos = model.env.envs[0].data.xpos[1][0]\n",
        "print(f\"Distance travelled: {end_pos - start_pos}m\")\n",
        "\n",
        "imageio.mimsave(filename, [np.array(img) for img in images], duration=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Soy3vFrMh2bR",
        "outputId": "c3825f5d-15b6-4f7c-9b1f-e3813562bf68"
      },
      "id": "Soy3vFrMh2bR",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distance travelled: 1.465750841958108m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zrIG9cNpjBpw"
      },
      "id": "zrIG9cNpjBpw",
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "4ecb1b3d-fb55-4f30-be2c-41347e017b13",
      "metadata": {
        "id": "4ecb1b3d-fb55-4f30-be2c-41347e017b13"
      },
      "source": [
        "Within the visualization, one side of a (black or white) square is $\\frac{2}{3}$ meters long."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9d3b41f-d0ab-4bad-8a36-ee3175bd3c3c",
      "metadata": {
        "id": "d9d3b41f-d0ab-4bad-8a36-ee3175bd3c3c"
      },
      "source": [
        "If you are interested in training robots in other environments, check out the already existing environments in Gymnasium: https://gymnasium.farama.org/environments/mujoco/.\n",
        "You should be able to use another environment id and everything else in the notebook should mostly continue to work. Of course, you then need to find out yourself what the positions in `self.env.data.xpos` represent for your chosen environment.\n",
        "Note: as of writing this (August 2023), there seem to be some problems with the Hopper and the Walker2D environment that prevent these environments from being used."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d885a1e",
      "metadata": {
        "id": "6d885a1e"
      },
      "source": [
        "Now let's see how high the robot is able to jump. Once again, we need to define a reward function.\n",
        "It might make sense to manually set `terminated` and `truncated` in this case.\n",
        "Have a look at https://gymnasium.farama.org/api/env/#gymnasium.Env.step for an explanation of their meaning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "75a8891d",
      "metadata": {
        "id": "75a8891d"
      },
      "outputs": [],
      "source": [
        "class HalfCheetahJumpRewardWrapper(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "\n",
        "    def step(self, action):\n",
        "        x_position_torso_before = self.env.unwrapped.data.xpos[1][0]\n",
        "\n",
        "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
        "\n",
        "        x_position_torso_after = self.env.unwrapped.data.xpos[1][0]\n",
        "        custom_reward = x_position_torso_after - x_position_torso_before\n",
        "\n",
        "        return obs, custom_reward, terminated, truncated, info\n",
        "\n",
        "# Wrap the existing environments\n",
        "jump_env = HalfCheetahJumpRewardWrapper(env_raw)\n",
        "jump_eval_env = Monitor(HalfCheetahJumpRewardWrapper(eval_env_raw))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42e3dc40",
      "metadata": {
        "id": "42e3dc40"
      },
      "source": [
        "Again, we need to create a new model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "2937a633",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2937a633",
        "outputId": "4ec95201-c435-4904-d6ed-982c47efa996"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n"
          ]
        }
      ],
      "source": [
        "jump_model = PPO(\"MlpPolicy\", jump_env, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "210b78c7",
      "metadata": {
        "id": "210b78c7"
      },
      "source": [
        "To measure the performance of the random policy, comment out the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "a089a18c",
      "metadata": {
        "tags": [],
        "id": "a089a18c"
      },
      "outputs": [],
      "source": [
        "# mean_reward, std_deviation = evaluate_policy(jump_model, jump_eval_env, n_eval_episodes=100)\n",
        "# print(\"mean reward:\", mean_reward)\n",
        "# print(\"standard deviation:\", std_deviation)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6045c7df",
      "metadata": {
        "id": "6045c7df"
      },
      "source": [
        "Train the network:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "99c63314",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99c63314",
        "outputId": "92ae0211-15d2-407e-876f-7799959ea5d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 2.09     |\n",
            "| time/              |          |\n",
            "|    fps             | 728      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 2        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines3.ppo.ppo.PPO at 0x7ddc7c15b010>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "jump_model.learn(total_timesteps=200) #200000"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5defac0e",
      "metadata": {
        "id": "5defac0e"
      },
      "source": [
        "As usual, you can run an evaluation to measure the new result:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "d239ff47",
      "metadata": {
        "tags": [],
        "id": "d239ff47"
      },
      "outputs": [],
      "source": [
        "# mean_reward, std_deviation = evaluate_policy(jump_model, jump_eval_env, n_eval_episodes=100)\n",
        "# print(\"mean reward:\", mean_reward)\n",
        "# print(\"standard deviation:\", std_deviation)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da6899bf",
      "metadata": {
        "id": "da6899bf"
      },
      "source": [
        "Of course, you can also export another GIF of your trained jumping behavior using the following code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "e16cc34b",
      "metadata": {
        "tags": [],
        "id": "e16cc34b"
      },
      "outputs": [],
      "source": [
        "import imageio\n",
        "import numpy as np\n",
        "\n",
        "filename = \"halfcheetah_jump.gif\"\n",
        "\n",
        "images = []\n",
        "obs = jump_model.env.reset()\n",
        "img = jump_model.env.render()\n",
        "for _ in range(100):\n",
        "    images.append(img)\n",
        "    action, _ = jump_model.predict(obs)\n",
        "    obs, _, _, _ = jump_model.env.step(action)\n",
        "    img = jump_model.env.render()\n",
        "\n",
        "imageio.mimsave(filename, [np.array(img) for img in images], duration=50)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:drl3]",
      "language": "python",
      "name": "conda-env-drl3-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}